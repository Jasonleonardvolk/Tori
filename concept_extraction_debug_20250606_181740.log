--- Running extraction on: C:\Users\jason\Desktop\tori\kha\data\memory\2407.15527v2.pdf ---

=== [PAGE 1] ===
Interpretable Concept-Based Memory Reasoning
David Debot
KU Leuven
david.debot@kuleuven.be
Pietro Barbiero
Universita’ della Svizzera Italiana
University of Cambridge
barbiero@tutanota.com
Francesco Giannini
Scuola Normale Superiore
francesco.giannini@sns.it
Gabriele Ciravegna
DAUIN, Politecnico di Torino
gabriele.ciravegna@polito.it
Michelangelo Diligenti
University of Siena
michelangelo.diligenti@unisi.it
Giuseppe Marra
KU Leuven
giuseppe.marra@kuleuven.be
Abstract
The lack of transparency in the decision-making processes of deep learning sys-
tems presents a significant challenge in modern artificial intelligence (AI), as it
impairs users’ ability to rely on and verify these systems. To address this challenge,
Concept-Based Models (CBMs) have made significant progress by incorporating
human-interpretable concepts into deep learning architectures. This approach
allows predictions to be traced back to specific concept patterns that users can
understand and potentially intervene on. However, existing CBMs’ task predic-
tors are not fully interpretable, preventing a thorough analysis and any form of
formal verification of their decision-making process prior to deployment, thereby
raising significant reliability concerns. To bridge this gap, we introduce Concept-
based Memory Reasoner (CMR), a novel CBM designed to provide a human-
understandable and provably-verifiable task prediction process. Our approach is
to model each task prediction as a neural selection mechanism over a memory
of learnable logic rules, followed by a symbolic evaluation of the selected rule.
The presence of an explicit memory and the symbolic evaluation allow domain
experts to inspect and formally verify the validity of certain global properties of
interest for the task prediction process. Experimental results demonstrate that
CMR achieves better accuracy-interpretability trade-offs to state-of-the-art CBMs,
discovers logic rules consistent with ground truths, allows for rule interventions,
and

=== [PAGE 2] ===
to a specific pattern of concepts, thus allowing CBMs to provide explanations in terms of high-level
interpretable concepts (e.g. concepts “red” and “round” were both active when the model classified
an image as “apple”) rather than low-level raw features (e.g. there were 100 red pixels when the
model classified an image as “apple”). In other terms, a CBM’s task predictor allows understanding
what the model sees in a given input rather than simply pointing to where it is looking [9].
However, state-of-the-art CBMs’ task predictors are either unable to solve complex tasks (e.g. linear
layers), non-differentiable (e.g. decision trees), or black-box neural networks. CBMs employing
black-box task predictors are still considered locally interpretable, as concept interventions allow
humans to understand how concepts influence predictions for individual input examples. However,
they lack global interpretability: the human cannot interpret the model’s global behaviour, i.e. on
any possible instance. This prevents a proper understanding of the model’s working, as well as any
chance of formally verifying the task predictor decision-making process prior to deployment, thus
raising significant concerns in practical applications. As a result, a knowledge gap persists in the
existing literature: the definition of a CBM with a task predictor whose behaviour can be inspected,
verified, and potentially intervened upon before the deployment of the system.
To address this gap, we propose Concept-based Memory Reasoner (CMR), a new CBM where the
behaviour and explanations can be inspected and verified before the model is deployed. CMR’s
task predictor offers global interpretability as it utilizes a differentiable memory of learnable logic
rules, making all potential decision rules transparent to humans. Additionally, CMR avoids the
concept bottleneck that often limits the accuracy of interpretable models when compared to black-box
approaches. Our key innovation lies in an attention mech

=== [PAGE 3] ===
3
Model
In this section, we introduce Concept-based Memory Reasoner (CMR), the first concept-based model
that is globally interpretable, provably verifiable and a universal binary classifier. CMR consists of
three main components: a concept encoder, a rule selector and a task predictor. CMR’s task prediction
process differs significantly from traditional CBMs. It operates transparently by (1) selecting a
logic rule from a set of jointly-learned rules, and (2) symbolically evaluating the chosen rule on the
concept predictions. This unique approach enables CMR not only to provide explanations by tracing
class predictions back to concept activations, but also to explain which concepts are utilized and
how they interact to make a task prediction. Moreover, the set of learned rules remains accessible
throughout the learning process, allowing users to analyse the model’s behaviour and automatically
verify whether some desired properties are being fulfilled at any time. The logical interpretation of
CMR’s task predictor, combined with its provably verifiable behaviour, distinguishes it sharply from
existing CBMs’ task predictors.
3.1
Probabilistic graphical model
In Figure 1, we show the probabilistic graphical model of CMR. There are four variables, three of
which are standard in (discriminative) CBMs: the observed input x ∈X, the concepts encoding c ∈C
x
c
y
r
Figure 1: Probabilistic graphi-
cal model of CMR
and the task prediction y ∈{0, 1}. CMR adds an additional variable:
the rule r ∈{P, N, I}nC. A rule is a conjunction in the concept set,
like c1 ∧¬c3. A conjunction is uniquely identified when, for each
concept ci, we know whether, in the rule, the concept is irrelevant (I),
positive (P) or negative (N). We call ri ∈{P, N, I} the role of the i-
th concept in rule r. For example, given the three concepts c1, c2, c3,
the conjunction c1 ∧¬c3 can be represented as r1 = P, r2 = I, r3 =
N, since the role of c1 is positive (P), the role of c2 is irrelevant (I)
and the role 

=== [PAGE 4] ===
𝑝𝑐𝑥
.8
.1
.0
R S T
∼
red=1, square=0, table=0
𝑝𝑠𝑥
RULEBOOK
.7
.3
𝑝𝑟𝑠
.7
.1
.1
R S T
.2
.8
.0
.1
.1
.9
decoded rule
P
N
I
∼
apple ←red ∧¬square
logical eval
𝑝𝑦𝑐, 𝑟
apple=1
(A)
(C)
(B)
(D)
(E)
𝑒1
𝑒2
“rule embedding”
= latent repr. of a rule
∼
𝑒1
concepts
Figure 2: Example prediction of CMR with a rulebook of two rules and three concepts (i.e. red (R),
square (S), table (T)). In this figure, we sample (∼) for clarity, but in practice, we compute every
probability exactly. Every black box is implemented by a neural network, while the white box is a
pure symbolic logic evaluation. (A) The image is mapped to a concept prediction. (B) The image is
mapped by the component selector to a distribution over rules. (C) This distribution is used to select
a rule embedding from the encoded rulebook. (D) The rule embedding is decoded into a logic rule by
assigning to each of the concepts its role in the rule, i.e. whether it is positive (P), negative (N), or
irrelevant (I). Finally, (E) the rule is evaluated on the concept prediction to provide the task prediction
on the task apple.
Here, p(s|x) is the categorical distribution defining the mixing weights of the mixture. It is parame-
terized by a neural network ϕ(s) : X →RnR, outputting one logit for each of the nR components.
Each p(r|s) is a distribution over all possible rules and is modelled as a product of nC categorical
distributions, i.e. p(r|s) = QnC
i=1p(ri|s). We assign to each component s = j a rule embedding
ej ∈Rq, and each categorical ri is then parameterized by a neural network ϕ(r)
i
: Rq →R3,
mapping the rule embedding to the logits of the categorical component. Intuitively, for each concept
ci, the corresponding categorical distribution p(ri|s = j) decodes the embedding ej to the three
possible roles ri ∈{P, N, I} of concept ci in rule r. This way, each embedding in the rulebook
is the latent representation of a logic rule.2 Lastly, we define the set E of all rule embeddings, i.e.
E = {ej}j∈[1,nR], as the encoded 

=== [PAGE 5] ===
4
Expressivity, interpretability and verification
In this section, we will discuss the proposed model along three different directions: expressivity,
interpretability and verification.
4.1
Expressivity
An interesting property is that CMR is as expressive as a neural network binary classifier.
Theorem 4.1. CMR is a universal binary classifier [13] if nR ≥3.
Proof. Recall that the rule selector is implemented by some neural network ϕ(s) : X →RnR.
Consider the following three rules, easily expressible in CMR as showed on the right of each rule:
y ←True (i.e. ∀i : ri = I),
y ←
nC
^
i=1
ci (i.e. ∀i : ri = P),
y ←
nC
^
i=1
¬ci (i.e. ∀i : ri = N)
By selecting one of these three rules, the rule selector can always make a desired y prediction,
regardless of the concepts c. In particular, to predict y = 1, the selector can select the first rule. To
predict y = 0 when at least one concept has probability less than 50% in the concept predictions c
(i.e. ∃i : p(ci|x) < 0.50), it can select the second rule. Lastly, to predict y = 0 when all concepts
have probability of at least 50% in c (i.e. ∀i : p(ci|x) ≥0.50), it can select the last rule.
Consequently, CMR can in theory always achieve the same accuracy as a neural network without
concept bottleneck, irrespective of which concepts are employed in the model. This distinguishes
CMR sharply from CBNMs.
4.2
Interpretability
CMR’s task prediction is the composition of a (neural) rule selector and the symbolic evaluation of
the selected rule. Therefore, we can always inspect the whole rulebook to know exactly the global
behaviour of the task prediction. In particular, let s be the selected rule, ej the embedding of the
j-th rule, and r(j)
i
∈{P, N, I} the role of the i-th concept in the j-th rule at decision time, i.e.
r(j)
i
= argmax(ϕ(r)
i (ej)). Then, CMR’s task prediction can be logically defined as the global rule
obtained as the disjunction of all decoded rules, each filtered by whether the rule has been selected or
not. That i

=== [PAGE 6] ===
Interventions. In contrast to existing CBMs, which only allow prediction-time concept interventions,
the global interpretability of CMR allows humans to intervene also on the rules that will be exploited
for task prediction. These interactions may occur during the training phase to directly shape the
model that is being learned, and may come in various forms. A first approach involves the manual
inclusion of rules into the rulebook, thereby integrating expert knowledge into the model [18]. The
selector mechanism within the model learns to choose among the rules acquired through training
and those manually incorporated. As a result, the rules that are being acquired through training
can change after adding expert rules. A second approach consists of modifying the learned rules
themselves, by altering the role ri of a concept within a rule. For instance, setting the logit of Pi to 0
ensures that ci cannot exhibit a positive role in that rule, and setting the logits of both Pi and Ni to 0
ensures irrelevance. This type of intervention could be exploited to remove (or prevent) biases and
ensure counterfactual fairness [19].
4.3
Verification
One of the main properties of CMR is that, at decision time, it explicitly represents the task prediction
as a set of conjunctive rules. Logically, the mixture semantics of the selector can be interpreted
as a disjunction, leading to a standard semantics in stochastic logic programming [20, 21]. As the
only neural component is in the selection and the concept predictions, task predictions generated
using CMR’s global formula (cf. Section 4.2) can be automatically verified by using standard tools of
formal verification (e.g. model checking), no matter which rule will be selected. Being able to verify
properties prior to deployment of the model strongly sets CMR apart from existing models, where
verification tasks can only be applied at prediction time. In particular, given any propositional logical
formula α over the propositional lan

=== [PAGE 7] ===
Proof. See Appendix A
The maximum likelihood approach only focuses on the prediction accuracy of the model. However,
as discussed in Section 4.2, we look for the set of learned rules r to represent prototypes of concept
predictions, as in prototype-based learning [15]. To drive the learning of representative positive
prototypes when we observe a positive value for the task, i.e. when y = 1, we add a regularization
term to the objective. Intuitively, every time a rule is selected for a given input instance x with task
label y = 1, we want the rule to be as close as possible to the observed concept prediction. At the
same time, since the number of rules is limited and the possible concept activations are combinatorial,
the same rule is expected to be selected for different concept activations. When this happens, we
will favour rules that assign an irrelevant role to the inconsistent concepts in the activations. The
regularized objective is:
max
Ω
X
(ˆx,ˆc,ˆy)∈D
 nC
X
i=1
log p(ci = ˆci|ˆx)
!
+


log
nR
X
ˆs=1
p(s = ˆs|ˆx) p(y = ˆy|ˆc, ˆs) preg(r = ˆc|ˆs)ˆy
|
{z
}
Regularization Term


(8)
and:
preg(r = ˆc|s) =
nC
Y
i=1
(0.5 p(ri = I|s) + p(ri = P|s) 1[ˆci = 1] + p(ri = N|s) 1[ˆci = 0])
This term favours the selected rule r to reconstruct the observed ˆc as much as possible. When such
reconstruction is not feasible due to the limited capacity of the rulebook, the term will favour irrelevant
roles for concepts. In this way, we will develop rules that have relevant terms (i.e. ri ∈{P, N}) only
if they are representative of all the instances in which the rule is selected. Appendix B contains an
investigation of the influence of this regularization on the optima of the loss.
5.2
Inference
After training, we replace each role distribution p(ri|s = j) for each concept i and rule embedding j
with the most likely role. This ensures that each embedding corresponds to a single logic rule rather
than a distribution over all possible rules. Moreover, at decision-time, the co

=== [PAGE 8] ===
Table 1: Task accuracy on all datasets. The best and second best for CBMs are shown in bold (black
and purple, respectively).
MNIST+
MNIST+∗
C-MNIST
CELEBA
CUB
CEBAB
CBM+linear
0.00±0.00
0.00±0.00
99.07±0.31
49.02±0.20
50.60±0.69
45.15±29.93
CBM+MLP
97.41±0.55
72.51±2.42
99.42±0.11
50.29±0.60
55.83±0.33
83.70±0.30
CBM+DT
96.73±0.39
77.63±0.44
99.44±0.03
49.60±0.20
51.83±0.48
83.15±0.15
CBM+XG
96.73±0.39
76.54±0.54
99.44±0.03
50.39±0.24
62.97±0.96
83.80±0.01
CEM
92.44±0.26
92.94±1.15
99.32±0.11
65.44±0.25
57.03±0.80
83.30±1.59
DCR
90.70±1.21
92.24±1.37
98.99±0.08
35.65±1.53
50.00±0.00
67.30±0.93
Black box
83.26±8.71
83.26±8.71
99.19±0.11
65.33±0.60
64.07±0.33
88.67±0.19
CMR (ours)
97.25±0.24
94.65±1.99
99.12±0.04
63.17±1.13
60.07±1.70
85.14±0.43
Data & task setup. We base our experiments on four different datasets commonly used to evaluate
CBMs: MNIST+ [22], where the task is to predict the sum of two digits; C-MNIST, where we
adapted MNIST to the task of predicting whether a coloured digit is even or odd; MNIST+∗, where
we removed the concepts for the digits 0 and 1 from the concept set; CelebA [23], a large-scale
face attributes dataset with more than 200K celebrity images, each with 40 concept annotations4;
CUB [24], where the task is to predict bird species based on bird characteristics; and CEBaB [25], a
text-based task where reviews are classified as positive or negative based on different criteria (e.g.
food, ambience, service, etc). These datasets range across different concept set quality, i.e. complete
(MNIST+, C-MNIST, CUB) vs incomplete (CelebA, MNIST+∗), and different complexities of the
concept prediction task, i.e. easy (MNIST+, MNIST+∗, C-MNIST), medium (CEBaB) and hard
(CelebA, CUB). All the datasets come with full concept annotations.
Evaluation. To measure classification performance on tasks and concepts, we compute subset
accuracy and regular accuracy, respectively. For CUB, we instead compute the Area Under the
Receiver Operating Characteristic C

=== [PAGE 9] ===
CelebA where we gradually decrease the number of concepts in the concept set. Figure 3 shows the
achieved task accuracies for CMR and the competitors. CMR’s accuracy remains high no matter the
size of the concept set, while the performance of the competitors with a bottleneck (i.e. all except
CEM) strongly degrades.
6.2.2
Explanations and intervention
CMR discovers ground truth rules. We quantitatively evaluate the correctness of the rules CMR
learns on MNIST+ and C-MNIST. In the former, the ground truth rules have no irrelevant concepts;
in the latter, they do. In all runs of these experiments, CMR finds all correct ground truth rules. In
C-MNIST, CMR correctly learns that the concepts related to colour are irrelevant for classifying the
digit as even or odd (see Table 2).
CMR discovers meaningful rules in the absence of ground truth. While the other datasets do
not provide ground truth rules, a qualitative inspection shows that they are still meaningful. Table 2
shows two examples for CEBaB, and additional rules can be found in Appendix C.
Table 2: Selection of learned rules. For brevity in C-
MNIST, negated concepts are not shown and irrelevant
concepts are shown between parentheses. We abbreviate
good to g, bad to b and unknown to u.
C-MNIST
yeven ←0 ∧(red) ∧(green)
yeven ←2 ∧(red) ∧(green)
yodd ←3 ∧(red) ∧(green)
CEBaB
yneg ←¬foodg ∧¬ambg ∧¬noiseg
ypos ←¬foodb ∧¬ambb ∧noiseu ∧
¬noiseb ∧¬noiseg
1
12
24
37
Number of Concepts
0.3
0.4
0.5
0.6
Task Test Accuracy
Black box
CBM+DT
CBM+MLP
CBM+XG
CEM
CMR
Figure 3: Task accuracy on CelebA with
varying numbers of employed concepts.
Rule interventions during training allow human experts to improve the learned rules. We
show this by choosing a rulebook size for MNIST+ that is too small to learn all ground truth rules.
Consequently, CMR learns rules that differ from the ground truth rules. After manually adding rules
to the pool in the middle of training, CMR (1) learns to select these new rules for training examples for
wh

=== [PAGE 10] ===
verifying one formula per concept j of digit k: ∀y, i ̸= j : y ∧ck,j ⇒¬ck,i. This is also easily
verifiable by simply inspecting the rules in Appendix C. Moreover, in CelebA, we can easily verify
that Bald ⇒¬Wavy_Hair with the learned rulebook for nC = 12 (see Table 10 in Appendix C), as
¬Bald is a conjunct in each rule that does not trivially evaluate to False.
7
Related works
In recent years, XAI techniques have been criticized for their vulnerability to data modifications
[30, 31], insensitivity to reparametrizations, [32], and lacking meaningful interpretations for non-
expert users [33]. To address these issues, Concept-based methods [34, 35, 5, 10] have emerged,
offering explanations in terms of human-interpretable features, a.k.a. concepts. Concept Bottleneck
Models [4] go a step further by directly integrating these concepts as explicit intermediate network
representations. Concept Embeddings Models (CEMs) [7, 8, 11] close the accuracy gap with black-
box models through vectorial concept representations. However, they still harm the interpretability, as
it is unclear what information is contained in the embeddings. In contrast, CMR closes the accuracy
gap by using a neural rule selector coupled with learned symbolic logic rules. As a result, CMR’s task
prediction is transparent, allowing experts to see how concepts are being used for task prediction, and
allowing intervention and automatic verification of desired properties. To the best of our knowledge,
there is only one other attempt at analysing CBMs’ task prediction in terms of logical formulae,
namely DCR [11]. For a given example, DCR predicts and subsequently evaluates a (fuzzy) logic
rule. As rules are predicted on a per-example basis, the global behaviour of DCR cannot be inspected,
rendering interaction (e.g. adding expert rules) and verification impossible. In contrast, CMR learns
(probabilistic) logic rules in a memory, allowing for inspection, interaction and verification.
The use of logic rules

=== [PAGE 11] ===
Acknowledgments and Disclosure of Funding
DD is a fellow of the Research Foundation-Flanders (FWO-Vlaanderen, 1185125N). This research has
also received funding from the KU Leuven Research Fund (STG/22/021, CELSA/24/008) and from
the Flemish Government under the "Onderzoeksprogramma Artificiële Intelligentie (AI) Vlaanderen"
programme. FG has been supported by the Partnership Extended PE00000013 - “FAIR - Future
Artificial Intelligence Research” - Spoke 1 “Human-centered AI”. PB acknowledges support from
SNSF project TRUST-ME (No. 205121L_214991). MD was supported by TAILOR and by HumanE-
AI-Net projects funded by EU Horizon 2020 research and innovation programme under GA No
952215 and No 952026, respectively. This study has received funding from the European Union’s EU
Framework Program for Research and Innovation Horizon under the Grant Agreement No 101073307
(MSCA-DN LeMuR).
References
[1] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and
Dino Pedreschi. A survey of methods for explaining black box models. ACM computing surveys
(CSUR), 51(5):1–42, 2018.
[2] Amina Adadi and Mohammed Berrada. Peeking inside the black-box: a survey on explainable
artificial intelligence (xai). IEEE access, 6:52138–52160, 2018.
[3] Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham
Tabik, Alberto Barbado, Salvador García, Sergio Gil-López, Daniel Molina, Richard Ben-
jamins, et al. Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and
challenges toward responsible ai. Information fusion, 58:82–115, 2020.
[4] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim,
and Percy Liang. Concept bottleneck models. In International conference on machine learning,
pages 5338–5348. PMLR, 2020.
[5] David Alvarez Melis and Tommi Jaakkola. Towards robust interpretability with self-explaining
neural networks. Advances in neural information processing systems, 31, 

=== [PAGE 12] ===
[15] Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong.
Interpretable machine learning: Fundamental principles and 10 grand challenges. Statistic
Surveys, 16:1–85, 2022.
[16] Oscar Li, Hao Liu, Chaofan Chen, and Cynthia Rudin. Deep learning for case-based reasoning
through prototypes: A neural network that explains its predictions. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 32, 2018.
[17] Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan K Su.
This looks like that: deep learning for interpretable image recognition. Advances in neural
information processing systems, 32, 2019.
[18] Michelangelo Diligenti, Marco Gori, and Claudio Sacca. Semantic-based regularization for
learning and inference. Artificial Intelligence, 244:143–165, 2017.
[19] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness.
Advances in neural information processing systems, 30, 2017.
[20] James Cussens. Parameter estimation in stochastic logic programs. Machine Learning, 44:
245–271, 2001.
[21] Thomas Winters, Giuseppe Marra, Robin Manhaeve, and Luc De Raedt. Deepstochlog: Neural
stochastic logic programming. Proceedings of the AAAI Conference on Artificial Intelligence,
36(9):10090–10100, Jun. 2022. doi: 10.1609/aaai.v36i9.21248. URL https://ojs.aaai.
org/index.php/AAAI/article/view/21248.
[22] Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, and Luc De
Raedt. DeepProbLog: Neural Probabilistic Logic Programming. In NeurIPS, pages 3753–3763,
2018.
[23] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the
wild. In ICCV, 2015.
[24] Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Be-
longie, and Pietro Perona. Caltech-ucsd birds 200. Technical Report CNS-TR-201, Cal-
tech, 2010. URL /se3/wp-content/uploads/2014/09/WelinderEtal10_CUB-200.pdf,
http://www.vision.caltech.edu/visipedia

=== [PAGE 13] ===
[31] Amirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile.
In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 3681–3688,
2019.
[32] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim.
Sanity checks for saliency maps. Advances in neural information processing systems, 31, 2018.
[33] Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, Jennifer Wortman Wort-
man Vaughan, and Hanna Wallach. Manipulating and measuring model interpretability. In
Proceedings of the 2021 CHI conference on human factors in computing systems, pages 1–52,
2021.
[34] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al.
Interpretability beyond feature attribution: Quantitative testing with concept activation vectors
(tcav). In International conference on machine learning, pages 2668–2677. PMLR, 2018.
[35] Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic concept-
based explanations. Advances in neural information processing systems, 32, 2019.
[36] Artur d’Avila Garcez, Sebastian Bader, Howard Bowman, Luis C Lamb, Leo de Penning,
BV Illuminoo, Hoifung Poon, and COPPE Gerson Zaverucha. Neural-symbolic learning and
reasoning: A survey and interpretation. Neuro-Symbolic Artificial Intelligence: The State of the
Art, 342(1):327, 2022.
[37] Giuseppe Marra, Sebastijan Dumanˇci´c, Robin Manhaeve, and Luc De Raedt. From statistical
relational to neurosymbolic artificial intelligence: A survey. Artificial Intelligence, page 104062,
2024.
[38] Jingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, and Guy Broeck. A semantic loss function
for deep learning with symbolic knowledge. In International conference on machine learning,
pages 5502–5511. PMLR, 2018.
[39] Samy Badreddine, Artur d’Avila Garcez, Luciano Serafini, and Michael Spranger. Logic tensor
networks. Artificial Intelligence, 303:103649, 2022.
[40] Gustav Sourek, Vo

=== [PAGE 14] ===
[47] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony
Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,
Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain
Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-
art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing: System Demonstrations, pages 38–45, Online, October 2020.
Association for Computational Linguistics. URL https://www.aclweb.org/anthology/
2020.emnlp-demos.6.
[48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,
high-performance deep learning library, 2019.
[49] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion,
Olivier Grisel, Mathieu Blondel, Andreas Müller, Joel Nothman, Gilles Louppe, Peter Pretten-
hofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau,
Matthieu Brucher, Matthieu Perrot, and Édouard Duchesnay. Scikit-learn: Machine learning in
python, 2018.
[50] J. D. Hunter. Matplotlib: A 2d graphics environment. Computing in Science & Engineering, 9
(3):90–95, 2007. doi: 10.1109/MCSE.2007.55.
14


=== [PAGE 15] ===
A
Maximum likelihood derivation
We show that the maximum likelihood problem in Equation 6 simplifies to:
log p(ˆy, ˆc|ˆx) =
 nC
X
i=1
log p(ci = ˆci|ˆx)
!
+
 
log
nR
X
ˆs=1
p(s = ˆs|ˆx) p(y = ˆy|ˆc, ˆs)
!
with:
p(y|c, s) =
nC
Y
i=1
(p(Ii|s) + p(Pi|s) 1[ci = 1] + p(Ni|s) 1[ci = 0])
Proof. Let n := nC. First, we express the likelihood as the marginalization of the distribution over
the unobserved variables
p(y, c|x) = p(c|x)
X
s
p(s|x)
X
r1
...
X
rn
p(r1, ..., rn|s)p(y|r1, ..., rn, c1, ..., cn)
Due to the independence of the individual components of the rule distribution:
p(r1, ..., rn|s) =
n
Y
i=1
p(ri|s)
The logical evaluation of a rule can be expressed in terms of indicator functions over the different
variables involved.
p(y|r1, ..., rn, c1, ..., cn) =
n
Y
i=1
1[ri = I] + 1[ri = P]1[ci = 1] + 1[ri = n]1[ci = 0]
Let us define f(ri, ci) := 1[ri = I] + 1[ri = P]1[ci = 1] + 1[ri = n]1[ci = 0].
Then, the likelihood becomes:
p(y, c|x) = p(c|x)
X
s
p(s|x)
X
r1
...
X
rn
n
Y
i=1
p(ri|s) f(ri, ci)
= p(c|x)
X
s
p(s|x)
 X
r1
p(r1|s) f(r1, c1)
!
(...)
 X
rn
p(rn|s) f(rn, cn)
!
= p(c|x)
X
s
p(s|x)
n
Y
i=1
X
ri
p(ri|s) f(ri, ci)
= p(c|x)
X
s
p(s|x)
n
Y
i=1
(p(Ii|s) f(Ii, ci) + p(Pi|s) f(Pi, ci) + p(Ni|s) f(Ni, ci))
= p(c|x)
X
s
p(s|x)
n
Y
i=1
(p(Ii|s) + p(Pi|s) 1[ci = 1] + p(Ni|s) 1[ci = 0])
= p(c|x)
X
s
p(s|x) p(y|c, s)
Using p(c|x) = QnC
i=1 p(ci|x), and applying the logarithm, we find:
log p(y, c|x) =
 nC
X
i=1
log p(ci|x)
!
+
 
log
nR
X
s=1
p(s|x) p(y|c, s)
!
15


=== [PAGE 16] ===
(a) p(y|c)
(b) p(y|¬c)
(c) p(y|c) ∗p(y|¬c)
(d) preg(r)
(e) preg(¬r)
(f) preg(r) ∗preg(¬r)
(g) p(y|c) ∗preg(r)
(h) p(y|¬c) ∗preg(¬r)
(i) p(y|c) ∗preg(r) ∗p(y|¬c) ∗
preg(¬r)
Figure 4: Likelihoods to be maximized when y = 1 w.r.t. the role r of a single concept in a selected
rule, for different situations. As P + N + I = 1, irrelevance is the coordinate (0, 0). Likelihoods that
cannot be achieved (i.e. when P + N > 1) are put to 0. In the first column, the concept label is 1. In
the second column, it is 0. In the third column, two examples with opposite labels select the rule.
B
Implementation and optimization details
Selector re-initialization
To promote exploration, we re-initialize the parameters of p(s|x) multiple
times during training, making it easier to escape local optima. The re-initialization frequency is a
hyperparameter that differs between experiments, see Appendix C.
Effect of the regularization term
Figure 4 shows the probabilities to be maximized when the
label y = 1, for a selected rule and a single concept, with respect to different roles r for that concept.
Figures 4a, 4b and 4c show the probabilities to be maximized without the regularization. Figures 4d,
4e and 4f show the regularization probabilities (remember, we only have these if y = 1). Figures
4g, 4h and 4i show the probabilities when both are present. As mentioned in the main text, when
c is True for all examples that select the rule, we want that concept’s role to be P. However, when
optimizing without regularization, it is clear that e.g. I is also an optimum (Figure 4a). Because the
regularization only has an optimum in P (Figure 4d), adding the regularization results in the correct
optimum (Figure 4g). A similar reasoning applies when c is False for all examples that select the rule;
consider Figures 4b, 4e and 4h. It should be noted that the regularization alone is insufficient to get
the correct optima; indeed, when examples with different c select the rule, the regularization term has

=== [PAGE 17] ===
Passing predicted concepts
While probabilistic semantics require passing the ground truth c to
the task predictor during training (as these are observed variables), CBMs typically instead pass the
concept predictions (opposite of teacher forcing) to make the model more robust to its own errors. For
this reason, we also employ this technique in CMR in the experiments with CEBaB, CUB and CelebA.
For competitors, we always employ this technique. However, we pass hard concept predictions in
both CMR and the competitors, as to avoid leakage (see Appendix C).
Weight in the loss
We introduce a weight β in the regularized objective of Equation 8 (see below).
This adjustment helps to balance the regularization and the task loss. For instance, if the regularization
dominates, CMR might learn rules that can only be used to predict y = 1 in practice. Additionally, a
larger β might assist in settings where achieving high accuracy is difficult for some concepts; in such
cases, concepts with lower accuracy can be more easily considered irrelevant by choosing a larger
β, as they are unable to reliably contribute to correct task prediction. The updated objective is as
follows:
max
Ω
X
(ˆx,ˆc,ˆy)∈D
 nC
X
i=1
log p(ci = ˆci|ˆx)
!
+


log
nR
X
ˆs=1
p(s = ˆs|ˆx) p(y = ˆy|ˆc, ˆs)β preg(r = ˆc|s)ˆy
|
{z
}
Regularization Term



(10)
C
Experiments
C.1
Datasets
MNIST+
This dataset [22] consists of pairs of images, where each image is an MNIST image of a
digit and the task is the sum of the two digits. For these tasks, all concepts are relevant. There is a
total of 30,000 training examples and 5,000 test examples.
MNIST+∗
We create this dataset as MNIST+ except that the concepts for digits 0 and 1 are removed
from the concept set. This makes the concept set incomplete.
C-MNIST
We derive this dataset from MNIST [44], taking the MNIST training (60,000 examples)
and test set (10,000 examples), randomly colouring each digit either red or green, and adding these
two colours as concepts. The

=== [PAGE 18] ===
is that the use of soft concepts also comes with the introduction of input distribution leakage: The
concept probabilities can encode much more information than what is related to the concepts, severely
harming the interpretability of the model [28, 29]. For this reason, in our experiments, all models use
hard concepts, which is realized by thresholding the soft concept predictions at 50%.
Model input
For MNIST+, MNIST+∗and C-MNIST, we train directly on the images. For CelebA
and CUB, instead of training on the images, we train the models on pretrained ResNet18 embeddings
[45]. Specifically, using the torchvision library, we first resize the images to width and height 224
(using bi-linear interpolation), then normalize them per channel with means (0.485, 0.456, 0.406)
and standard-deviations (0.229, 0.224, 0.225) (for CelebA only). Finally, we remove the last (clas-
sification) layer from the pretrained ResNet18 model, use the resulting model on each image, and
flatten the output, resulting in an embedding. For CEBaB, we use a pretrained BERT model [46] to
transform the input into embeddings. Specifically, using the transformers library [47], we create a
BERT model for sequence classification from the pretrained model ’bert-base-uncased’ with 13 labels
(1 per concept and 1 representing both tasks). Then, we fine-tune this model for 10 epochs with batch
size 2, 500 warm-up steps, weight decay 0.01 and 8 gradient accumulation steps. After training, we
use this model to transform each example into an embedding by outputting the last hidden states for
that example.
General training information
In all experiments, we use the AdamW optimizer. All neural
competitors are optimized to maximize the log-likelihood of the data with a weight on the likelihood
of the task (1 if not explicitly mentioned below). After training, for each neural model in CelebA,
CEBaB, MNIST+ and MNIST+∗, we restored the weights that resulted in the lowest validation loss.
In CUB, C-MNIST and the MNI

=== [PAGE 19] ===
into concept predictions. The embedding and the concepts are fed to the task predictor, which is a
Concept Reasoning Layer using the product-t norm and a temperature of 10.
For CBM+DT and CBM+XGboost, we train a CBM+MLP and use its concept predictions as training
input to the trees.
In the MNIST+ and MNIST+∗experiments, as mentioned earlier, we use as input the two images
instead of embeddings. Therefore, in all models, we use a CNN (trained jointly with the rest of the
model) to transform the images into an embedding. This CNN consists of a Conv2d layer with 6
output channels and kernel size 5, a MaxPool2d layer with kernel size and stride 2, ReLU activation,
a Conv2d layer with 16 output channels and kernel size 5, another MaxPool2d layer with kernel size
and stride 2, and another ReLU activation. The result is flattened, and a linear layer is used to output
an embedding per image (with size the same as the number of units in the hidden layers of the rest
of the models). This results in 2 embeddings (one per image), which are combined using 10 linear
layers, each with ReLU activation (except the last one) and again the same number of units (except
the first layer, which has 2*number of units). Additionally, the CNN can output concept predictions
by applying 3 linear layers with ReLU activation and a linear layer with a Softmax to each image
embedding. This results in 10 concept predictions per image, which are concatenated. The final
output of the CNN is the tuple of concept predictions and embedding.
In the C-MNIST experiments, the input is a single image. We use a similar CNN architecture as for
MNIST+ but with two differences. Firstly, the activation before the concept prediction is a Softmax
on only the first 10 concepts (related to the digits), while the activation on the last 2 concepts is
Sigmoid. Secondly, after the flattening operation, we use 3 linear layers with ReLU activation and 1
linear layer without activation to transform the embedding into a diff

=== [PAGE 20] ===
most 6 rules per task, with rule embeddings of size 1000. We use a β of 1, reset the selector every 40
epochs, and additionally put a weight on the concept reconstruction relative to the concept counts.
The CBM+MLP we train for CBM+DT and CBM+XGboost has a weight of 0.01 on the task.
Hyperparameter search
For CBM+DT, we tune the maximum depth parameter, trying all values
between 1 and nC, and report the results with the highest validation accuracy (training accuracy
in absence of a validation set). Parameters for the neural models were chosen that result in the
highest validation accuracy (training accuracy in absence of a validation set) (for CMR the β
parameter, rulebook size and rule embedding size were also chosen based on the learned rules).
For β, we searched within the grid [0.1, 1, 3, 4, 10, 30], for the embedding size within the grid
[10, 100, 300, 500, 1000], and for the rule embedding size within [100, 500, 1000]. For the deep
neural network’s number of hidden layers, we searched within the grid [2, 10, 20]. For unmentioned
parameters in the competitors, we used the default values.
Remaining setup of the rule intervention experiment
We first train for 300 epochs. Then, we
check in an automatic way for rules that differ from the ground truth. A representative example
is the rule y3 ←(c0,1) ∧(c1,2) ∧(c0,3) ∧(c1,0)5, which can be used by the selector for correctly
predicting that 1 + 2 = 3 and 3 + 0 = 3. After this, we add each missing ground truth rule, except
one, and let CMR continue training for 100 epochs. Consequently, CMR improves its originally
learned rules that differed from ground truth to the ground truth ones and learns to correctly select
between the learned and manually added rules. For instance, if CMR originally learned a rule
y3 ←(c0,1) ∧(c1,2) ∧(c0,3) ∧(c1,0), after manually adding y3 ←c0,1 ∧c1,2 and fine-tuning, the
rule improves to y3 ←c0,3 ∧c1,0, and CMR will no longer select this rule for the examples 3 + 0,
instead selecting the manua

=== [PAGE 21] ===
Table 5: Concept accuracies for all datasets.
MNIST+
MNIST+∗
C-MNIST
CELEBA
CUB
CEBAB
CBM+linear
99.76±0.02
99.73±0.02
99.72±0.03
87.63±0.02
86.24±0.05
92.69±0.07
CBM+MLP
99.75±0.05
99.71±0.06
99.80±0.02
87.68±0.02
86.32±0.08
92.67±0.07
CBM+DT
99.69±0.04
99.68±0.01
99.81±0.01
87.70±0.04
85.95±0.08
92.83±0.02
CBM+XGB
99.69±0.04
99.68±0.01
99.81±0.01
87.70±0.02
85.95±0.08
92.83±0.02
CEM
99.26±0.04
99.26±0.13
99.77±0.01
87.57±0.04
85.82±0.23
92.46±0.13
DCR
99.21±0.09
99.74±0.02
99.76±0.02
85.35±0.26
85.67±0.15
92.66±0.04
CMR (ours)
99.74±0.02
99.73±0.01
99.82±0.01
86.80±0.22
85.95±0.08
92.70±0.08
other rule learners could be used as well. We use the CEBaB dataset where we only take the first task
and the first 6 concepts.
In Figure 5, we compare CMR’s accuracy against 3 alternatives: (1) using a CBM+DT; (2) injecting
the rules learned by the CBM+DT for predicting a positive class into CMR while preventing CMR
from learning any rules itself; and (3) injecting these same rules but allowing CMR to learn an
additional 15 rules.
CMR’s rule learning component enhances accuracy. When CMR is allowed to learn rules (both
CMR (n=15) and DT →CMR (n=15)), its accuracy improves significantly compared to when only
the decision tree’s rules are used (DT →CMR (n=0)).
Injecting rules into CMR does not reduce accuracy when CMR is allowed to learn additional
rules. CMR achieves similar levels of accuracy in both cases where it only learns rules (CMR (n=15)
and when supplemented with the decision tree’s rules (DT →CMR (n=15)).
CMR’s rule selector enhances accuracy beyond the pre-obtained rules. Even when CMR is
restricted from learning any rules (DT →CMR (n=0)), it still performs better than the original
CBM+DT (DT) because of the rule selector.
For learning the decision tree (and the corresponding CBM used for predicting the concepts before-
hand), we used seed 1.
C.3.4
Accuracy robustness to the number of rules
In this experiment, we investigate the robustness of CMR’s accuracy with res

=== [PAGE 22] ===
CMR (n=15)
DT
DT 
 CMR (n=0)
DT 
 CMR (n=15)
Model
0.70
0.75
0.80
0.85
0.90
Task Test Accuracy
Figure 5: Rule interventions on CEBaB where we predict only the first task and employ 6 concepts.
We compare CMR’s accuracy with a CBM using a decision tree (DT) and CMR after adding the
decision tree’s rules to CMR’s memory (DT →CMR) without any additional learnable rules (n =
0) and when allowing 15 additional learnable rules (n = 15). The mean and standard deviation is
shown over 3 seeds. This means that (1) CMR’s end-to-end rule learning allows it to obtain higher
accuracy than when purely integrating pre-obtained rules from other rule learners (removing CMR’s
rule learning component), (2) just integrating pre-obtained rules in CMR (while still allowing more
rules to be learned) does not decrease its accuracy, and (3) using CMR with only pre-obtained rules
still surpasses the performance of the rules in isolation due to the selector.
5
10
15
20
25
Number of Rules
0.75
0.80
0.85
0.90
0.95
1.00
Task Test Accuracy
Mean Accuracy
Standard Deviation
Figure 6: Robustness of CMR’s accuracy w.r.t. the number of rules on CEBaB where we predict the
first task. The mean and standard deviation is shown over 3 seeds. High accuracy can be obtained
regardless.
C.3.5
Decoded rulebooks
In this section, we provide examples of the decoded rulebooks after training with seed 1 for our
experiments. We do not provide decoded rulebooks for MNIST+, as these rules always correspond to
the ground truth and are the same over all seeds, i.e. for every task i, CMR learns a rule per possible
pair of digits that correctly sums to that number i. An example of such a rule is y3 ←c1,2 ∧c2,1 ∧D
where D is a conjunction of nC −2 conjuncts, where each conjunct is the negation of a different
concept. In the tables in this section, we provide the decoded rulebooks for MNIST+∗, C-MNIST,
CelebA (for all concept subsets, but not the complete rulebook for nC = 37), CUB (not complete
either) and CEBaB. Importantly

=== [PAGE 23] ===
C-MNIST
CMR has learned that the concepts red and green are irrelevant for predicting even or
odd, learning the (same) ground truth rules for all seeds. We show these rules in Table 6.
MNIST+∗
For brevity, we drop the notation that shows whether the concept is related to the first
or second digit. Firstly, consider the tasks y0 and y1. For these tasks, we are essentially missing all
the concepts we need for the ground truth rules; we can only predict True by learning the rule that
is the negation of all concepts (in the used rule notation, the empty rule), which CMR does. Then,
to be able to predict False, it learns some arbitrary rules. Secondly, consider the tasks y2 to y10. As
we are missing the concepts for digits 0 and 1, we cannot learn all ground truth rules for these tasks.
For example, while CMR can (and does) still learn the rule y4 ←2 ∧2, it is impossible to learn a
rule like y4 ←1 ∧3. Instead, CMR learns the rule y4 ←3, which can be selected in case it needs to
predict that 1 + 3 = 4. Lastly, consider the tasks y11 to y18. CMR finds all ground truth rules, as they
can be found. We show the learned rules in Figure 7.
CEBaB
For predicting negative, CMR has learned as one of its rules the empty rule (rule 1),
signifying that the other rules (prototypes) it has learned are unable to predict True for all training
examples. Similarly, it has learned a rule that always evaluates to False (rules 5, 6 and 7, as food has
to be both good and bad). The remaining rules are prototypes. For instance, rule 2 signifies that for
some examples if food is bad and service is not good, even though ambiance might not be bad and
noise is unknown, the review is negative. Rule 3 signifies that even when food, ambiance and noise
are not bad, if service is not good, the review can be negative. We show the learned rules in Table 7.
CUB
We show some of the learned rules in Table 8 and some examples that satisfy these rules in
Figure 8.
CelebA
We show the learned rules in Tables 9, 10

=== [PAGE 24] ===
y0 ←
y0 ←3 ∧5
y0 ←6 ∧7 ∧8 ∧9
y0 ←7 ∧4
y0 ←8
y1 ←
y1 ←3
y1 ←9 ∧9
y2 ←
y2 ←2
y3 ←2
y3 ←3
y4 ←2 ∧2
y4 ←3
y4 ←4
y5 ←2 ∧3
y5 ←3 ∧2
y5 ←4
y5 ←5
y6 ←2 ∧4
y6 ←3 ∧3
y6 ←4 ∧2
y6 ←5
y6 ←6
(a)
y7 ←2 ∧5
y7 ←3 ∧4
y7 ←4 ∧3
y7 ←5 ∧2
y7 ←6
y7 ←7
y8 ←2 ∧6
y8 ←3 ∧5
y8 ←4 ∧4
y8 ←5 ∧3
y8 ←6 ∧2
y8 ←7
y8 ←8
y9 ←2 ∧7
y9 ←3 ∧6
y9 ←4 ∧5
y9 ←5 ∧4
y9 ←6 ∧3
y9 ←7 ∧2
y9 ←8
y9 ←9
y10 ←2 ∧8
y10 ←3 ∧7
y10 ←4 ∧6
(b)
y10 ←5 ∧5
y10 ←6 ∧4
y10 ←7 ∧3
y10 ←8 ∧2
y10 ←9
y11 ←2 ∧9
y11 ←3 ∧8
y11 ←4 ∧7
y11 ←5 ∧6
y11 ←6 ∧5
y11 ←7 ∧4
y11 ←8 ∧3
y11 ←9 ∧2
y12 ←3 ∧9
y12 ←4 ∧8
y12 ←5 ∧7
y12 ←6 ∧6
y12 ←7 ∧5
y12 ←8 ∧4
y12 ←9 ∧3
y13 ←4 ∧9
y13 ←5 ∧8
y13 ←6 ∧7
(c)
y13 ←7 ∧6
y13 ←8 ∧5
y13 ←9 ∧4
y14 ←5 ∧9
y14 ←6 ∧8
y14 ←7 ∧7
y14 ←8 ∧6
y14 ←9 ∧5
y15 ←6 ∧9
y15 ←7 ∧8
y15 ←8 ∧7
y15 ←9 ∧6
y16 ←7 ∧9
y16 ←8 ∧8
y16 ←9 ∧7
y17 ←8 ∧9
y17 ←9 ∧8
y18 ←9 ∧9
(d)
Figure 7: Rulebook for MNIST+∗.
Table 7: Rulebook for CEBAB.
(1)
negative ←
(2)
negative ←¬food_unknown∧food_bad∧¬food_good∧¬ambiance_bad∧¬service_good∧noise_unknown∧
¬noise_bad ∧¬noise_good
(3)
negative ←¬food_bad ∧¬ambiance_bad ∧¬service_good ∧noise_unknown ∧¬noise_bad ∧¬noise_good
(4)
negative ←¬food_unknown∧food_bad∧¬food_good∧¬ambiance_good∧¬service_good∧noise_unknown∧
¬noise_bad ∧¬noise_good
(5)
negative
←
food_unknown ∧food_bad ∧food_good ∧¬ambiance_unknown ∧ambiance_bad ∧
¬ambiance_good ∧service_unknown ∧service_bad ∧service_good ∧¬noise_unknown ∧¬noise_bad ∧noise_good
(6)
negative
←
food_unknown ∧food_bad ∧food_good ∧¬ambiance_unknown ∧ambiance_bad ∧
¬ambiance_good ∧service_unknown ∧service_bad ∧service_good ∧¬noise_unknown ∧noise_bad ∧noise_good
(7)
negative
←
food_unknown ∧food_bad ∧food_good ∧¬ambiance_unknown ∧¬ambiance_bad ∧
¬ambiance_good ∧service_unknown ∧service_bad ∧service_good ∧¬noise_unknown ∧¬noise_bad ∧noise_good
(8)
positive ←
(9)
positive ←¬food_bad ∧¬ambiance_bad ∧noise_unknown ∧¬noise_bad ∧¬noise_good
(10)
positive ←¬food_unknown ∧food_bad ∧¬food_good ∧¬service_good ∧noise_unknown ∧¬noise_bad ∧
¬noise_good
(11)
positive ←¬food_unknown ∧food_bad ∧¬food_go

=== [PAGE 25] ===
(a)
Laysan_Albatross ←underparts_white ∧breast_solid ∧breast_white ∧throat_white ∧
eye_black ∧bill_length_about_the_same_as_head ∧belly_white ∧back_solid ∧belly_solid ∧
(bill_hooked_seabird) ∧(wing_grey) ∧(wing_white) ∧(upperparts_grey) ∧(upperparts_black) ∧
(upperparts_white) ∧(...)
(b) Sooty_Albatross ←bill_hooked_seabird ∧wing_grey ∧upperparts_grey ∧breast_solid ∧
upper_tail_grey
∧
throat_black
∧
eye_black
∧
bill_length_about_the_same_as_head
∧
forehead_black ∧under_tail_grey ∧under_tail_black ∧size_medium_(9_16_in) ∧back_solid ∧
tail_solid ∧belly_solid ∧leg_grey ∧bill_black ∧crown_black ∧wing_solid
(c)
Brewer_Blackbird
←
wing_black ∧upperparts_black ∧breast_solid ∧eye_black ∧
under_tail_black∧belly_solid∧bill_black∧(bill_all_purpose)∧(wing_white)∧(underparts_black)∧
(back_black)
∧
(upper_tail_black)
∧
(head_plain)
∧
(breast_black)
∧
(throat_black)
∧
(bill_length_about_the_same_as_head) ∧(...)
Figure 8: Selection of training examples satisfying learned rules for CUB. For brevity, we drop some
of the irrelevant concepts and replace them with (...).
Table 9: Rulebook for CelebA (nC = 1) (seed 1).
(1)
Black_Hair ←
(2)
Black_Hair ←5_o_Clock_Shadow
(3)
Black_Hair ←¬5_o_Clock_Shadow
(4)
Male ←
(5)
Male ←5_o_Clock_Shadow
(6)
Male ←¬5_o_Clock_Shadow
(7)
W avy_Hair ←
(8)
W avy_Hair ←5_o_Clock_Shadow
(9)
W avy_Hair ←¬5_o_Clock_Shadow
25


=== [PAGE 26] ===
Table 10: Rulebook for CelebA (nC = 12).
(1)
Black_Hair ←5_o_Clock_Shadow ∧Arched_Eyebrows ∧Bags_Under_Eyes ∧Bald ∧Bangs ∧Big_Lips ∧
Big_Nose ∧Blond_Hair ∧Blurry ∧Brown_Hair ∧Bushy_Eyebrows ∧Chubby
(2)
Black_Hair
←
¬5_o_Clock_Shadow ∧¬Bags_Under_Eyes ∧¬Bald ∧¬Big_Lips ∧¬Big_Nose ∧
¬Bushy_Eyebrows ∧¬Chubby
(3)
Black_Hair ←¬Arched_Eyebrows ∧¬Blond_Hair ∧¬Blurry
(4)
Male ←5_o_Clock_Shadow ∧Arched_Eyebrows ∧Bags_Under_Eyes ∧Bald ∧Bangs ∧Big_Lips ∧
Big_Nose ∧Blond_Hair ∧Blurry ∧Brown_Hair ∧Bushy_Eyebrows ∧Chubby
(5)
Male ←¬5_o_Clock_Shadow ∧¬Bags_Under_Eyes∧¬Bald∧¬Blond_Hair ∧¬Blurry ∧¬Brown_Hair ∧
¬Chubby
(6)
Male ←¬Bald ∧¬Blond_Hair
(7)
W avy_Hair ←5_o_Clock_Shadow ∧Arched_Eyebrows ∧Bags_Under_Eyes ∧Bald ∧Bangs ∧Big_Lips ∧
Big_Nose ∧Blond_Hair ∧Blurry ∧Brown_Hair ∧Bushy_Eyebrows ∧Chubby
(8)
W avy_Hair ←¬5_o_Clock_Shadow ∧¬Bags_Under_Eyes ∧¬Bald ∧¬Big_Nose ∧¬Bushy_Eyebrows ∧
¬Chubby
(9)
W avy_Hair ←¬Arched_Eyebrows ∧¬Bald ∧¬Blond_Hair ∧¬Blurry ∧¬Chubby
Table 11: Rulebook for CelebA (nC = 37).
(1)
Black_Hair ←5_o_Clock_Shadow∧Arched_Eyebrows∧Bags_Under_Eyes∧Bald∧¬Big_Lips∧Chubby∧
¬Double_Chin ∧¬Eyeglasses ∧¬Gray_Hair ∧Heavy_Makeup ∧¬Mouth_Slightly_Open ∧¬No_Beard ∧
Oval_F ace ∧P ale_Skin ∧¬P ointy_Nose ∧Receding_Hairline ∧¬Rosy_Cheeks ∧¬W earing_Necklace ∧
¬W earing_Necktie
(2)
Black_Hair ←5_o_Clock_Shadow ∧Arched_Eyebrows ∧Bags_Under_Eyes ∧¬Bald ∧¬Big_Lips ∧
Big_Nose ∧Blond_Hair ∧¬Bushy_Eyebrows ∧Chubby ∧¬Double_Chin ∧Eyeglasses ∧Goatee ∧¬Gray_Hair ∧
Heavy_Makeup∧¬Mouth_Slightly_Open∧Mustache∧¬Narrow_Eyes∧¬No_Beard∧Oval_F ace∧P ale_Skin∧
¬P ointy_Nose ∧Receding_Hairline ∧Rosy_Cheeks ∧Straight_Hair ∧W earing_Hat ∧W earing_Lipstick ∧
W earing_Necklace ∧¬W earing_Necktie ∧Attractive ∧¬Y oung
(3)
Black_Hair
←
¬Arched_Eyebrows ∧¬Bangs ∧¬Blond_Hair ∧¬Blurry ∧¬Heavy_Makeup ∧
¬Narrow_Eyes ∧¬P ale_Skin ∧¬P ointy_Nose ∧¬Rosy_Cheeks ∧¬W earing_Earrings ∧¬W earing_Necklace
(4)
Black_Hair
←
¬Bald ∧¬Big_Lips ∧¬Blurry ∧¬Chubby ∧¬Double_Chin ∧¬Eyeglasses ∧
¬Goatee ∧¬Gray_Hair ∧¬Mustach

=== [PAGE 27] ===
D
Code, licenses and resources
For our experiments, we implemented the models in Python 3.11.5 using open source libraries.
This includes PyTorch v2.1.1 (BSD license) [48], PyTorch-Lightning v2.1.2 (Apache license 2.0),
scikit-learn v1.3.0 (BSD license) [49] and xgboost v2.0.3 (Apache license 2.0). We used CUDA
v12.4. Plots were made using Matplotlib v3.8.0 (BSD license) [50]. Our code is publicly available at
https://github.com/daviddebot/CMR under the Apache License, Version 2.0.
All datasets we used are freely available on the web with licenses:
• MNIST - CC BY-SA 3.0 DEED,
• CEBaB - CC BY 4.0 DEED,
• CUB - MIT License 6,
• CelebA - The CelebA dataset is available for non-commercial research purposes only7.
We will not further distribute them.
The experiments for MNIST+, MNIST+∗, C-MNIST, CelebA and CEBaB were run on a machine
with an NVIDIA GeForce GTX 1080 Ti, Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz with 128
GB RAM. The experiment for CUB and the fine-tuning of the BERT model used for the CEBaB
embeddings were run on a machine with i7-10750H CPU, 2.60GHz × 12, GeForce RTX 2060
GPU with 16 GB RAM. Table 12 shows the estimated total computation time for a single run per
experiment.
Table 12: Estimated total computation time for a single run of each experiment.
Experiment
Time (hours)
CelebA
3.7
CUB
1.8
CEBaB
0.1
MNIST+
4.1
MNIST+ rule int.
0.2
MNIST+∗
4.1
C-MNIST
4.4
6https://huggingface.co/datasets/cassiekang/cub200_dataset
7https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html
27


=== [PAGE 28] ===
NeurIPS Paper Checklist
1. Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The structure of our model (neural nets + interpretable memory), discussion
(expressivity, interpretability and verification) and experiments (research questions) closely
follow the claimed contributions in both abstract and introduction.
Guidelines:
• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
[Yes]
Justification: This is done in our conclusion (Section 8).
Guidelines:
• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empi

=== [PAGE 29] ===
Answer: [Yes]
Justification: Theorem 4.1 is proven in Section 4.1 and Theorem 5.1 is proven in Appendix
A.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: Limited details are given in Section 6, with the remaining details provided in
Appendix C.
Guidelines:
• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for o

=== [PAGE 30] ===
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The code is provided at https://github.com/daviddebot/CMR. The data
is publicly available on the internet, and we do not redistribute it. This is mentioned in the
main text.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to und

=== [PAGE 31] ===
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We reported the details of the machines exploited in the experiments in
Appendix D.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justifi

=== [PAGE 32] ===
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary s

=== [PAGE 33] ===
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: Documentation for the code is provided in the form of a README file in the
corresponding repository.
Guidelines:
• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in t

=== [PAGE 34] ===
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
34


=== SECTION MAP ===
{'Abstract': 'and', 'Introduction': 'accurately reflect the'}
=== TITLE ===
Interpretable Concept-Based Memory Reasoning


=== RAW TEXT DUMP ===
and
accurately reflect the
=== END RAW TEXT ===
--- spaCy not available, using regex fallback ---

=== CANDIDATE CONCEPTS ===
=== END CANDIDATES ===

!!! No concepts extracted by main logic. Fallback to most frequent 5+ char words.
=== FINAL CONCEPTS (2) ===
{'name': 'accurately', 'frequency': 1, 'sections': 'FullText', 'boosted': False, 'title_match': False, 'sources': 'fallback', 'context_snippet': ''}
{'name': 'reflect', 'frequency': 1, 'sections': 'FullText', 'boosted': False, 'title_match': False, 'sources': 'fallback', 'context_snippet': ''}
=== END FINAL CONCEPTS ===

