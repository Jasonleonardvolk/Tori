Implementation-Ready Starter Modules for TORI Cognitive Blanket Architecture

TORI Cognitive Blanket: Module Implementation Plan
Multi-Scale Hierarchy
File Path: src/lib/hierarchy/MultiScaleHierarchy.ts
Purpose: Implements a hierarchical knowledge structure that organizes content at multiple scales (paragraph, section, document). This module breaks documents into smaller units, generates summaries for larger units (sections, entire documents), and enables tiered context retrieval. By maintaining embeddings for each node of the hierarchy, the system can retrieve relevant context at the appropriate scale for a query, improving both performance and accuracy in long-document understanding. All processing is local, using on-disk files for storage of parsed content and summaries (no external databases or APIs).
Key Functions/Classes:
class HierarchyManager – Central class managing the hierarchy tree. Provides methods to add documents and query the hierarchy. It stores nodes in a local file cache (e.g. JSON files) to persist the structure. Each node in the tree (paragraph, section, document) may include text, an embedding vector, and an optional summary.
HierarchyManager.addDocument(docPath: string): Promise<void> – Reads a document (e.g. PDF or text) from the local filesystem, splits it into sections and paragraphs, computes embeddings for each paragraph (using the local @xenova/transformers embedding model), and stores the structure. It also generates summaries for each section and the whole document (e.g. using a local summarization pipeline like Xenova/t5-small for efficiency) and caches these summaries
medium.com
.
HierarchyManager.getSummary(nodeId: string): string – Returns a stored summary for a given section or document node. If a summary isn’t already computed, it uses the local transformer to generate one on the fly and then caches it (e.g. saving to a .summary file on disk).
HierarchyManager.retrieveContext(query: string, level?: 'paragraph' | 'section' | 'document'): ContextSnippet[] – Performs a tiered retrieval for a query. It uses the embedding index to find relevant paragraphs first. If deeper context is needed or if level is higher (section or document), it can retrieve the pre-computed section summaries or document summaries. This ensures that a small query first searches fine-grained units (paragraphs), but for broader or ambiguous queries it can escalate to section or document-level context.
(Internal) function computeEmbedding(text: string): number[] – Utility that wraps the @xenova/transformers embedding model to produce a vector for the given text. Used for similarity search in retrieval. Could be initialized once and reused for performance.
Integration Notes: This module integrates with the ingestion pipeline and query engine. When new documents are added via the pipeline, call HierarchyManager.addDocument() to ingest and index the content hierarchically. The resulting structure (and embeddings) are stored on the local filesystem (for example, under a data/hierarchy/ folder) for persistence and quick reload on startup. The query or reasoning system uses retrieveContext() to fetch supporting context for answering questions, pulling in paragraph-level details or higher-level summaries as needed. This module should work with the Background Orchestrator to periodically rebuild or optimize the hierarchy (e.g. re-summarize sections if content updates) and can send alerts via the WebSocket system if a large document is processed or when summaries are updated (e.g. to notify the UI that a new summary is ready). Being local and modular, the HierarchyManager avoids external calls – all summarization and embedding computations use in-house models, ensuring data privacy and low latency.
Advanced Wormhole Formation
File Path: src/lib/graph/WormholeManager.ts
Purpose: Manages dynamic “wormholes” – direct semantic links between concept nodes in the knowledge graph – to enable quick traversal between related ideas. This module creates and prunes relationships based on semantic similarity and usage patterns. By analyzing embeddings of concepts and content usage, it forms high-relevance shortcuts (“wormholes”) in the graph that connect distant but related nodes. The goal is to improve knowledge retrieval and reasoning speed by skipping through intermediate layers when appropriate. All operations run locally, storing the graph and link data on disk (e.g. as JSON or a lightweight graph file) so that no external database is needed.
Key Functions/Classes:
class WormholeManager – Maintains the concept graph’s special wormhole links. It likely holds an in-memory graph structure of concepts (could reuse an existing ConceptGraph structure if present) and monitors similarity and usage. On initialization, it can load existing wormhole links from a local file (e.g. data/graph/wormholes.json).
WormholeManager.createWormhole(a: ConceptNode, b: ConceptNode): void – Creates a bidirectional semantic link between two concept nodes if they are highly related. This might be triggered when a new concept is added or when periodic analysis finds two existing nodes with embedding similarity above a threshold. It records the link in the concept graph data structure and persists the update to disk.
WormholeManager.findSimilarNodes(node: ConceptNode, topN: number): ConceptNode[] – Utility to query the embedding index for nearest neighbors of a given concept. Used to suggest potential wormholes. It uses the same embedding from @xenova/transformers as other modules, ensuring consistency in vector space.
WormholeManager.recordUsage(path: ConceptNode[]): void – Should be called whenever a “wormhole” link is traversed during a query or reasoning session. It updates usage counters for the involved link or nodes. This data (stored in memory and flushed to a local file periodically) allows the system to know which connections are frequently helpful.
WormholeManager.pruneWormholes(): void – Scans wormhole links and removes those that are no longer useful. For example, if a link hasn’t been used in a long time or if the underlying concepts have drifted (their embeddings changed after learning new information), the link is removed from the graph. Pruning criteria might include low usage frequency or an updated similarity score falling below a threshold. Removing a wormhole means deleting that edge from the graph and updating the on-disk storage.
Integration Notes: The WormholeManager ties into concept ingestion and query processing. After the HierarchyManager or other ingestion processes add new concepts/nodes to the knowledge base, WormholeManager.findSimilarNodes() is invoked to identify if the new node should be linked to existing ones via a wormhole. During query answering or reasoning, if the system jumps from one concept to another via a wormhole, recordUsage is called so that frequently used shortcuts are kept. This module works with the Background Orchestrator to periodically run pruneWormholes() (and possibly a routine to discover new wormholes in idle time). It may also subscribe to events like “newConceptAdded” or “conceptUpdated” to evaluate link creation. Alerts or logs can be sent through the WebSocket/alert system whenever a new wormhole is created or an old one pruned (for transparency or debugging). The entire implementation is self-contained – relying on in-process memory and local files for persistence – and optimized by using cached embeddings and incremental graph updates to handle potentially large concept networks efficiently.
Braid Memory
File Path: src/lib/memory/BraidMemory.ts
Purpose: Implements a causal and temporal memory system that “braids” together the narrative of information – tracking cause-effect relationships, temporal sequences, and reasoning paths. Braid Memory extracts causal links from text (e.g. X causes Y), keeps track of events along timelines, and records the paths taken during reasoning or problem-solving. This creates a richer context for the knowledge base, enabling the system to answer "why" and "how" questions and to understand the chronology of events. All data is stored locally (for example, maintaining a file of causal relations and a timeline of events), ensuring no external data storage is needed.
Key Functions/Classes:
class BraidMemory – Central class managing causal and temporal knowledge. It might contain sub-structures like a causal graph (nodes and directed edges for cause→effect) and a timeline of events. On initialization, it loads any saved memory state from disk (e.g. data/memory/causals.json and timeline.json).
BraidMemory.extractCausalRelations(text: string, sourceId: string): CausalLink[] – Analyzes a piece of text (such as a paragraph or answer) to identify causal statements (e.g. “because”, “leads to”, “due to”). It returns a list of CausalLink objects (each could be {cause: string, effect: string, context: sourceId}). Internally, this could use simple patterns or a local NLP model for causal extraction. The results are stored in the causal graph, linking the concept nodes representing those causes and effects.
BraidMemory.extractTemporalEvents(text: string, sourceId: string): TimeEvent[] – Extracts temporal information (dates, times, sequence of events) from text. Returns structured events (e.g. {time: Date|Order, event: string, context: sourceId}). These events are added to a timeline data structure, which can be used to answer questions about "when" something happened or the order of events.
BraidMemory.recordReasoningPath(questionId: string, path: ConceptNode[]): void – Stores the sequence of concepts or intermediate conclusions that were used to arrive at an answer for a given question or task. This “reasoning path” is saved (e.g. in data/memory/paths.json) so that the system can trace back its steps later. It’s crucial for explainability – if asked how an answer was derived, the system can retrieve this path.
BraidMemory.queryWhy(conceptId: string): string[] – Looks up known causes or reasons for a given concept/event. For example, if the user asks "Why did X happen?", this function searches the causal graph for edges leading to X and returns any recorded causes (possibly with source references from the stored context).
BraidMemory.queryWhen(conceptId: string): string – Retrieves temporal information for a given concept or event (e.g. the date it occurred or its position in a sequence) from the timeline data.
Integration Notes: BraidMemory works alongside the main question-answering and ingestion pipeline. After the HierarchyManager processes text, the pipeline should call extractCausalRelations and extractTemporalEvents on that text to enrich the knowledge base with causal and temporal links. These extracted relations can also be added as special edges in the global concept graph (for instance, a "CAUSES" edge between concept nodes, managed by this module). When the system answers a question, it uses recordReasoningPath to log how it got the answer. If the user or a process later needs an explanation, BraidMemory’s stored paths can be retrieved to explain the reasoning (possibly presented via the WebSocket alert system or UI). This module does not rely on external services – all NLP is done via local methods or transformer models (for example, a small local language model could assist in complex causal extraction). The data is stored in local JSON files for persistence. Performance considerations: BraidMemory should use efficient lookups for the causal graph (e.g. indexing by effect and cause for quick “why” queries). It is designed to be extensible – one could plug in more advanced NLP for causal extraction or integrate with the concept graph – without affecting other parts of the system.
Multimodal Integration
File Path: src/lib/multimodal/MultimodalIntegration.ts
Purpose: Allows the cognitive blanket to incorporate and understand images, audio, and video alongside text. This module extracts concepts and features from non-text media and links them to the text-based knowledge graph, creating a unified schema for all modalities. By doing so, the system can answer questions or draw inferences using information across text and media (e.g. recognizing an image contains a certain object that’s also mentioned in text). All processing uses local resources – for example, using on-device models for image recognition or audio transcription (no external APIs) – and intermediate data (like extracted labels or transcripts) are stored on the file system (e.g. under data/multimodal/).
Key Functions/Classes:
class MultimodalIntegrator – Coordinates processing of different media types and manages the cross-modal concept links. On initialization, it may load any cached media analysis results from disk so as not to reprocess the same file twice (for performance).
MultimodalIntegrator.processImage(filePath: string, contextId: string): string[] – Analyzes an image file (from local storage) and returns a list of extracted concept labels or descriptions. For example, using a local vision transformer or CNN model (via @xenova/transformers if available, or another in-house model) it might identify objects/scenes in the image (e.g. ["cat", "grass", "sunset"]). Each label can be linked to or created as a ConceptNode in the knowledge base. The contextId (e.g. an ID of a document or section the image is part of) helps tie the extracted concepts back to that context. Results are cached (perhaps saving labels in a JSON sidecar file next to the image).
MultimodalIntegrator.processAudio(filePath: string, contextId: string): string – Transcribes an audio file to text (using a local speech-to-text model, e.g. Whisper small running locally) and returns the transcript. Then it can perform concept extraction on the transcript (possibly using the HierarchyManager or a simpler keyword extractor) to get key concepts, which are linked into the knowledge graph. The raw transcript might be saved to a .txt file for reference.
MultimodalIntegrator.processVideo(filePath: string, contextId: string): { transcript: string, keyFrames: string[] } – Processes a video by extracting audio (then calling processAudio internally for speech) and perhaps sampling key frames for image analysis. It returns a transcript of speech and a list of key frame image labels. For each key frame image (captured every N seconds or at scene changes), it uses processImage logic to identify visual concepts. The result is a set of multimodal concepts tied to time codes in the video.
MultimodalIntegrator.linkConceptsAcrossModalities(concepts: string[], sourceId: string): void – Takes a list of concept labels extracted from media and ensures they are linked to matching text-based concepts in the knowledge graph. For example, if an image yielded "Eiffel Tower" and a text document mentions "Eiffel Tower", this function will link the image’s concept node to the existing text concept node (or create a unified node) rather than treating them separately. It may do a lookup in the concept graph for each label (using string matching or embedding-based similarity for less exact matches). New links or merged nodes are saved to the knowledge base on disk.
Integration Notes: This module plugs into the content ingestion pipeline whenever a non-text media file is encountered. For instance, if a PDF with embedded images is ingested, the pipeline can call processImage for each image and then use linkConceptsAcrossModalities to integrate those results with the text content. Similarly, adding an audio recording or video to the system will be handled by this module to extract transcripts and visuals. The HierarchyManager can incorporate transcripts as pseudo-documents (sections of text) so that summarization and retrieval can treat them like normal text. The extracted visual concepts are essentially additional nodes/edges in the concept graph, managed by the WormholeManager and Concept Graph Optimizer for any further linking or merging. All computation is local – e.g. image analysis might use a small pre-trained vision model loaded via xenova/transformers or another library, and audio transcription uses a local model – adhering to the no-cloud constraint. By unifying modalities, the system can, for example, alert via WebSocket when a new media file’s content is available (like “Image X analyzed, found: cat, tree”), enhancing extensibility. The design is modular so that improved models (for vision or audio) can be swapped in easily, and all intermediate data is cached on disk to avoid repeat heavy computation.
Concept Fuzzing and Adaptive Reorganization
File Path: src/lib/graph/ConceptGraphOptimizer.ts
Purpose: Oversees continuous semantic space exploration and optimization of the concept graph. This module “fuzzes” concepts by exploring slight variations or adjacent meanings to discover hidden connections, merges duplicate or highly similar concepts, and compresses subgraphs to prevent the knowledge base from bloating or fragmenting. Essentially, it keeps the concept network well-organized and adaptable as new information comes in. By adaptively reorganizing, the system maintains performance (fewer, more general nodes when appropriate) and improves inference (by merging synonymous concepts). Everything happens in-house: the optimizer reads and writes the concept graph stored on the local filesystem (no external databases) and uses local embedding computations for semantic similarity checks.
Key Functions/Classes:
class ConceptGraphOptimizer – Manages the optimization routines on the concept graph. It likely works closely with or as part of the concept graph data structure, accessing all concept nodes and their embeddings/relations. Typically invoked as a background service since these operations can be periodic and asynchronous.
ConceptGraphOptimizer.fuzzConceptSpace(baseConceptId: string, options?: FuzzOptions): string[] – Explores the neighborhood of a given concept in the embedding vector space. For example, it might add small random perturbations to the concept’s embedding or use alternative phrasings (via a local language model) to find related terms. It returns a list of new potential concept labels or synonyms. These suggestions can then be evaluated – if they aren’t already in the graph, the system might add them (with links to the base concept). This helps discover latent knowledge or connect concepts that haven’t been explicitly linked.
ConceptGraphOptimizer.detectMergeCandidates(): ConceptNode[][] – Scans the entire concept graph for nodes that are very similar (e.g. high cosine similarity in embeddings or identical meaning). It returns sets of concept nodes that should be merged. For instance, "NASA" and "National Aeronautics and Space Administration" might both exist – this function would identify them as a merge candidate.
ConceptGraphOptimizer.mergeConcepts(candidateGroup: ConceptNode[]): ConceptNode – Takes a group of equivalent/very similar concept nodes and merges them into a single representative node. It updates all references (edges/wormholes) so that they now point to the merged node, and deletes or archives the redundant nodes. The merged concept carries aliases (maybe storing the other labels for reference). All changes are written back to the graph storage on disk.
ConceptGraphOptimizer.compressSubgraph(rootId: string, threshold: number): void – Traverses a subgraph of the knowledge base (e.g. all concepts under a certain domain or a heavily interconnected cluster) and compresses it if it’s too dense or large. “Compression” could mean elevating common properties to a parent node or condensing rarely used nodes. For example, if there are 50 very specific concepts all linked to each other, the optimizer might create a new abstract concept to group them, link that parent in place, and mark the specifics as children or aliases, reducing the number of edges. The threshold might determine how aggressively to compress (e.g. if a subgraph has more than N nodes or average degree above a certain number).
(Internal) ConceptGraphOptimizer.updateEmbedding(conceptId: string): void – If concept nodes are merged or significantly changed, their text representation may change. This function recomputes the embedding using the local model to keep the embedding index up to date. It’s called internally after merges or major edits.
Integration Notes: The optimizer is intended to run periodically or when triggered by certain events (like after a large ingestion of new data). The Background Orchestrator will schedule detectMergeCandidates() and subgraph compression runs during low-load times. When mergeConcepts or compressSubgraph executes, it should coordinate with WormholeManager (e.g. to update or remove wormholes involving merged nodes) and with HierarchyManager/BraidMemory if the structural changes affect stored summaries or causal links. This module can emit events or logs (through the alert system) when significant changes occur – for example, “Merged concepts: X, Y -> Z” or “Compressed subgraph under ‘Astronomy’ concept”. This helps developers or even the system UI to keep track of how the knowledge graph is evolving. By keeping the graph trimmed and well-structured, memory usage stays efficient and query performance remains high. The design is extensible: new strategies for semantic clustering or dimensionality reduction can be added to the optimizer without impacting other modules, since it acts on the graph in a contained manner. All operations respect the in-house constraint (e.g. using the same local embedding model and storing results on disk).
Background Jobs and Orchestration
File Path: src/lib/system/BackgroundOrchestrator.ts
Purpose: Coordinates periodic tasks and maintenance jobs that keep the TORI system running optimally. This module acts as a scheduler and orchestrator for background processes like discovery (e.g. finding new wormholes), optimization (graph cleanup, re-embedding), and routine checks. It ensures that heavy tasks are done outside of real-time user queries, and provides hooks for monitoring and alerts. The orchestrator uses only local timing/scheduling (e.g. Node’s setInterval or a simple cron-like scheduler library) and leverages the existing WebSocket/alert system to notify about job status or important events. No external cloud cron or job service is used.
Key Functions/Classes:
class BackgroundOrchestrator – Singleton-like class that initializes all scheduled jobs on server startup. It holds references to other module instances (HierarchyManager, WormholeManager, BraidMemory, MultimodalIntegrator, ConceptGraphOptimizer) to invoke their maintenance methods.
BackgroundOrchestrator.scheduleTask(name: string, intervalMs: number, taskFn: () => Promise<void>): void – Schedules a recurring task with a given interval. It uses Node’s timers to call taskFn periodically. For example, a task might be “Recompute section summaries daily” or “Prune wormholes every hour”. Task definitions are likely configured in a list and registered on init.
BackgroundOrchestrator.runImmediate(taskFn: () => Promise<void>): void – Utility to run a maintenance task on-demand (for instance, triggered by a manual admin action or a specific event). This executes the function outside the normal schedule (could simply spawn a detached promise).
Predefined tasks (likely set up in the constructor or an init() method):
Periodic Similarity Scan: Uses WormholeManager.findSimilarNodes on new or randomly selected concepts to discover new wormhole candidates. Runs, say, every N minutes. If new wormholes are created, it logs or alerts the event.
Usage Pruning: Calls WormholeManager.pruneWormholes() every hour or at midnight to remove stale links.
Graph Optimization: Triggers ConceptGraphOptimizer.detectMergeCandidates() and compressSubgraph perhaps once a day or when the number of concepts grows beyond a threshold. Merges are applied and saved if found.
Memory Maintenance: Calls BraidMemory routines to maybe compact the causal graph or remove obsolete reasoning paths (e.g. for old queries) on a schedule. Also could periodically save in-memory structures to disk as a checkpoint.
Summarization Update: Checks if any document or section in HierarchyManager lacks a summary or if a summary is outdated (perhaps the document was appended to), and uses the summarization pipeline to update it. This could run in off-peak hours.
BackgroundOrchestrator.monitorHooks(): void – Sets up listeners for system events (like a new document added, or an error in a module). For example, if a new media file arrives, the orchestrator might immediately trigger MultimodalIntegrator.processImage/Audio/Video rather than waiting for the next interval. It also listens for error events or exceptions in background tasks; if something fails or takes too long, it can send an alert via the WebSocket system to inform developers/admins.
Integration Notes: The BackgroundOrchestrator should be initialized when the server starts (for instance, in the SvelteKit server entry or a Node startup script). It will then schedule all tasks. It uses the existing WebSocket/alert systems to emit logs and alerts – for example, using a global AlertService.send(message) or directly accessing a WebSocket instance to broadcast messages like “Maintenance complete: 5 wormholes pruned”. Each scheduled task is careful to run in isolation (e.g. using try/catch and not blocking the main event loop for too long). Since no external job runner is used, the orchestrator must efficiently manage timings within Node’s single process – using asynchronous operations and avoiding overlap of heavy jobs. It’s advisable that each module’s maintenance functions (like pruning or merging) are themselves optimized (possibly processing in batches) so they don't freeze the app. The orchestrator can stagger tasks (not all running at the same exact interval) to balance load. This module makes the system extensible: new background tasks can be added easily by registering a new schedule in one place. It strictly confines to local resources – any state needed (like last run times or logs) can be stored in memory or a small file. By concentrating the scheduling logic here, other modules remain focused on their domain logic and performance-critical code, while the orchestrator ensures everything gets a turn for upkeep.