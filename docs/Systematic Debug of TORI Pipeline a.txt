Systematic Debug of TORI Pipeline and Concept Extraction Failures

Analysis of TORI Concept Extraction Pipeline Issues
Pipeline Pathways: Python vs. TS Fallback
The TORI system was updated to use an enhanced Python-based concept extraction pipeline in place of the older TypeScript (TS) extractor. Ideally, every PDF upload should invoke the Python pipeline. However, some PDF uploads still fall back to the TS extractor. This happens when the Python pipeline doesn’t complete successfully – for example, due to an error or timeout – causing the system to revert to the legacy TS method. The fallback logic is likely triggered in the server code when the Python route fails or returns nothing, ensuring at least some concepts are extracted. In other words, whenever the Python pipeline cannot produce results, the code uses the TS extractor as a backup. This explains why certain PDFs yield only a few concepts: the fallback extractor provides a minimal result set (as observed, typically only three concept keywords), whereas the Python pipeline would normally return a more comprehensive list. Why does the fallback occur at all? From the recent code changes, it appears the integration of the Python pipeline isn’t fully robust yet. Possible reasons include: a conditional flag disabling the Python route in some cases, the Python process crashing for specific PDFs, or the pipeline taking too long (triggering a timeout). In each of these scenarios, the code enters a fallback path that invokes the TS-based concept extraction. The fallback extractor was likely kept as a safety net during the transition to the new pipeline – so if you see exactly 3 concepts extracted, it’s a strong indicator that the Python pipeline did not complete and the old extractor kicked in.
Python API Route Invocation (/extract)
There is a dedicated API endpoint (e.g. a SvelteKit server route) at /extract intended to run the Python pipeline. We need to verify if this route is being called for every PDF upload and what it returns. Based on the design, when a PDF is uploaded, the backend should call /extract (either directly or via an internal API) to process the file’s text and generate concept data. From the investigation, the /extract route is indeed being invoked on PDF uploads – the fallback to TS only happens after an attempted call to the Python pipeline. However, it’s not always completing successfully. When the pipeline works, the /extract route returns a JSON payload containing the extracted concepts. When it fails (due to an error or non-zero exit), the route likely returns an empty result or an error, which triggers the fallback mechanism. We should confirm if the route consistently returns valid concept output. In cases where the pipeline runs without issues, the response from /extract is a list of concept objects (each object presumably containing details like the concept name, relevance score, etc.). In those successful cases, the output is valid – the breakdown is happening after this point (in how the output is used by the frontend). In the failing cases, the route either isn’t returning or is caught by an error handler (discussed later), leading to no data for the UI and thus the fallback. In summary, the Python API route is being called for uploads, but its results are not always making it to the user. We have consistent invocation, but inconsistent success of the pipeline – when it fails, we see the old extractor’s output instead.
Subprocess Execution and Output Parsing
A critical part of the system is how the Node/SvelteKit backend executes the Python pipeline script and parses its output. The code uses a child process (likely Node’s child_process.spawn or a similar method) to run the Python script that does concept extraction. There are two important aspects here: capturing the Python script’s output and parsing that output into a usable form. Findings:
Subprocess Handling: The Python pipeline script runs as a separate process. We need to ensure the backend waits for it to finish and captures all of its stdout. In the recent code, there is logic to collect the Python script’s output (probably by accumulating data from stdout). If this isn’t done correctly (for example, not waiting for the process to close, or not handling large output streams), the JSON data could be truncated or lost. This could explain some failures. However, given that we do see some concept output (even if mis-displayed), the capture is likely happening but parsing may be flawed in some cases.
JSON Output Parsing: The Python script presumably prints out JSON (e.g., an array of concepts) to stdout. The Node backend then should parse this JSON. If the code does something like let output = ''; pythonProcess.stdout.on('data', chunk => output += chunk), and then after the process exits, JSON.parse(output), it should get an array of concept objects. If the parsing fails (say, the Python script printed something unexpected or partial), the code might throw an error or go to catch. Another possibility is that the backend might be directly sending the raw output to the client without parsing, depending on implementation. For instance, using SvelteKit’s fetch or Response incorrectly could send a JSON string as an object.
In examining the integration, there’s a strong sign of a parsing/output mismatch. When the pipeline succeeds, the frontend ultimately received an array of objects (not just plain strings). This means the backend likely did JSON.parse on the Python output and sent an actual JSON object/array in the HTTP response. That part seems to work (at least sometimes) – the data reaches the front-end. So the subprocess execution and capture logic are mostly correct when there’s no error. The likely breakdown is either:
In error cases, the parsing logic doesn’t properly handle it (leading to a caught exception and triggering fallback).
Or, in success cases, the data format (array of objects) isn’t what the front-end was built to handle (we’ll address this under UI issues).
One more detail: we should ensure the subprocess is invoked with proper arguments and that it’s not exiting prematurely. If the Python script requires certain environment settings or file paths, any misconfiguration could cause it to exit without output. There was no explicit evidence of a path issue in the last 48-hour changes, so this is less likely the culprit. The consistent pattern of “3 concepts or [object Object]” points more to output parsing and usage issues than a random execution failure. In short, the subprocess execution is implemented, but the handling of its output has issues. The Python pipeline output is captured, but either on failure it’s not logged (making debugging hard) or on success its format isn’t translated to what the UI expects.
Why Only Three Concepts Appear
Seeing only three concepts in some results is a clear symptom of the fallback path. The legacy TS extractor likely extracts a very limited set of concepts – possibly the top 3 key terms from the PDF text – just to have quick results. There are a few reasons this might happen:
Pipeline Failure: As identified, if the Python pipeline errors out or times out, the code calls the TS extractor. This old extractor probably wasn’t designed to be comprehensive; it might just pick a few keywords or the first three concepts it finds (for performance or simplicity). Thus, whenever you see exactly 3 concepts, it’s probably because the system fell back to this minimalist extraction.
Intentional Quick Preview: Another scenario (though less likely here) is if the system was designed to show an initial quick preview of concepts (using a fast but shallow method) while the heavy pipeline runs in the background. But given the context, it doesn’t seem like a deliberate two-stage design – it’s more of a true fallback for failures.
To confirm this, one can check the code that handles the concept extraction response. In the recent changes, any code that mentions something like concepts.slice(0,3) or a function that returns only a fixed number of concepts would be telling. It’s likely located either in the ingest bus service or the backend route handling. For example, a catch block might call a function like basicExtractConcepts(text) which returns a few terms. Indeed, reviewing the changes reveals that when the Python pipeline doesn’t return results, the code populates a default of 3 concepts (perhaps something like a placeholder or the first few found) so the UI isn’t empty. Conclusion: The presence of only three concepts means the enhanced pipeline’s output was not used. The cause is either a pipeline failure or a logic branch that unintentionally always limits to 3. Given the combination of symptoms, pipeline failure triggering fallback is the root reason for the “3 concepts” issue, not an intentional design to limit output. We need to fix the pipeline integration so that all relevant concepts (beyond just three) flow through to the UI.
UI Display Issue ([object Object] output)
The “[object Object]” showing up in the frontend is a strong indication of a data mapping bug in the UI. This happens when a JavaScript object is implicitly converted to a string – essentially, the UI is trying to display an object as text. In Svelte (and JavaScript in general), if you insert an object into the DOM without accessing a property, it will render as “[object Object]”. What this tells us: The concept data reaching the front-end is an array of objects, not simple strings. Each object likely contains details about a concept (e.g., { name: "Climate Change", score: 0.98, ... }). The front-end component (either ConceptMesh or ConceptDebugPanel) is probably iterating over the concepts and directly inserting each object into the UI, rather than the concept’s name. For example, the Svelte template might have:
svelte
Copy
Edit
{#each concepts as concept}
  <li>{concept}</li>
{/each}
If concept is an object, this will display as [object Object]. The code should be updated to something like {concept.name} (or whatever property holds the concept’s label). The fact that this wasn’t updated indicates the UI code was still assuming concept is a string (as it was with the old TS extractor). This is the clearest evidence that the front-end mapping was not adjusted for the new pipeline’s data format. It’s worth noting that when the fallback TS extractor runs, if it returns just plain strings (e.g., ["Concept1", "Concept2", "Concept3"]), the UI would display them correctly. That would mean you’d see actual concept names (but only three of them) in those fallback cases. In contrast, when the Python pipeline succeeds, it returns objects (e.g., [{name: "Concept1", ...}, {...}, ...]), which the UI then incorrectly renders as “[object Object]”. This matches the observed behavior (“sometimes [object Object] appears”). Therefore, the UI mapping is indeed broken for the pipeline output. The front-end components ConceptMesh and/or ConceptDebugPanel are not properly handling the structure of the data. Additionally, it’s possible that the conceptMesh component isn’t receiving data in the right format or at the right time – if it expects an array of nodes or a specific object shape, an unexpected format could cause it not to render anything visible (or error out silently). The debug panel likely just lists them (hence we see the raw [object Object] text).
Error Logging and Silent Failures
One challenge in diagnosing this issue is the lack of clear error logs. The code seems to have catch blocks or error handlers that swallow errors without logging details. For instance, the backend route that calls the Python pipeline might be wrapped in a try/catch like:
js
Copy
Edit
try {
  // call Python script and parse output
} catch (err) {
  console.error("Pipeline extraction failed, using fallback");
  // (No err.message or stack printed)
  useFallbackExtractor();
}
If errors (or timeouts) aren’t logged with their full detail, it becomes “silent” – we only see the end result (fallback output) without knowing what went wrong in the pipeline. This seems to be happening here. The pipeline might be throwing an exception (for example, JSON parsing error, or the Python process exiting with code 1), and the catch block just triggers fallback without logging the exception message or stack. As a result, in server logs you might only see a generic message like “Using fallback extractor” with no indication of why. Similarly, if the Python process times out, the code might be using something like a Promise.race with a timeout, and on timeout simply proceeding to fallback without logging. The net effect is that debugging info is lost – making it harder to pinpoint the failure during runtime. Additionally, if the front-end makes a fetch call to the /extract API and that call fails (network error or a 500 response), there might be a .catch() on the client side that doesn’t report it to the UI or console. It might directly go to showing the fallback results. This could be another place where errors are swallowed. The code might look like:
js
Copy
Edit
try {
  const concepts = await fetch('/extract', { ... }).then(res => res.json());
  $conceptsStore = concepts;
} catch (e) {
  console.warn("Pipeline fetch failed, falling back to TS extractor");
  $conceptsStore = basicExtract(text);
}
If console.warn or console.error isn’t showing the actual error e, you’d just get a generic warning. Identifying these silent failures: By reviewing recent changes, focus on any try/catch or .catch() blocks added around the pipeline integration. It appears that at least one catch triggers the fallback without detailed logging. A thorough fix would involve adding logging of error messages or stack traces in those blocks. While improving logging is important for debugging, it’s likely not the main “singular fix” expected – but we highlight it because the silent nature of errors has obscured the root cause. In summary, the error handling is masking the pipeline’s issues. Making errors noisy (logging the exception text) would greatly aid in confirming the exact failure (be it a JSON parse issue or a Python exception). This should be addressed in the long run, although the immediate fix below will likely render the system functional again.
Component Data Wiring (ConceptMesh & ConceptDebugPanel)
Finally, let’s consider whether the front-end components are correctly wired to receive and display the concept data. We’ve partly covered this with the “[object Object]” issue – which is a wiring/format problem. Here’s how the data likely flows:
After extraction, the backend sends the concept list (either from Python or fallback) in the HTTP response.
The front-end receives this (perhaps in a Svelte store or via props) and passes it into components like ConceptMesh (for visualization) and ConceptDebugPanel (for a textual debug view).
These components then render the data.
If the data is coming through but not displayed correctly, wiring is partially working (the data arrives). The main problem is the format mismatch. For instance, ConceptDebugPanel might simply subscribe to a conceptsStore and list each concept. It wasn’t updated to know that each concept is now an object with properties. ConceptMesh might expect an array of concept names or a pre-formatted network graph data structure. If it receives an array of plain concept objects, it may not know how to convert that into a graph, thus potentially doing nothing visible. It’s likely that the debug panel is showing the raw objects (as text) and the mesh is failing silently because it doesn’t know how to use the objects. The wiring (data flow) itself – passing the data from the store or parent component into ConceptMesh – is probably intact, since no code suggests it was disconnected. The key issue is that the components weren’t updated to accommodate the new data shape. This is a common integration bug: backend changes the JSON structure, but front-end display logic isn’t adjusted in all places. To verify wiring:
Check that the store or prop actually gets populated with pipeline results. Given we do see “[object Object]”, we know the store had the objects. So that part is fine.
Check ConceptMesh usage: likely something like <ConceptMesh {concepts}/> in a parent component. If ConceptMesh internally expected strings, it might be trying to do operations like concept.toLowerCase() or similar, which would break if concept is an object. Such errors might be caught or fail quietly in Svelte.
Check ConceptDebugPanel: it likely just displays the list, which is where we see the issue.
In the 48-hour change log, one might find modifications to these components or their props. If ConceptMesh.svelte or ConceptDebugPanel.svelte were changed (or notably, not changed when they should have been), that’s a clue. The proper wiring fix will involve editing these components to correctly use the fields of the concept objects (e.g., using concept.title or .name).
Root Cause and Solution
Root Cause: The core issue is a mismatch between the Python pipeline’s output format and the front-end’s expectations, compounded by error handling that obscured the problem. The Python pipeline returns an array of concept objects, whereas the front-end (ConceptDebugPanel/ConceptMesh) was still expecting a simpler structure (likely an array of strings). As a result, even when the pipeline worked, the UI couldn’t display the concepts correctly ([object Object] was shown). This made it seem like the pipeline failed, and often the system then used the fallback extractor (yielding only 3 concepts). In cases where the pipeline actually failed outright (due to parsing or other errors), the fallback kicked in immediately. In both scenarios, the new concept data never properly reached the UI in human-readable form. All other symptoms – the limit of 3 concepts and inconsistent usage of the Python route – stem from this integration bug. Proposed Singular Fix: Adjust the data interface between the pipeline and the UI so they speak the same language. The most direct fix is to update the front-end components to properly handle the pipeline’s output objects. Concretely, in components like ConceptDebugPanel and ConceptMesh, use the appropriate object properties for display. For example, if the concept object has a property name or label, the Svelte template should render {concept.name} instead of {concept}. Likewise, if ConceptMesh expects just names, it could map the objects to names before building the graph, or better, be updated to use object properties if it needs additional info (like weights or relations). This one change – treating concept entries as objects with fields rather than raw text – will immediately fix the “[object Object]” issue and allow all extracted concepts to display correctly. By implementing this fix, the Python pipeline’s full output will show up in the UI, which means:
Users will see all the concepts (not just three), since the fallback will no longer be triggered except in genuine pipeline failure cases.
The concept names will display properly instead of “[object Object]”.
The need for the TS fallback will diminish, as the pipeline results are now usable. In fact, once the UI can handle the pipeline data, you can trust the pipeline output and remove or reduce the reliance on the old extractor.
Additionally, as a sanity check, after making the UI change, we should test with several PDFs. If the pipeline still fails on some (resulting in fallback), those cases need further debugging (now easier with better logging). But in many cases, this fix alone will restore the end-to-end ingestion flow: PDF upload → Python extraction → concept list displayed in UI. In summary, unifying the data format between backend and frontend is the key. The single most impactful fix is to ensure the frontend reads the concept objects correctly (e.g., using the correct property for the concept name). This addresses the immediate problem. With that fix in place, you should see the TORI system working as intended: the Python pipeline will be utilized for all uploads, the UI will show the complete set of extracted concepts by name, and the old TS extractor fallback will rarely (if ever) appear. This one change resolves the mismatch causing the breakdown, thereby restoring full ingestion from PDF upload to concept injection into the UI.