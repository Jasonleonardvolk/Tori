From Cognitive Fabric to Cognitive Blanket: End‑to‑End Pipeline & Blueprint
Current Cognitive Fabric Pipeline Overview
1. PDF Upload and Ingestion (Frontend to Backend): In the UI (e.g. ScholarSpherePanel.svelte), a user uploads a PDF. This triggers an API call to the backend (likely to an endpoint handled by efficient_pdf_server.py). The PDF file is received server-side, where it’s processed efficiently to extract text content. The server might use a PDF parsing library to get raw text or perform OCR if needed. The text is then segmented into manageable units (pages, paragraphs, etc.) for downstream processing. 2. Concept Extraction and Processing (Backend Pipeline): The segmented text is fed into a pipeline (implemented in pipeline.py) that performs concept extraction. This step identifies key concepts, entities, or topics in the text and captures the context around them. For each segment (paragraph or section), the pipeline could use NLP techniques or LLM calls to extract:
Key Concepts/Entities: Important terms or phrases representing the segment’s main ideas.
Contextual Summaries or Metadata: E.g. a summary of the paragraph or the section hierarchy (to know if it’s part of a chapter, etc.).
Relationships: Links between concepts (if two concepts are mentioned together or in a cause-effect relation, etc.). These could later form edges in a concept graph.
This processing is designed to create a structured representation of the document’s knowledge. For example, Royse (2025) outlines a similar approach where conversation text is transformed into a graph with extracted entities, relations, embeddings for similarity, and so on
researchgate.net
. By analogy, the pipeline here likely transforms document text into a set of concept nodes and relations, effectively a mini knowledge graph for that document. 3. Storage in SQLite (Persistent Memory): After extraction, the results are stored in a SQLite database (concept_db.sqlite). The schema might include tables for:
Concepts: each with an ID, name (or description), and perhaps a type (e.g. “paragraph” or “section” or “entity”).
Context Snippets: text of the paragraph/section where the concept appears, or a summary thereof.
Relations/Links: a table mapping concept IDs to other concept IDs (for relationships like “concept A is mentioned in context of concept B” or hierarchical relations like “paragraph X is in section Y”).
Each PDF or document might be identified so that concepts are tied to a source. The stored data essentially forms a knowledge base of the document’s content in a structured form. This gives the system a long-term memory beyond the LLM’s context window, since the info is now in a database. (Such external knowledge memory is known to improve retrieval and reduce the load on the LLM’s own context
researchgate.net
.) 4. Building the In-Memory Concept Mesh: On the application side, the system loads the stored knowledge into an in-memory graph structure (conceptMesh.ts). This TypeScript module likely reads from the SQLite DB on startup or on demand, and constructs objects like nodes (for each concept) and edges (for relations). The concept mesh could be a graph or network where:
Nodes represent concepts, key paragraphs, or sections.
Edges represent relationships (semantic links, or parent–child links for sections, etc.).
The mesh in memory would allow fast traversal and search. For example, if the user asks about a particular concept, the system can quickly find the node and its connected context in memory rather than querying the database each time. It’s essentially a cached knowledge graph of all ingested documents. (This mirrors approaches in knowledge-graph-based LLM memory which store data in a graph database and use it for retrieval
researchgate.net
.) 5. Query Handling and AI Response Generation: When the user poses a question to the AI, the enhancedApi.ts module’s generateResponse function is invoked. This logic is supposed to utilize the concept mesh to provide contextual knowledge to the LLM:
Understanding the Query: The query text may be analyzed to identify relevant keywords or concepts.
Retrieving Relevant Context: Ideally, the system should search the concept mesh (or concept_db) for nodes matching the query topics. For example, if the query mentions “quantum flux,” the system finds the “quantum flux” concept node or related nodes in the graph, retrieves the paragraph or section that was extracted about it, and any related concepts around it.
Composing the Prompt with Context: The retrieved context snippets (factual text from the PDF or summaries from the knowledge base) should be inserted into the prompt for the LLM. The prompt might be structured like: “Using the following information from documents, answer the question. Context: [relevant text]. Question: [user’s question]”. This way the LLM has the stored knowledge to draw on when formulating its answer.
LLM Generates Answer: The enhanced API likely calls an LLM (e.g. via OpenAI API) with the prompt. The LLM then produces an answer which hopefully uses the provided context.
Response Back to User: The answer is returned to the frontend to display.
In theory, this pipeline means the AI’s responses can incorporate details from the PDFs that were ingested. The concept extraction and storage ensure even large documents can inform answers without exceeding token limits, since only relevant bits are retrieved.
Issue: Extracted Context Not Appearing in Responses
In practice, it was observed that despite successfully extracting and storing context in the pipeline, the AI’s answers do not reflect this context. The likely culprit is in the generateResponse integration logic (in enhancedApi.ts). There are a few reasons this could happen:
No Retrieval Implementation: It’s possible that the generateResponse function currently does not actually query the concept mesh at all. It might be sending the user question directly to the LLM without appending any context. In that case, the LLM is answering from its own knowledge only, ignoring the PDF data entirely. This would certainly cause the “missing context” issue.
Failure to Match Query to Concepts: Alternatively, the code may attempt retrieval but fail to find the context. For example, if the query wording doesn’t exactly match how concepts are labeled in the database, a naive lookup (exact string match) could return nothing. Without a more robust search (like embeddings or fuzzy match), the system might not retrieve the stored info, leading to answers that omit the PDF details.
Context Not Injected Properly: It could also be a bug where the retrieved context isn’t being appended to the prompt correctly. If the context is fetched but not included (or incorrectly formatted), the LLM would not actually see it. For instance, a logic error might clear or ignore the context variable when building the prompt.
Scope/Availability of Concept Mesh: If conceptMesh.ts runs on the client side (for visualization) and not on the server, the server’s generateResponse logic might not have access to it. In such a design, the server would need to query the SQLite DB directly. If it doesn’t do so, it ends up blind to the knowledge. This architectural mismatch can cause the context to be siloed in the frontend, never reaching the LLM on the backend.
In summary, the pipeline has populated a rich knowledge base, but the Retrieval-Augmented Generation (RAG) loop is broken at the final step. The AI answers are coming straight from the base LLM, unaugmented by the stored knowledge. The primary pain point here is the lack of integration between the concept memory and answer generation – a classic last-mile issue in cognitive systems.
Limitations of the Current System
Beyond the immediate context injection bug, the current Cognitive Fabric approach has several broader limitations that need addressing before it can evolve into a true “Cognitive Blanket”:
Flat Memory with Limited Scope: The memory is essentially flat and segment-level (each paragraph or concept is stored similarly). There is no hierarchical understanding – e.g. no distinction between a small detail vs. a high-level summary. This makes it hard to provide both detailed answers and big-picture insight, and can miss the “local vs global” knowledge gap
arxiv.org
arxiv.org
.
No Multi-Modal or Multi-Scale Integration: The system handles PDFs (text) but has no support for other formats like Word documents, images, or videos. It cannot ingest visual content or audio transcripts. Even within text, it doesn’t explicitly link different granularities (paragraphs to sections to documents). This siloed approach limits the breadth of memory – a truly comprehensive system should unify text, visuals, and even dynamic data under one framework.
No Non-Local Link “Wormholes”: In the current concept mesh, relationships likely exist only if explicitly found in the text (or adjacency in the document). There’s no mechanism to connect semantically related concepts that are far apart or in different documents. In graph terms, the system lacks “wormhole” links – shortcuts that jump across the graph connecting two distant but related nodes. Without this, the AI might have to traverse many intermediate nodes (or may not traverse at all) to link related ideas, causing it to miss insights that require connecting far-flung pieces of knowledge
arxiv.org
.
No Temporal or Causal Memory (Braid Memory): The system treats knowledge as static. It does not preserve the history of reasoning, the sequence of how information was learned or discussed. If a contradiction is found and resolved, the system doesn’t keep a “scar” of that event or the resolution (“healing”). There’s no notion of cause and effect over time or tracking argument threads. This means the AI cannot reflect on why a piece of information is true or how two facts were reconciled – it lacks a notion of causality and narrative in memory.
Stagnant Knowledge Graph: The concept database is built once per document and presumably updated only when new documents are ingested. It’s not designed for continuous learning or evolution. If new data comes in, updating relations or summaries could be manual or non-existent. A true living knowledge graph, by contrast, would update in real-time, handle conflicting info by marking old data as outdated (not just overwriting), and allow querying historical states. The current setup doesn’t support that level of dynamism – it’s more of a snapshot memory, not a self-updating organism.
With these limitations in mind, we can now envision solutions. The goal is to transform this fabric (a basic woven layer of knowledge) into a blanket – a thicker, more integrated, and expansive memory system. Below is a blueprint addressing each limitation with next-generation enhancements.
Next-Generation Cognitive Blanket Architecture
To evolve the system, we propose a multi-layered, dynamic knowledge architecture that incorporates hierarchical memory, cross-modal integration, and adaptive learning. The Cognitive Blanket will cover the following key enhancements:
Multi-Scale Memory Hierarchy (Paragraphs, Sections, Concepts, Scenes)
In human cognition and recent advanced RAG research, hierarchy is crucial for dealing with complex information
arxiv.org
. The system should organize knowledge at multiple scales, ensuring both granular details and high-level summaries are stored:
Paragraph-Level or “Atomic” Memory: These are fine-grained snippets (paragraphs, individual facts, or video frames). They capture detailed information (e.g., a specific experimental result in a paper, or a single statement). This is akin to the current system’s basic units. Paragraph nodes hold the raw content or a short summary of it.
Section or Topic-Level Memory: Above paragraphs, the system should create summary nodes for sections or topics. For instance, if a PDF is structured into chapters and sections, the ingestion pipeline can generate a section summary concept that distills the main idea of that section. This node would link to all paragraph nodes within that section (forming a “contains” or hierarchical relation). By doing so, the system knows that those paragraphs collectively contribute to a broader idea. Similarly for video, a “scene” node could summarize a sequence of frames or shots comprising a scene.
Document or Global Concepts: Some concepts are overarching themes of an entire document or across documents. For example, a concept like “climate change impact” might be discussed across multiple sections or even multiple documents. The system should have higher-level concept nodes that represent these broader topics, possibly created by clustering lower-level mentions or by an LLM summarization across a community of nodes. Each such node would link to instances where that concept appears. This is analogous to creating a hierarchical knowledge graph as in HiRAG, where higher-layer nodes summarize clusters of entities in lower layers
arxiv.org
arxiv.org
.
Video Scenes and Temporal Segments: For video input (when integrated), a scene-level memory node represents a continuous event or setting in the video (a span of time where certain entities interact). Within it, frame-level or object-level nodes can exist. This parallels paragraph vs. section: a scene node links to the moments (frames or shots) that compose it, along with a text description (from video captions or extracted via an LLM). Scene nodes could be linked further to text concepts if the same concept appears in a document and in a video (e.g., a video scene about “quantum experiment” links to the concept “quantum experiment” described in a PDF).
With multi-scale memory, the AI can retrieve information at the appropriate level of detail. For broad questions, it can pull higher-level summaries (section or concept nodes). For specific questions, it can drill down to precise paragraph or frame details. Crucially, when answering, the system can provide a three-level context: a global summary, a bridging mid-level explanation, and specific details
arxiv.org
arxiv.org
. This mirrors the HiRAG approach where after hierarchical retrieval, they feed local descriptions, bridge-level info, and global context together to the LLM
arxiv.org
. The result is a comprehensive yet concise context that helps the AI give informed answers. Implementation: Achieving this requires extending the pipeline:
pipeline.py: After extracting paragraph-level data, add steps to generate section summaries (perhaps using an LLM prompt: “summarize this section”) and identify global themes (maybe via clustering keywords or another LLM call on the whole document). Insert these as new nodes in concept_db (with a level or type field like “section_summary” or “global_concept”). Also store links (edges) mapping section summaries to their child paragraphs, and global concepts to related sections.
concept_db schema: Introduce fields for hierarchy (e.g., parent_id for a concept node) or a separate edges table to represent “A is part of B”. This creates an explicit tree/graph structure.
conceptMesh.ts: Update to build this multi-level graph. It should allow queries like “find relevant paragraphs and also return their section summary and document-level summary”. This way, generateResponse can retrieve a stack of contexts (e.g., document summary -> section summary -> specific paragraph) to feed the LLM.
The benefit will be addressing the local-vs-global gap: the AI will no longer miss the forest for the trees or vice versa, because it has both views. As researchers note, bridging local and global knowledge ensures both granular details and broader context inform the reasoning
arxiv.org
.
Wormhole Links for Non-Local Semantic Jumps
In a static document, connections between concepts are linear or based on explicit references. But human knowledge forms a web of associations, where two ideas might be deeply related even if the documents are totally separate or far apart in sequence. To capture this, the knowledge graph should support wormhole links – direct connections that cut across the usual topology to link semantically similar or contextually related nodes regardless of their locations. Use cases for wormholes:
Two different PDFs discuss the same concept or person or theory. The system should recognize this (perhaps via identical names or embedding similarity) and link those concept nodes together (or unify them into one node). For example, if “Quantum Flux” appears in both Document A and Document B, a wormhole edge connects these nodes or they get merged into a single “Quantum Flux” concept entry with two sources.
A concept in one context implies another concept elsewhere (non-obvious connection). E.g., a PDF on climate science talking about “El Niño” and another PDF on agriculture talking about “crop yield” could be linked if an analysis shows El Niño strongly affects crop yields. The system could form a wormhole relation “influences” between those concept nodes, even though the documents are unrelated domains. This could emerge through cross-domain entity linking or an external knowledge base.
In a video vs text scenario, if a video scene depicts something described in text, a wormhole can connect the scene’s node to the text concept node. For instance, a video showing an “exploding supernova” could link to an astronomy paper’s section on supernovae.
How to establish wormholes: There are a few mechanisms:
Semantic Similarity: Using embeddings (vector representations) of node content to find nodes that are similar in meaning. If two nodes in different parts of the graph have high cosine similarity above a threshold, automatically create an undirected “related-to” edge between them. This acts as a wormhole shortcut for the retrieval algorithm. Notably, Huang et al. (2025) achieve something analogous by introducing summary nodes that act as shortcuts between distant entities
arxiv.org
 – in effect, reducing the degrees of separation for semantically close concepts.
User or Agent Feedback: If the AI (or user) often traverses from one concept to another during Q&A, the system can learn that connection. For example, if multiple queries link concepts A and B (they frequently co-occur in answers), the system can create a direct link A<->B to expedite future reasoning. This dynamic wormhole formation ensures the graph reconfigures based on actual usage patterns.
External Knowledge Graphs: Use known connections from Wikidata or other sources to link entities. If the ingested content mentions entities that are known to be related (e.g., via an ontology or knowledge base), wormhole edges can be added with relation types like “is a type of”, “affects”, “part of”, etc.
Benefits: Wormholes dramatically shorten the path between related pieces of knowledge. Instead of a long chain of intermediate nodes (which a query might never successfully traverse), the LLM or retrieval algorithm can hop directly. This improves the system’s ability to synthesize information across documents. It addresses “Challenge (1)” noted in HiRAG: in a plain graph, semantically similar entities might be far apart structurally, hindering reasoning; shortcuts (wormholes) bridge those entities efficiently without exhaustive traversal
arxiv.org
. In practical terms, an LLM using this graph can see connections that would otherwise require multiple hops or might be missed entirely. Implementation:
Augment the ingestion pipeline with a post-processing step that runs embedding-based clustering/linking. Each concept node can be assigned an embedding (perhaps using a sentence transformer or the LLM’s embedding API on the node’s content or name). A service or script periodically scans new nodes against existing ones to find high similarity and creates wormhole links accordingly.
Alternatively, maintain a vector index (like FAISS or an ANN index) of concept embeddings. On each query, find the nearest concepts in vector space which might not be directly connected in the graph. Use those as additional retrieval cues (this is a form of concept fuzzing, discussed later). If certain nearest neighbors are repeatedly relevant, consider adding a permanent link in the graph between those nodes (making the semantic connection explicit).
The concept database might get a new table “related_concepts” for these wormhole links (storing pairs of concept IDs and a relation like “similar_to” or the similarity score).
Ensure conceptMesh.ts loads these wormhole edges and that the search logic in generateResponse traverses them. For instance, a query about concept A could retrieve not only A’s direct info but also immediately connected wormhole neighbors’ info, since those are highly relevant by design.
Overall, wormhole links give the Cognitive Blanket non-local connectivity, much like folding the fabric so two distant patches touch. This makes knowledge retrieval more resilient: even if the answer lies in a different document or a far-off section, the shortcut increases the chance the system finds and uses it.
Braid Memory: Temporal Causality and Knowledge Evolution
A Cognitive Blanket should not just store facts – it should weave a narrative memory of how those facts come about, interact, and change. Braid memory refers to intertwining multiple strands of information over time, preserving the sequence (causality), the cross-links between threads (arguments and counter-arguments), and even the “knots” or scars where conflicts occurred. Key aspects of braid memory:
Causal Links: If the content indicates causation (“X causes Y”, “Because of A, B happened”), the knowledge graph should capture that explicitly. Instead of just two disconnected fact nodes, an edge with type “causes” or “leads to” connects them. This allows the AI to follow chains of reasoning. For example, if a user asks “Why did phenomenon B occur?”, the system can traverse the causal link back to A. In video or text narratives, this is crucial for understanding sequences of events. (Traditional sequential memory often misses these cause-effect relations in long sequences
arxiv.org
arxiv.org
, whereas a graph memory excels at it
arxiv.org
.)
Argument Paths: When the AI or user engages in a line of reasoning (say, in a conversation, or an internal chain-of-thought to answer a complex question), that path should be stored. Each step (a hypothesis, a piece of evidence, a conclusion) can be nodes in the graph, chained by “follows” or “infers” edges. If later a similar question is asked, the AI can recall this path instead of reasoning from scratch. It also helps in explaining answers (“I know this because A -> B -> C as established earlier”). Essentially, the system learns from its reasoning processes, not just from documents. This is related to the concept of replay or chain-of-thought memory. By keeping these paths, the system also records which reasoning succeeded and which failed.
Scars (Conflict Memory) and Healing (Resolution): In the course of ingesting information or having dialogues, the AI might encounter conflicting data or make an error which is later corrected. Instead of overwriting the old info, the cognitive blanket keeps it as a “scar.” For example, if Document1 said “Theory X is true” and Document2 later says “Theory X was refuted,” the system should mark the Theory X node as refuted, but not delete it. The outdated assertion remains in memory with a flag or link “conflicts_with new finding Y” along with a timestamp. The resolution (refutation) is stored as well, perhaps an edge “updated_by Y”. This way, the system maintains historical accuracy and can explain how its knowledge changed
medium.com
. It can also warn if a user’s query references a scar (“That was believed earlier, but later evidence suggests otherwise…”). This concept is akin to a truth-maintenance system, where beliefs and their justifications are tracked over time.
Temporal Indexing: Every piece of knowledge could carry a timestamp of when it was added and (if applicable) when it was invalidated. A bi-temporal model (like in the Graphiti framework) can be used, where each relationship knows when it was valid
medium.com
. This allows “time travel” queries, like “What did we know about X last year versus now?” or “Show the progression of understanding on topic Z.” It also helps in conflict resolution: new info gets a newer timestamp, and older info can be marked outdated without deletion. The memory thus behaves like a living timeline of knowledge.
Implementing braid memory requires changes across the stack:
Ingestion & Dialogue Tracking: The pipeline should extract causal relations from text (perhaps using an NLP tool or prompting an LLM: “does this sentence imply a cause/effect?”). It should also detect contradictions (if two statements about the same entity conflict). This might involve an additional consistency-checking module. For dialogues with the AI, the system should log the question and answer nodes, and if a user or the AI later corrects something, mark the link.
Schema Extensions: The concept_db needs to support relationship types and properties. Instead of a generic concept link, we have typed edges like causes/effects, supports/opposes, etc., and attributes like valid_from and valid_to timestamps on each edge (and perhaps node). This effectively transforms the static concept graph into a temporal knowledge graph with rich semantics.
Reasoning Engine Integration: When generating responses, the system can use these causal and historical links to form a reasoning chain. It might not rely purely on the LLM’s internal chain-of-thought, but explicitly follow graph paths. For example, for a “why” question, find if there’s a cause edge leading to the node in question and pull that context. This leads to more explainable answers.
Conflict Handling Module: Whenever new knowledge is added (new document or new user feedback), run a check against existing knowledge for conflicts. Graphiti’s approach could be instructive here: it performs semantic, keyword, and graph searches to detect contradictions between new info and the graph, then uses temporal markers to invalidate but not erase old info
medium.com
. Implement something similar so that the Cognitive Blanket can gracefully handle evolving knowledge without losing the past.
By braiding memory in this way, the system attains a form of memory of reasoning in addition to memory of facts. This aligns with how humans remember not just facts but also the stories and justifications behind those facts, including missteps and corrections. It will preserve causality (so the AI can understand sequences of events or logic), maintain coherence in long-term dialogs, and become resilient to knowledge evolution (no more brittle updates that forget old context).
Unified Multimodal Ingestion Pipeline (Text, Documents, Images, Video)
To truly be a blanket covering all knowledge, the system must break out of the modality silo. We extend the pipeline to handle multiple input formats in a consistent way, so they all feed into the same knowledge graph. Supported Modalities & Approach:
Structured Text (PDF, Word, HTML): For PDFs and Word documents, we use similar text extraction as now, possibly augmented for DOCX (using a library for Word files). The structure (headings, lists, etc.) should be preserved to inform the section hierarchy in the graph. If HTML or markdown files are ingested, parse them into text and structure as well.
Images: For images, we perform image-to-text conversion. This could involve captioning the image with an LLM or using an image recognition model to detect objects and scenes. The result is a textual description or a set of entities in the image. We then treat those like a paragraph: run concept extraction on the caption (the “paragraph” describing the image) to identify key entities in the image and their relations (e.g., “a cat on a chair” yields entities cat and chair, relation on). The image can be represented as a node with those entities as linked nodes and possibly a thumbnail or reference to the file.
Audio/Video: For video, first transcribe audio (using ASR) to get a text transcript. Then segment the video into scenes (perhaps via shot change detection or by clustering transcript segments by time or topic). Each scene gets a summary (using an LLM on the transcript segment) and key entities (using object detection on frames or named entities from the transcript). The video scene node is linked to those entity nodes. Additionally, as GraphVideoAgent research suggests, representing the temporal interactions of entities is crucial
arxiv.org
arxiv.org
. So for each video, we might build an internal scene graph: nodes for objects (people, things) and edges for their interactions in that scene (“chases”, “talks to”, etc.). That scene graph’s info can be merged into the global knowledge graph by linking those object nodes to global concepts (e.g., the “John Doe” in a video scene linked to the known person node “John Doe” from text).
All these ingestion processes funnel into one unified knowledge store. The concept_db (or successor) would have an identifier for the source modality and perhaps store raw references (like file IDs or timestamps in video). But critically, an entity detected in any modality maps to the same concept node in the graph if possible. For example, an image of the Eiffel Tower and a text mention of “Eiffel Tower” should converge on one node representing the Eiffel Tower concept, linked to both the image node and the text context where it appears. This unified representation is an instance of a multimodal knowledge graph, where nodes can have textual info, visual info, etc., connected for a comprehensive understanding
medium.com
. Unified Knowledge Graph (Living Memory Graph): With all data in one graph, the Cognitive Blanket behaves like a continuously growing and adapting network of knowledge:
It can handle queries across modalities. A user could ask “What did the video show when the speaker mentioned climate change?” – the system can traverse from the text concept “climate change” to the video scene node where that was mentioned, then describe the visual content from that scene node (since it has or links to an image/frame or description). Because everything is connected, answering cross-modal questions becomes feasible.
It provides a unified world model to the AI agent. Instead of separate pipelines for text vs images, the agent sees one graph. Graphiti (Zep AI’s framework) emphasizes having chat, JSON data, and text all feeding a single graph for a unified view
medium.com
. We aim for the same: whether the knowledge came from a PDF, a conversation, or a video, it ends up interlinked in one structure.
Scaling and Technology: As this graph grows (potentially large, with many node types), using SQLite might become a bottleneck. We might consider migrating to a dedicated graph database (like Neo4j or a graph library) for better query performance and native graph queries (traversals, pathfinding, etc.). Neo4j, for instance, could handle the interconnected data and provide indexes for fast lookups (e.g., Graphiti uses Neo4j to manage its memory store
medium.com
medium.com
). In the interim, SQLite can be extended with indices (on concept names, embeddings) and we can load portions of the graph into memory as needed for speed. Modular Ingestion: Architecturally, we might split the ingestion by modality:
A DocumentIngest Service for PDFs, Word, etc. (text-oriented extraction).
An ImageIngest Service for images (handles OCR or captioning and tagging).
A VideoIngest Service for video (handles transcription, scene detection, object tracking as done in Graph-VideoAgent, etc.).
All produce outputs in a common intermediate format (e.g., a set of nodes/edges to add to the graph). Then a Merger component takes those and unifies concepts by checking for duplicates (for instance, by name or embedding similarity to existing nodes, akin to entity resolution). This merger can also leverage LLMs to decide if two things are the same (“Is the John mentioned in Document A likely the same as John in Video B?”).
By unifying ingestion, we ensure all formats contribute to one living knowledge graph rather than isolated silos. The cognitive blanket thus “covers” knowledge from text, visuals, and audio uniformly.
Advanced Capabilities and Speculative Enhancements
Finally, we incorporate forward-thinking features that push the Cognitive Blanket beyond a static knowledge base and towards an active, self-improving memory system:
Concept Fuzzing: This mechanism introduces slight perturbations or variations to concepts to explore the knowledge space. In practice, concept fuzzing could mean generating alternative phrasings, synonyms, or hypothetical scenarios involving a concept, and seeing how the system handles them. For example, if the concept is “neural network optimization,” the system might auto-generate related queries or subtopics (“neural network regularization?”, “does optimization converge if...”) and attempt to retrieve or infer answers, thus discovering implicit connections or gaps in knowledge. Technically, this could be done by taking concept node embeddings and nudging them in various directions (random or toward related categories) to find neighboring vectors that don’t exactly match an existing node but might represent a missing concept. If something useful is found (e.g., the AI finds relevant info for the fuzzed concept), the system can add a new node or link. Concept fuzzing keeps the knowledge graph “fuzzy” at the edges, meaning it’s not limited to only what was explicitly ingested – it probes nearby semantic space to anticipate information needs. This can help catch questions that use synonyms or uncover knowledge that wasn’t directly given but can be inferred via LLM. Essentially, it’s a form of creative exploration of the knowledge graph using the LLM’s generative ability.
Dynamic Wormhole Formation: While we discussed wormhole links above in a mostly static sense (post-ingestion analysis creates them), here we emphasize continuous adaptation. The system can monitor query patterns and graph usage to dynamically add or even remove wormholes. If two concepts start co-occurring in user queries or AI reasoning frequently, a link is formed on-the-fly if not already present. These dynamic wormholes could also incorporate context-specific links: for the duration of answering a particular complex question, the system might create a temporary shortcut between nodes relevant to that query (like a cognitive “bridge” just for that reasoning session). Once the task is over, it may keep the link if it proved useful or let it fade if not used again (almost like synapses strengthening or weakening). Over time, this would lead to an evolving topology of the graph that mirrors how the AI’s understanding grows. Rarely used connections might be pruned (or marked low weight), frequently used ones become permanent strong edges. This concept draws inspiration from how human memory reinforces frequently recalled associations. It could be implemented by maintaining counters on edges (how often they were traversed or contributed to answers) and periodically boosting or dampening edge weights / existence accordingly.
“Blanket Folding” (Adaptive Graph Reorganization): This is a metaphor for re-structuring the knowledge graph dynamically to better suit the current context or to compress knowledge for efficiency. Blanket folding might involve:
Merging Nodes: If over time the system realizes two separate nodes are effectively the same concept (through usage or new info), it “folds” them into one, merging their relationships. This reduces redundancy and makes retrieval quicker (no need to search two places for one concept).
Contextual Folding: When a very complex query spanning multiple domains comes in, the system might temporarily pull together a subgraph from different parts of the knowledge base into a “folded” structure that’s easier to traverse. For instance, it might create a composite node that represents a scenario combining elements from various sources, so the LLM can focus on that. It’s akin to creating a mental map specifically oriented to the question at hand.
Compression and Summarization: Folding can also mean collapsing a long chain of reasoning or a large cluster of nodes into a summary node on the fly. If an answer doesn’t require all intermediate details, the system can fold them into a concise summary and present that to the LLM, saving token space and focusing the reasoning. This could be done by recursive summarization of subgraphs (e.g., summarize an entire subgraph of a topic into 2-3 sentences and treat that as a single node).
Unfolding on Demand: Conversely, when detail is needed, the blanket can “unfold” a summarized node back into its detailed constituents. This dynamic expand-collapse of parts of the graph gives flexibility: the memory can be coarse-grained or fine-grained as needed, much like how humans zoom in and out in their memory of a subject.
Implementing blanket folding requires a high degree of automation and possibly heuristic or ML-driven graph algorithms. A scheduler could run when the system is idle to optimize the graph structure (merging duplicates, reorganizing clusters by similarity or usage). Also, an interface for the LLM to request unfolding might be needed (if the LLM finds an answer is too high-level, it could query for more detail on that node, causing the system to reveal the underlying nodes). In summary, these speculative features would give the Cognitive Blanket a self-optimizing quality. The knowledge graph isn’t static; it actively explores new connections (fuzzing), adapts its network (dynamic wormholes), and reshapes itself (folding) to best serve the AI’s reasoning needs. This moves toward the vision of a persistent, reasoning-capable memory that grows and self-organizes akin to a living brain rather than a fixed database.
Implementation Roadmap and Module Upgrades
To turn this blueprint into reality, each component of the system will need upgrades or extensions:
Frontend (ScholarSpherePanel.svelte): Expand the upload interface to support new formats (allowing users to upload not just PDFs, but also .docx, image files, video files). Provide feedback on ingestion progress (videos might take time to transcribe, for example). Possibly allow users to tag uploads with context (if needed) or verify merged concepts (in case the system guesses two concepts are same, a user could confirm/correct).
Backend Ingestion Services: Refactor efficient_pdf_server.py into a more general ingestion server. We might split handlers: one for text docs, one for images, one for videos:
The text handler will use existing PDF logic, plus integrate a library for Word/HTML. It should output text chunks with structure metadata (like headings).
An image handler will use a vision model or API (e.g., Azure Cognitive Services or BLIP captioning model) to get descriptions, then call the pipeline on those descriptions.
A video handler will orchestrate speech-to-text (using something like Whisper), then chunk the transcript by time or scene. For scene detection, we might use an algorithm (like monitoring when subjects or location changes significantly, possibly with the help of an LLM analyzing the transcript for scene breaks). It could also use a lightweight video processing library to extract key frames for object detection.
Each handler then calls into pipeline.py (or its own specialized pipeline functions) to do concept extraction on the resulting textual data (transcripts, captions, etc.) and relationships (including temporal ones for video).
pipeline.py Enhancements: This becomes the brain of ingestion:
Incorporate hierarchical node creation (as described in multi-scale memory) – likely by doing multiple passes (first pass paragraphs, second pass sections, etc.).
Use NLP to extract relations: possibly integrate an existing information extraction model (for causality, coreference, etc.) or prompt an LLM with specific tasks (e.g., “read this paragraph and output any relationship triple X–relation–Y it contains”).
Merge or link concepts across sources: implement an entity resolution step (could use string similarity and vector similarity). If an entity from new content matches something existing, link to that ID instead of creating duplicate.
Assign unique IDs for each node, and perhaps maintain a mapping from raw text references to concept IDs (for quick lookups).
If using a graph database instead of SQLite, pipeline.py would contain code to upsert nodes and edges via that DB’s API (Cypher queries for Neo4j, for example). The approach of encoding conversation into a Neo4j knowledge graph, as Royse did, can serve as a reference for building and querying such graph memory
researchgate.net
.
Database / Knowledge Store (concept_db.sqlite or graph DB): Extend the schema:
Add a node table if not present (to unify concepts, documents, images, etc., in one table with columns like id, type, name, content, timestamp, embedding, source_id).
Add a edge table with source_node, target_node, relation_type, t_valid, t_invalid to capture relationships with temporal validity (for braid memory).
Add an embedding_index or use an external vector index for concept embeddings to enable semantic search (concept fuzzing and wormhole suggestions).
If continuing with SQLite, consider using FTS (Full-text search) for quick keyword search of content, and a separate table for embeddings (storing vectors as blob or using an extension). If switching to a graph DB, design the labels and relationship types to match our needs (e.g., nodes labeled as Concept, SectionSummary, Image, etc., and relations like MENTIONS, CAUSES, PART_OF, SIMILAR_TO).
ConceptMesh / Memory Manager: conceptMesh.ts might evolve into a more sophisticated memory manager module. If using a graph DB, this module could serve as an interface between the application and the database, handling queries and caching:
Implement query functions like findRelevant(query) that do a hybrid search: it could combine vector similarity (to find concept nodes semantically close to the query) with graph traversal (to expand to neighboring nodes like details or causes). This hybrid approach is shown to be effective in Graphiti, which uses semantic, keyword, and graph search together
medium.com
 for fast, rich retrieval.
Manage in-memory caching of recently used subgraphs. E.g., if a certain topic was just queried, keep those nodes in memory for quick follow-up queries.
Provide utility to the response generator to get multi-scale context. For example, a function getContextFor(node_id) that returns the node’s content plus its parent summary and any critical child detail, assembling the three-tier context automatically.
Continuously update itself when new data is ingested or when dynamic wormholes are formed. Possibly run a background thread to listen for changes (if using a DB that can notify, or simply polling).
enhancedApi.ts (GenerateResponse Logic): This is where retrieval-augmented generation truly happens now:
Before calling the LLM, call the memory manager (conceptMesh interface) with the user’s query. Use a combination of methods: keyword search on the graph for exact matches, vector search for semantic matches, and perhaps query parsing to identify if it’s asking for cause/effect or a specific source (if the question says “in the PDF about X, ...” then directly find that document’s nodes).
Collect a set of relevant nodes/snippets. For each, perhaps fetch a short description. If too many, rank them (maybe by relevance score or limit by top 5).
Compose the prompt: Introduce the context in a structured way. For example: *“You are a knowledgeable AI with an external memory. Here are some relevant snippets from various sources:
[Summary] ... (source: Document A, Section 2)
[Detail] ... (source: Document A, p.5)
[Related] ... (source: Video B, scene 3)
Using the information above, answer the question: [user question].”*
By enumerating context, we give the LLM organized input. Each snippet can be annotated with its origin and level (summary or detail), which might help the LLM use them properly. Ensure that if the query is causal, include any cause/effect edges related to the query concept (e.g., “Node X is caused by Node Y” as part of context).
After getting the LLM’s answer, optionally post-process it to cite sources or check against the knowledge graph for any contradictions. For instance, if the LLM says something that conflicts with stored info, we could add a warning or have the LLM self-verify by asking it to double-check given the context (this could be a future improvement to reduce hallucinations).
Agent Memory and Ongoing Learning: If the system has a conversation with a user over multiple turns, treat that conversation as another data source to ingest into the graph (similar to how Royse’s system encodes dialogues into the knowledge graph
researchgate.net
). That means each user question and AI answer becomes part of the memory (perhaps marked as ephemeral or permanent depending on if we want the AI to remember it later). This can enrich the AI’s context awareness (e.g., if the user asks a follow-up, the relevant previous Q&A is already in the graph and can be retrieved without prompt history if needed). It also means any new facts revealed in conversation can be added to the knowledge base (with appropriate source labeling like “user provided fact”). Over time, the agent builds a long-term memory of its interactions, not just the documents.
Performance Considerations: With these enhancements, retrieval might involve multiple steps (embedding search, graph traversal, etc.) but it can be optimized. For example, maintain a community index: group nodes by topic communities (as in Microsoft’s GraphRAG approach
medium.com
) so that queries first narrow down to a subset of the graph (e.g., if the query is about physics, focus on the physics community of nodes). Within that, do detailed retrieval. This prevents searching the entire graph every time and improves speed as knowledge grows. Also, ensure asynchronous processing where possible: ingesting a huge video can be done in the background with status updates, rather than blocking responses.
By following this roadmap, each part of the system becomes more robust and interconnected. We move from a simple pipeline to a persistent cognitive architecture where ingestion, storage, and querying all reinforce each other in a loop:
New data in any format updates the knowledge graph.
The graph in turn provides better context for new queries.
Queries feed back (via dynamic links or new nodes from conversation) into updating the graph.
Conclusion
The proposed Cognitive Blanket architecture turns the current system into a unified, intelligent memory fabric that is:
Hierarchical and Multi-scale, bridging detailed facts with big-picture summaries so the AI has full context at every level
arxiv.org
.
Cross-modal and Unified, integrating text, images, and video into one knowledge graph for a holistic world view
medium.com
.
Dynamic and Self-Evolving, with the ability to form new connections on the fly (wormholes), remember and learn from reasoning processes (braids with causal links), and adapt its structure over time (folding and unfolding).
Persistent yet Adaptable, preserving history and past knowledge (no forgetting, only marking obsolete) while staying current by resolving conflicts and updating facts
medium.com
.
By addressing the pain points (like missing context injection) with these solutions, the AI will not only include the right context in its answers, but do so in a way that demonstrates understanding of the relationships, causes, and evolution of the knowledge. The Cognitive Blanket essentially functions as a long-term external memory that augments the LLM’s short-term memory
researchgate.net
researchgate.net
, enabling more coherent, accurate, and explainable responses. It’s a step toward an AI that can truly know, remember, and reason over a growing base of knowledge – a foundation for more advanced cognitive capabilities in the future.