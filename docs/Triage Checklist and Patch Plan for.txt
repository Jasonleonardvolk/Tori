Triage Checklist and Patch Plan for TORI Concept Ingestion Pipeline
Introduction
TORI’s concept ingestion pipeline processes content from PDFs, videos, and audio transcripts to extract key “concepts” (important topics or phrases) for downstream use. Recently, this capability has degraded – logs are incomplete, too few concepts are captured, and critical metadata is lost. This report identifies the root issues and provides a prioritized checklist of fixes to restore and enhance concept ingestion across all media types. We detail recommended code patches (with target files/sections), as well as diagnostic tools to verify improvements. Optional enhancements are also suggested for longer-term refinement of the concept extraction process.
Summary of Issues and Recommended Fixes (Prioritized)
Logging Gaps: Missing iteration logs (loopRecord) and no semantic concept output in logs. Fix: Re-enable detailed logging for each content segment and post-extraction results.
Overzealous Concept Filtering: Confidence thresholds filter out most concepts, yielding too few (sometimes zero). Fix: Lower the threshold or use adaptive logic to retain more concepts (ensure a minimum count of concepts).
Hard-Capped Concept Limit: The pipeline truncates to top 5 concepts when writing to memory, dropping many relevant concepts. Fix: Remove or raise the cap (e.g. allow all or top N>5), or make it configurable
mathworks.com
.
Missing Concept Metadata: Extracted concepts lack confidence scores, extraction method, or source reference. Fix: Preserve full metadata for each concept (confidence value, which extraction method was used, and source page/time).
Lack of Diagnostic Tools: No easy way to validate ingestion quality before vs. after fixes. Fix: Develop PowerShell/Python scripts and add logging to measure concept counts, coverage, and log completeness for test files.
Each of these issues and their solutions are discussed in detail below, along with file targets and example patches.
Issue 1: Logging Gaps (Missing loopRecord and Semantic Concepts Logs)
Symptom: The ingestion logs currently lack entries for each processing loop or segment, and there is no log of the extracted semantic concepts. In practice, this means when TORI ingests a PDF or video transcript, we do not see the expected “loopRecord” log for each page or time segment, nor any semantic_concepts output confirmation. This makes it impossible to trace which parts of the content were processed or to verify that concept extraction even occurred. Impact: Without these logs, debugging the pipeline is very difficult. Developers cannot tell if the pipeline skipped certain sections or if concept extraction failed silently. The absence of a semantic concepts log (or output file) suggests the extraction step might not be executing or might be returning empty results without notice. This lack of observability obscures all other issues and prevents confidence in the ingestion process. Cause: It appears that the logging calls for segment processing and concept output were removed or never activated. Possibly a refactor or a misconfigured log level caused loopRecord entries to be suppressed. Similarly, the code that prints or saves the semantic_concepts results might be bypassed if no concepts are found (e.g., not logging an empty result) or the logging statement was omitted entirely. Proposed Fixes:
Re-enable Iteration Logging: Add a log statement inside the content processing loop (in the ingestion module that handles iterating through pages or transcript chunks). For example, in the content ingestion service (e.g., IngestionManager or similar class), after each segment is processed, log an info-level message with a unique identifier and summary. This could be:
“LoopRecord: Processed segment X (e.g., Page 3 or Video timestamp 00:02:00-00:03:00) – extracted Y candidate concepts.”
Ensure this uses a logger level that is captured in production logs (e.g., INFO rather than DEBUG if debug logs are not enabled in deployment). The code section to update is the loop that iterates over content segments; insert or un-comment the logging line there.
Log Concept Extraction Results: After the pipeline extracts semantic concepts for the entire content, insert a log entry summarizing the outcome. For example, in the concept extraction module (perhaps where semantic_concepts JSON is written), log a message like:
“Semantic concept extraction complete: N concepts identified (saved to semantic_concepts.json)”.
If the pipeline writes out a JSON or database record of concepts, log the path/ID of that output. This confirms in logs that the step ran successfully. Update the section of code where the semantic_concepts output is generated (e.g., right after writing the JSON file or updating the memory store) to include this log.
Handle Empty Results Explicitly: If no concepts are found for a segment or an entire file, add a warning log: “Warning: No concepts extracted for [file/segment].” This will alert engineers that the pipeline ran but yielded nothing, prompting investigation of threshold or parsing issues. Place this in the extraction function after attempting to get concepts – e.g., if the result list is empty, log a warning (and still create an empty output file if applicable so that the process is transparent).
Logging Configuration Check: Verify that the logging configuration (log4j, Python logging, etc. depending on the stack) is not filtering out these messages. For instance, ensure the logger for the ingestion component is set to INFO level or lower. If loopRecord was meant as a custom log marker, make sure the format or category is correctly set up in the logging config.
Target Files/Sections: Look at the ingestion loop code (e.g., ToriIngestionService.cs in .NET or ingest_pipeline.py in Python) – inside the loop that processes each page or transcript chunk, add the loopRecord log. Also update the concept output stage (possibly in ConceptExtractor.extract() or a post-processing function) to log the count of concepts found. If a function writes the concepts to a file (semantic_concepts.json), add a log right after the file write. For example, in a Python pipeline, update the section like:
python
Copy
concepts = extract_concepts(text_segment)
...
logger.info(f"LoopRecord: segment {segment_id} -> {len(concepts)} concepts extracted.")
...
# After all segments processed or after clustering concepts:
logger.info(f"Extracted total of {len(final_concepts)} semantic concepts for {content_title}.")
These additions will fill the current logging gap. After patching, run an ingestion on a sample file and confirm that logs now show per-segment processing and a final concept summary.
Issue 2: Overly Strict Concept Filtering Thresholds
Symptom: The concept extraction step is using thresholds that filter out too many concepts. In recent ingestions, the final concept list is extremely small (often just 1-3 concepts, or even empty) despite the content being rich. This suggests that many extracted candidates were discarded for not meeting a confidence or relevance threshold. Impact: Important topics are being lost, leaving TORI with an impoverished understanding of the content. For example, if a PDF discusses 10 key ideas but the threshold is so high that only 2 concepts are kept, the system misses 80% of the relevant knowledge. In extreme cases, if no concept meets the threshold, the pipeline produces nothing (as evidenced by missing semantic_concepts output). This undermines the purpose of ingestion, as the system’s memory will not reflect the content's actual themes. Cause: The threshold for filtering concepts (e.g. a minimum confidence score or similarity metric) is likely set too high. Perhaps the pipeline only keeps concepts above, say, 0.8 confidence, which is rarely attained except by the top 1-2 concepts. Alternatively, there may be a bug where the threshold logic is comparing scores incorrectly (e.g., normalized vs. raw) causing most candidates to drop out. Another possibility is that a one-size-fits-all threshold doesn’t suit all media types (speech-to-text transcripts might have inherently lower confidence scores, for instance). Proposed Fixes:
Lower the Confidence Threshold: Adjust the minimum confidence or relevance score required for a concept to be kept. For instance, if it’s currently 0.8, consider lowering it to 0.5 (or an empirically determined optimal value). This simple change (e.g., editing a constant or config value in the concept filtering function) will allow more concepts to pass through. After this change, test on several documents to ensure we get a reasonable number of concepts (not too many noise terms).
Dynamic Threshold or Minimum Count: Implement logic to avoid zero outputs. For example, always retain at least a baseline number of concepts even if they are below the nominal threshold. The code could be: “if filtered list is empty (or < 3), then take the top 3 concepts by score regardless of threshold.” This ensures every ingestion yields some concepts for context. Similarly, you could make the threshold dynamic based on content length – e.g., for longer content, allow more concepts (or slightly lower threshold if very few concepts were extracted).
Review Scoring Mechanism: Double-check how confidence is calculated. If the scoring is on a different scale or if using an embedding similarity measure, ensure the threshold is appropriate. For instance, if using cosine similarity or an internal “resonance_score,” recalibrate what a “good” score is. It might help to log the distribution of raw concept scores on a few sample runs (temporarily) to choose a sensible cutoff.
Make Threshold Configurable: Instead of a hard-coded value, use a configuration setting (e.g., in a JSON or .env or settings file) for the concept confidence threshold. This allows quick tuning in the future without code changes. For example, in the concept extraction module (ConceptExtractor.py or ConceptFilter.cs), replace the constant with a parameter loaded from config (and document it for the team).
Per Media-Type Tuning (Optional): Consider if different media should use different thresholds. E.g., transcribed speech might include more noise, so perhaps use a slightly lower threshold or require a higher minimum count for audio/video content as compared to well-structured PDF text. In the code, this could be implemented by branching on content type (if that info is available in the pipeline) and applying a multiplier or alternative threshold. (This is a secondary tweak – the primary fix is simply to lower the overly strict cut-off that’s currently in place for all.)
Target Files/Sections: Locate the portion of code where concepts are filtered or ranked. This may be a function like filter_concepts_by_confidence(concepts, min_conf) or a segment after computing concept scores where it does something like concepts = [c for c in concepts if c.score >= THRESHOLD]. In that file (perhaps ConceptExtraction.py or SemanticAnalysis.cs), update the THRESHOLD value to a more permissive level. If the threshold is hard-coded, change it; if it’s in a config, update the config default. Also, consider adding the fallback logic after filtering: e.g., if len(concepts) ends up too low, take the top few regardless. A pseudo-code example:
python
Copy
MIN_CONF = 0.5  # lowered from 0.8
filtered = [c for c in concepts if c.confidence >= MIN_CONF]

if len(filtered) < 3:
    # Ensure a minimum of 3 concepts
    filtered = sorted(concepts, key=lambda c: c.confidence, reverse=True)[:3]
    logger.info(f"Threshold too strict; taking top {len(filtered)} concepts as fallback.")
By implementing the above, the pipeline will retain more concepts from each source. Verify the effect by running the ingestion on a known document: previously where only 1-2 concepts appeared, you should now see a broader set of concepts (e.g., 10+ from a long PDF, depending on content richness). This directly addresses the problem of missing key concepts due to filtering.
Issue 3: Write-Time Cap of 5 Concepts in Memory Injection
Symptom: Even after concept extraction produces a list of concepts, only the top 5 are being written into the system’s memory or knowledge store. For example, if 15 concepts were identified from a video, the pipeline truncates the list to 5 when injecting into memory. This artificial cap means any concepts ranked 6th and beyond are dropped entirely. Impact: This severely limits TORI’s knowledge representation. Important concepts that are slightly lower-ranked than the top 5 are lost. In content with many topics, TORI will only remember a small fraction, possibly missing context needed for accurate responses or reasoning. Essentially, the system’s “memory injection” is bottlenecked, impairing the richness of the AI’s memory of the content. If a user asks about a concept that was 6th in importance in a document, TORI might not recall it at all because it was never stored. Cause: The cap was likely introduced as a precaution (perhaps to limit memory size or token count for prompt injection) or as a placeholder for testing. It might be implemented as a hard-coded slice or limit (e.g., concepts = concepts[:5]) just before writing to memory. In some cases, if a clustering approach was used, the code might have set a fixed number of clusters (k=5), effectively yielding five concepts. Either way, the design is too restrictive for general use. Proposed Fixes:
Remove or Raise the Fixed Cap: The simplest remedy is to eliminate the hard-coded limit of 5 concepts. The memory injection code should take all relevant concepts from the extraction stage. If there is a reason to limit the number (like performance or memory constraints), raise the limit to a higher number (e.g., 10 or 20) or make it configurable. In most cases, it’s better to store as many concepts as were found to be above the confidence threshold. Many key-phrase extraction algorithms return all identified key phrases by default
mathworks.com
, and only limit if asked – our system should do the same unless strictly necessary.
Configurable Top-N Setting: If completely removing the cap is not viable, turn it into a configuration parameter (e.g., MAX_CONCEPTS_TO_STORE). Set it to a higher default (for example, 15) initially. This way, the team can easily adjust if needed. For instance, define MAX_CONCEPTS_TO_STORE = None # no cap or = 15 in a config file, and in code do:
python
Copy
if MAX_CONCEPTS_TO_STORE:
    concepts_to_store = concepts_sorted[:MAX_CONCEPTS_TO_STORE]
else:
    concepts_to_store = concepts_sorted  # no truncation
By making it explicit, everyone is aware of the limit and can tune it.
Consider Dynamic Limits (Optional): If memory usage is a concern (say the downstream component can only handle so much data), implement a dynamic approach rather than an arbitrary cut. For example, limit by total character length or combined token count of concepts rather than a flat count of 5. Or base the number of concepts on the content length (e.g., allow more concepts for longer documents). This ensures we don’t drop information for large inputs while still keeping extremely large lists in check.
Adjust Clustering Strategy: If the “top 5” cap is a result of using a clustering algorithm with k=5, then refine the clustering. You could switch to an algorithm that finds an optimal number of clusters or set k based on content complexity. (We discuss better clustering in the Optional Enhancements section.) In the interim, if k=5 is hard-coded, increase k to a higher value (like k=10) to immediately allow more concepts, or use a simple heuristic (e.g., k = min(10, floor(N/2)) for N candidate concepts). This change would be in the clustering function call (for instance, if using KMeans, change n_clusters=5 to a higher number or a variable).
Target Files/Sections: Find where the concept list gets written to memory or output. This could be in a memory integration module (e.g., MemoryWriter.insertConcepts() or a section of code after concept ranking). Look for any code that explicitly takes only the first 5 items. Common patterns could be a slice (concepts[0:5]) or a loop that breaks after 5 iterations. Remove this limitation. For example, in a .NET context, if there’s a LINQ like .Take(5) when selecting concepts to save, eliminate it or change it to .Take(MaxConcepts) with MaxConcepts set higher. In Python, remove the slice or replace 5 with a larger number/config value. For instance, if we see code:
csharp
Copy
var topConcepts = allConcepts.OrderByDescending(c => c.Score).Take(5);
memoryStore.Write(topConcepts);
This should be updated to:
csharp
Copy
var topConcepts = allConcepts.OrderByDescending(c => c.Score);
memoryStore.Write(topConcepts);  // No Take(5), write all
(or use a configurable .Take(N) where N is higher or comes from config). After applying this patch, test the pipeline on a content piece that used to generate more than 5 concepts. Confirm that all concepts beyond the 5th are now present in the final memory storage or output. This can be verified by checking the length of the semantic_concepts.json output or the memory database entries – it should exceed 5 if the content merits it. The result is that TORI will retain a much richer set of concepts about each document, improving its contextual knowledge.
Issue 4: Missing Concept Metadata (Confidence, Method, Source)
Symptom: The stored concepts currently only include basic information (likely just a name/label, and perhaps an ID or embedding). Important metadata – such as confidence scores, the extraction method used, and the source location – are not preserved in the output. For example, in the concept JSON, you might see a concept “User Engagement Model” but not see how confident the system was about this being a key concept, nor whether it came from text analysis vs. an LLM, nor which page or timestamp in the source it was derived from. Impact: Lack of metadata makes it difficult to validate and utilize the concepts:
Without a confidence score, we cannot easily prioritize which concepts are most important or filter out borderline ones in later processing. We also can’t track improvements to extraction quantitatively.
Without knowing the extraction method, debugging is hard – if multiple strategies (e.g., clustering vs. direct keyphrase extraction) are used, we can’t tell which method yielded which concept or if a fallback was triggered. It also hampers future enhancements; for instance, if we introduce a new extraction algorithm, we’d want to label concepts by method to compare performance.
Missing source reference (page number, paragraph, or timestamp) means we lose the context of where the concept came from. This is problematic for auditing (we can’t quickly go back to the original document to see the context of a concept) and for any future features like highlighting relevant source text to users. It also makes concept lineage tracking impossible (see optional enhancements).
Cause: The pipeline likely constructs a concept object or JSON with minimal fields, dropping extra info either to save space or because it wasn’t considered necessary at first. Perhaps the underlying extraction library provided confidence scores but the code didn’t capture them, or once concepts were clustered the original scores were discarded. Similarly, the code may not be recording the origin of each concept – e.g., which page or time segment it came from – possibly because concepts might be merged from multiple places or just oversight. Proposed Fixes:
Augment the Concept Data Model: Modify the structure that holds a concept (could be a class Concept or a dict in Python) to include additional fields:
Confidence Score: Attach the numeric score or strength for each concept. For instance, if using an embedding clustering approach, you might use the cluster “resonance_score” or a normalized frequency as the confidence. If the extraction method itself provides a score (like an ML model probability or TextRank score), use that. Even a relative ranking or percentile can be stored if an absolute score isn’t available. This will allow us to later say “Concept A had confidence 0.92 vs Concept B 0.47” which is valuable for debugging and possibly for downstream logic (maybe only use high-confidence concepts in certain critical reasoning paths, etc.).
Extraction Method: Define an enumeration or simple string to note how the concept was extracted. Examples: "embedding_cluster" for concepts derived from embedding clustering of text, "LLM_summary" if an LLM was used to suggest key concepts, "OCR" if from image text, etc. In the current pipeline, if all concepts come from one method, this might seem trivial, but adding it now allows for future flexibility. If the pipeline has a fallback (say, if clustering yields nothing, it tries another method), then we can mark those accordingly. This field should be set at the point of extraction. For instance, in the code where concepts are appended or created, add method: "embedding_cluster" (or appropriate label).
Source Reference: Store where in the source the concept originated. For text documents (PDFs), this could be page number and maybe section/paragraph identifier. For example, source: {"page": 4, "paragraph": 2} or a simple “page 4” if that’s as granular as needed. For transcripts, use timecodes, e.g., source: {"start_time": "00:02:10", "end_time": "00:02:30"} for the segment where the concept was mentioned. If the concept was mentioned in multiple places, one approach is to store the first occurrence or a list of occurrences (IDs of segments) – the latter is more complex but very useful. At the very least, storing one representative source location or the originating segment ID is much better than nothing.
Preserve Context Snippet (if possible): It appears there is a "context" field in the JSON already (likely a snippet of text around the concept). Ensure this remains, and ideally, ensure it corresponds to the source reference. For example, context could be “...Adaptive UI with individualized human feedback significantly enhances User Engagement...”, source says page 12. This combination lets an engineer or user trace the concept back.
Implement in Code: In the concept assembly section (likely where the pipeline compiles final concepts), add these fields. For instance, if currently doing:
python
Copy
concept = {
    "name": concept_phrase,
    "embedding": vector,
    ... 
}
expand it to:
python
Copy
concept = {
    "name": concept_phrase,
    "confidence": score,
    "method": extraction_method,
    "source": source_ref,
    "embedding": vector,
    "context": snippet,
    ...
}
You may need to gather this information: ensure the score or confidence is carried through (from filtering or ranking step), ensure the code knows the extraction_method (could be a constant if single method, or passed in if multiple), and have the source_ref available (perhaps the loop that processes segments can pass an identifier into the concept extractor). You might modify the signature of the concept creation function to accept context info. For example, in a PDF loop: extract_concepts(text, page_num=3) and then within that, attach source: {"page": page_num} to each concept found in that text. For transcripts: maybe each segment has a start time stored, so pass that in.
Propagate Metadata Through Pipeline: If there are intermediate steps (like clustering merges concepts), take care to propagate or combine metadata. For example, if two similar raw keyphrases from pages 2 and 5 get merged into one cluster concept, how do we store the source? One approach: list both pages (source could be an array of locations) or pick the page where it was most strongly mentioned. For now, even a single representative source is fine, but make a note in code/comments that if one concept comes from multiple places, we might extend the schema to handle multiple sources in future. Similarly, if confidence scores are combined (like cluster average), decide what to store (maybe the highest member score or the cluster’s own score). The key is not to drop the info entirely.
Update Output Writers: Ensure that when writing the concept JSON or saving to the memory database, these new fields are included. This might involve updating a serialization function or adjusting a database table schema if one is used. For JSON output, it should be automatic if we add to the dict, but double-check any code that explicitly lists fields to output. For a database, you might need to add new columns (confidence, method, source) to the concepts table or JSON blob. Coordinate with the database if so (this might require a migration or at least an update query to add fields, if using something like a document store ensure new keys can be added without breaking consumers).
Verify in Logs/Output: After implementing, run a test ingestion and inspect the resulting semantic_concepts.json (or memory entry). You should now see entries like:
json
Copy
{
  "name": "Adaptive User Interfaces",
  "confidence": 0.76,
  "method": "embedding_cluster",
  "source": {"page": 1},
  "context": "…Adaptive UI with individualized human feedback…",
  "embedding": [0.123, …],
  "cluster_members": [ ... ],
  "resonance_score": 0.8,
  "narrative_centrality": 0.5
}
This richness will greatly aid in debugging and future processing. Confidence and method let us gauge trust in each concept, and source/context allow pinpointing the origin if questions arise.
Target Files/Sections: The primary changes are in the concept data construction code – likely in the part of the pipeline after extracting or clustering the concepts. For example, if using Python, this might be in concept_extractor.py where the final list of concepts is compiled. If using C#, it might be in a class that models a Concept (add properties) and in the logic that creates new Concept instances (populate those new fields). Specifically:
Add new properties to the Concept class or keys in the concept dictionary.
Pass necessary context info (confidence, method, source) into the creation of each concept. This might involve minor changes in upstream functions (e.g., have the PDF parser provide the page number to the concept extractor).
Update any function that filters or sorts concepts to now use the confidence field explicitly if it wasn’t before (since previously it might have just been implicit or done via an external array).
Update output formatting: e.g., if there is a function save_concepts_to_json(concepts), ensure it does not omit fields. If there is a structured output to a UI, confirm it can handle the new fields (perhaps not immediately needed, but good to note).
After these patches, the concept metadata will be fully preserved. This not only helps developers but also opens possibilities for richer UI (showing confidence to end-users or internal dashboards) and for more intelligent post-processing (like maybe using confidence to weigh which concepts to emphasize). It’s a foundational improvement for quality assurance of the ingestion pipeline.
Issue 5: Lack of Diagnostic Tooling for Ingestion Quality
Symptom: There are no dedicated tools or scripts to easily verify the quality of concept ingestion before and after making changes. In other words, after deploying the above fixes, we need a way to confirm that they indeed improved the situation (more concepts captured, logs present, etc.). Currently, engineers would have to manually comb through logs or outputs to gauge this, which is tedious and error-prone. Impact: Without diagnostic tooling, it’s hard to measure progress or catch regressions. For example, if a future change accidentally disables concept extraction again, there’s no automated check that would raise a red flag (until a human notices missing concepts). Also, to convincingly demonstrate that our patches worked, we want quantitative and qualitative evidence (e.g., “Document X now yields 12 concepts instead of 2, and log shows all segments processed”). Lacking tools, the team might overlook subtle issues or not realize if something is still off in certain media types. Cause: This is not a code bug per se, but a gap in our development/QA process. Historically, perhaps the system was small enough to test manually, or the focus was on building features rather than measuring them. Now that we are refining the pipeline, it’s clear we need some automated diagnostics. Proposed Fixes (Tooling):
We will create simple diagnostic scripts and logging enhancements to validate ingestion. These tools will be used during development and possibly in production monitoring to ensure concept extraction is performing as expected. The approach includes:
Logging-Based Diagnostics: Leverage the improved logging from Issue 1 to track ingestion behavior. For instance, we can search the logs for the presence of our new entries (loopRecord and concept counts). A PowerShell one-liner can continuously monitor logs:
powershell
Copy
Get-Content -Path "ToriIngestion.log" -Wait | Select-String "LoopRecord"
This would live-tail the log and show each segment processed in real-time. Similarly, after an ingestion job completes, one can run:
powershell
Copy
Select-String -Path "ToriIngestion.log" -Pattern "extracted total"
to find the line where the total concept count was logged. This confirms not only that the pipeline ran fully, but also how many concepts were produced. We should document these log message patterns for the team so they know what to grep for when verifying runs.
Automated Concept Count Checker: Develop a small Python or PowerShell script that reads the output semantic_concepts.json file and reports the number of concepts and some sample data. For example, a Python script diagnose_concepts.py could do:
python
Copy
import json, sys
file = sys.argv[1]  # path to concepts json
data = json.load(open(file))
print(f"Total concepts: {len(data)}")
# List concept names and confidences
for c in data[:10]:  # show first 10
    name = c.get('name')
    conf = c.get('confidence')
    print(f"- {name} (confidence: {conf})")
Running this on a concept output file from before and after the fixes will clearly show the difference (e.g., before: “Total concepts: 3”, after: “Total concepts: 12” plus the list). We can make this script accept multiple files to compare results side by side. In PowerShell, similar can be done using ConvertFrom-Json. For instance:
powershell
Copy
$concepts = Get-Content "semantic_concepts.json" -Raw | ConvertFrom-Json
Write-Host "Total concepts extracted:" $concepts.Count
$concepts[0..4] | ForEach-Object { Write-Host "- $($_.name) (conf:" $_.confidence ")" }
This prints the count and first 5 concept names with confidence. Such a script can be run after each ingestion as a quick quality check.
Cross-Run Comparison: Extend the tooling to compare ingestion output from before vs. after patch (if we have a saved baseline). For example, if we saved the old output JSON and the new output JSON for the same content, a script can diff the concept lists. This could simply sort the concept names and compare, or do a more sophisticated comparison of counts and any common vs. new concepts. This will highlight improvements (e.g., “+9 new concepts extracted after fixes”). Even without a saved baseline, we can use a known expected outcome as a reference (for instance, if a document has known key topics, ensure they appear in the concept list).
Diagnostic Mode in Code (Optional): Introduce a verbose or debug mode for the ingestion pipeline that can be toggled for testing. In this mode, the pipeline could output extra info (like the full list of initial candidate phrases, intermediate scores, etc.). This isn’t for regular use but for developers tuning the system. For example, a flag DIAGNOSTIC_MODE=true could cause the pipeline to dump a detailed log or a second JSON with intermediate data (like raw_concepts.json before filtering/clustering). This helps in verifying that each stage (extraction → filtering → clustering → final) is working as intended. Given this is optional, it can be implemented if the team anticipates doing a lot of tweaking and wants insight without stepping through a debugger.
Using the Tools:
After implementing the fixes (Issues 1–4), run the ingestion on a set of test documents (covering PDF, video transcript, audio transcript). Use the above scripts to collect metrics: number of concepts, presence of logs, etc. Check that for each test file:
Logs show all expected entries (loopRecord for each segment, and a final concept count).
The concept count is reasonable (not zero or excessively low). For example, a 10-page PDF might now yield ~10–20 concepts instead of 2. A 5-minute video transcript might yield, say, 5–15 concepts depending on content density.
The concepts have the new metadata fields (open the JSON and see confidence, source, etc. present for each).
No obvious important topic is missing: do a quick sanity check that major topics from the content appear in the concept list. (This is a qualitative check; in future we could use an NLP model to summarize content and see if our concepts overlap with the summary, but that’s beyond current scope.)
It would be wise to integrate some of these diagnostics into a unit or integration test. For instance, write a test that ingests a small known text (maybe a sample paragraph with 3 known key points) and assert that at least those 3 key points appear in the concept output. Similarly, a test can assert that semantic_concepts.json is not empty and log contains the expected lines after an ingestion run. This will prevent regressions going forward.
Live Monitoring: In a production or staging environment, consider adding a periodic check or alert. For example, if an ingestion job completes but produces 0 concepts, trigger an alert to engineers. This can be done by parsing logs or the output count. Likewise, if a normally medium-sized document suddenly produces an extremely low concept count, flag it – it could indicate an issue with the pipeline or an unusually formatted document. Having these as part of monitoring ensures the improvements remain in effect and any future issue is caught quickly.
Target Files/Sections for Tools: These tools are mostly new auxiliary scripts rather than modifications to existing pipeline code (except the optional diagnostic mode flag in code). We will create scripts like diagnose_concepts.py (as described above) and perhaps a PowerShell script Test-IngestionQuality.ps1 that runs an ingestion on a test file and prints out the results. If the ingestion can be invoked via command-line, the PowerShell script could even automate running it on sample files and then calling ConvertFrom-Json to evaluate output. Document these scripts in the repository (perhaps in a utils/ or diagnostics/ folder) so the team can use them. Ensure any credentials or file paths needed are configurable (for example, if the pipeline requires a database or API call, the diagnostic might run in a lower environment with test data). By implementing these diagnostics, we gain confidence that the concept ingestion pipeline is performing correctly. We’ll be able to measure the before/after improvements (e.g., concept count increased, richer data captured) and quickly catch any similar issues in the future. This tooling turns our fixes into a sustainable improvement with proper verification.
Optional Enhancements and Future Improvements
In addition to the critical fixes above, we identified several opportunities to further enhance TORI’s concept ingestion. These are not immediately required for functionality, but they can significantly improve the quality and maintainability of the system going forward:
Enhanced Clustering Algorithm: The current approach to clustering or grouping concepts may be simplistic (possibly a fixed number of clusters or a basic KMeans). We can improve this by using a more adaptive clustering technique. For example, consider using a density-based algorithm or hierarchical clustering that can determine the number of concept groups based on the data (instead of a fixed k). This would allow the pipeline to yield a variable number of high-level concepts corresponding to the actual themes in the content. An algorithm like HDBSCAN or affinity propagation could be explored, as they don’t require preset cluster counts. At minimum, we could adjust the clustering parameters (distance thresholds or number of clusters) based on content length – e.g., more clusters for longer documents to capture more distinct topics. Improved clustering will ensure that concept grouping is meaningful and that we’re not merging unrelated ideas or limiting to too few groups. It may also produce cleaner concept “names”: rather than a concatenation of multiple terms or a long phrase, we could derive a concise label for each cluster (perhaps by picking the most central phrase). This will result in more intelligible concepts in the output.
Improved Concept Scoring and Ranking: We should refine how we score and rank concepts. Currently, it seems we rely on whatever scoring comes from the extraction method (like internal model confidence or frequency-based scores). We could introduce composite scoring that accounts for multiple factors:
Frequency/Salience in text: how often or prominently the concept appears in the document (e.g., in titles or repeatedly mentioned).
Semantic centrality: if we have a narrative centrality measure (as hinted by a narrative_centrality field), we should implement it properly. For instance, measure how widely the concept is spread across the document (appears in many sections -> high centrality) versus just in one spot. This can help distinguish truly key themes from very localized terms.
Confidence from model: any model-generated confidence or relevance score (like TextRank value, embedding cluster strength, etc.).
User-defined or domain knowledge (later): e.g., certain keywords might always be considered important if present (domain-specific terms).
By combining these, we can produce a more robust ranking of concepts. This means the pipeline will prioritize concepts that are not only identified by the algorithm but also are clearly central to the document’s meaning. We could output this as an improved confidence metric. For example, if narrative centrality wasn’t being used, we can start using it to adjust the final score. All this will further ensure that our concept filtering (Issue 2) is intelligent – removing truly irrelevant candidates but keeping those that matter.
Concept Lineage and Traceability: Building on the metadata preservation (Issue 4), we can enhance concept lineage logging. This means being able to trace how each concept was formed and from where. We can log, at debug level, the full life cycle of a concept:
e.g., “Raw phrase ‘user engagement model’ extracted from page 5, score 0.4” (initial extraction),
“Merged with similar phrase ‘model of user engagement’ from page 8 into cluster 3” (clustering step),
“Cluster 3 representative chosen as ‘User Engagement Model’ with combined score 0.75” (final outcome).
Such detailed logging (or a structured trace data) would be invaluable for developers to troubleshoot why certain concepts appeared or why some were dropped. It could be turned on in diagnostic mode due to verbosity. Additionally, storing lineage info in the data (e.g., listing all source locations or all variant phrases that were merged into that concept) is an idea. This would give end-users or future features the ability to say “this concept covers these specific terms in the document.” While not needed for core function, lineage data paves the way for transparency (important in explainable AI) and more sophisticated uses (like automatically linking the concept back to snippets in a UI).
Continual Evaluation Mechanism: As a longer-term practice, consider integrating a routine evaluation for concept extraction. For instance, maintain a small corpus of test documents with known “ground truth” key concepts (perhaps identified by humans or by reliable external services). After any major changes, run the pipeline on them and compare. This ties into the diagnostic tooling but formalizes it into a regression test suite for concept ingestion. Over time, this helps quantify improvements (e.g., “we now capture 90% of the key topics on average, up from 70% before”). It can also alert if a change inadvertently drops performance.
Implementing the above enhancements will further solidify TORI’s concept ingestion capability. They ensure that not only have we fixed the immediate problems, but we are also moving towards a more intelligent, transparent, and tunable system. Each enhancement can be tackled after the primary patches are in place, and tested gradually for effectiveness.
By following this triage checklist and applying the recommended patches, the engineering team will restore TORI’s concept ingestion across PDFs, videos, and audio sources to full functionality. We will regain comprehensive logging, extract a richer set of concepts (with all relevant metadata), and have tools to verify and maintain quality. These improvements lay a strong foundation for TORI’s knowledge ingestion, enabling more robust performance and easier future enhancements. The optional improvements can then be pursued to continuously refine the concept extraction quality, ensuring TORI keeps getting smarter and more reliable in capturing the essence of diverse media content.