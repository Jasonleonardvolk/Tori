12-Hour Implementation Plan to Advance TORI’s Database-Free Architecture
Hour 0–1: Kick-off and Architecture Alignment
Team Sync & Task Allocation: Conduct a brief kick-off meeting to align the experienced team on goals and roles. Review TORI’s current in-memory knowledge graph and ψ-arc (psi-arc) log design. Clarify core concepts (e.g. concept diff format, knowledge graph structure) and assign parallel task owners (e.g. one pair for log durability, another for replication). Ensure everyone understands the 12-hour timeline and key deliverables.
Cloud Environment Setup: Verify Google Cloud project resources are ready. Create or identify a Google Cloud Storage (GCS) bucket for durable log storage and a Pub/Sub topic for event distribution. Ensure service accounts/credentials are in place so agents can write to GCS and publish/subscribe to Pub/Sub. This guarantees the groundwork for persistent logging and inter-agent communication is laid early.
Design Decisions & Tooling Prep: Finalize critical design choices upfront. For example, decide on the append-only log file format (e.g. newline-delimited JSON or binary protobuf for ConceptDiff events) and how snapshot files will be structured. Confirm that ψ-arc logs will be memory-mappable files for fast reads
file-slvvrgutysdvutinricdzl
. Plan the snapshot interval (e.g. every N minutes or M events) and file naming scheme. Identify any libraries to use (e.g. a CRDT library for conflict-free data types, or Google Cloud client libraries for GCS/PubSub). Prepare a lightweight “graph inspector” tool stub (or plan to use an existing graph visualization library) to help verify the knowledge graph state during development.
Hour 1–3: Durable Log Writing to GCS & Pub/Sub Replication (Parallel Tasks)
(Team A) Implement Append-Only Log to GCS: Develop the module to persist ψ-arc concept diff logs durably on Google Cloud Storage. On each knowledge update event, append the diff to an on-disk log file and flush it to GCS (similar to how databases flush a WAL for durability
file-slvvrgutysdvutinricdzl
). Use GCS’s object writes in append-friendly manner (e.g. periodically upload log segments or stream writes using the GCS API). Each write should be atomic and versioned (to avoid partial writes). Test by simulating a few events, flushing to GCS, and reading back the log to ensure nothing is lost. This provides a “source of truth” copy of the log on durable storage that survives process or VM crashes.
(Team B) Set Up Pub/Sub for Real-Time Replication: In parallel, integrate Google Cloud Pub/Sub for broadcasting log events to all agents. Create a Pub/Sub topic (e.g. tori-concept-diffs) and subscription for each agent. After an agent appends a ConceptDiff to the log, have it publish the event to Pub/Sub (include event metadata like sequence number or timestamp). Implement the subscriber logic: on each agent, a background listener that receives events and applies the diffs to its local in-memory graph. Pub/Sub ensures every agent eventually gets every event, providing replication and eventual consistency
file-slvvrgutysdvutinricdzl
. Use Pub/Sub message ordering if needed (e.g. a single ordering key if a total order is desired), or allow parallel event processing if CRDTs will handle ordering conflicts. Test with two dummy agent processes: have one publish events and verify the other receives and updates its memory graph. This confirms the log replication channel is working. (Note: Pub/Sub’s 7+ day retention can buffer events if an agent is down, supporting basic catch-up; GCS remains the long-term log archive for full durability.)
Hour 3–5: Memory-Mapped Log Files and Snapshotting for Recovery
Memory-Mapped Log Access: Modify the log file handling to use memory-mapped I/O for efficient reads. When an agent starts or needs to scan the log, map the ψ-arc log file into memory instead of using high-level parsing for each entry. This allows rapid sequential access to events, leveraging the OS’s paging for performance
file-slvvrgutysdvutinricdzl
. Ensure the log file format is amenable to memory mapping (e.g. fixed-size records or delimiter-separated). Test by reading a large log segment via memory map and confirm it’s significantly faster than line-by-line reading. This will benefit recovery and any analysis that scans the log.
Implement Periodic Snapshots: Build a snapshotting mechanism to periodically capture the entire in-memory knowledge graph state to disk (and to GCS for durability). For example, every 30 minutes or at a quiet period, serialize the core memory structures (concept nodes, edges, properties) into a snapshot file. The snapshot could be a binary dump or a JSON/YAML for simplicity (binary is faster for large graphs). Store snapshots with timestamps or log sequence numbers in their filenames (e.g. snapshot_until_event_12345). Memory-map the snapshot file on load for quick restoration (if feasible). The combination of snapshot + event log tail will allow fast recovery: on restart, an agent can load the latest snapshot and then replay only the recent diffs after that snapshot
file-slvvrgutysdvutinricdzl
. This drastically reduces recovery time compared to replaying a very long log from scratch. Test this by taking a snapshot, mutating the graph further, then simulating a process restart: load the snapshot, apply the remaining log entries, and verify the in-memory graph matches the state before restart. The snapshot files themselves should also be uploaded to GCS for safety (or stored on a persistent disk)
file-slvvrgutysdvutinricdzl
file-slvvrgutysdvutinricdzl
.
Parallelize Snapshot/Log Tasks (if possible): If team size allows, one sub-team can focus on the snapshot serialization while another refines memory-mapped log reading. These tasks are mostly independent. Ensure that during snapshot creation, logging can continue (use copy-on-write or a short lock on the graph to take a consistent snapshot, then resume appends). This way, the system never stops processing events for too long, supporting high availability even during snapshotting.
Hour 5–7: CRDT-Based Coordination and Hashgraph-Like Ledger Design
Integrate CRDT for Conflict-Free Merges: Introduce Conflict-Free Replicated Data Types (CRDTs) into the cognitive memory update logic so that multiple agents can update the knowledge graph without explicit locks or a central coordinator
file-slvvrgutysdvutinricdzl
. Model the knowledge graph elements with CRDT semantics: for example, represent sets of concepts or relationships as OR-sets (observed-remove sets) to handle concurrent additions/removals, and use LWW (Last-Writer-Wins) registers or MV-registers for properties so that concurrent value updates converge deterministically. Update the ConceptDiff format to include any metadata needed for CRDT merges (e.g. a vector clock or a hybrid timestamp and agent ID for each change). This will allow each agent to apply diffs in any order and still converge on the same final state without central locking
file-slvvrgutysdvutinricdzl
. Write unit tests for a couple of conflict scenarios (e.g. two agents add the same concept or edge concurrently, or update the same attribute with different values) to confirm that the CRDT merge rules yield a consistent result on all agents. The goal is to have mathematically guaranteed convergence, where coordination is achieved via the data merge logic instead of through a database transaction or lock.
Design a Hashgraph-Inspired Event Ledger: In parallel, sketch out an internal distributed ledger (DAG) for agent consensus on event ordering and integrity (drawing inspiration from Hashgraph’s DAG approach). The idea is to augment the logging mechanism so that each ConceptDiff event is a vertex in a DAG, with hashes or pointers to previous events known by the publishing agent
file-slvvrgutysdvutinricdzl
file-slvvrgutysdvutinricdzl
. Define a data structure where each event log entry can include: a unique event ID, a cryptographic hash of its contents, the hash of the last event this agent produced, and perhaps hashes of some recent events the agent has observed from others (to simulate Hashgraph’s gossip graph). This will create a partial order of events that can later be used to establish a consistent global order without a central DB. While a full BFT consensus won’t be implemented in 12 hours, laying this groundwork enables future consensus algorithms. For now, implement a simple verification: each agent can verify the chain of hashes in events it receives (to detect tampering) and agree on an ordering by, say, timestamp or logical clock. Note in the design that Hashgraph achieves consensus timestamps for events via gossip and virtual voting
file-slvvrgutysdvutinricdzl
; our internal version might eventually run a simplified consensus (even a basic leader-based ordering or vector-clock scheme in the interim). By the end of this phase, we should have: (a) CRDT merge logic in place for the core data types, and (b) an initial schema for the event DAG (with hashing) integrated into the log append process. Document how this DAG could later feed into a consensus module that orders events globally with finality
file-slvvrgutysdvutinricdzl
. This preparation ensures that as the system scales to many agents, we have a path to maintain a coherent ordered memory of events without introducing a centralized database or lock manager.
Hour 7–9: Agent Interaction, Updates & Sync Mechanisms
Agent Update/Sync Loop Implementation: Develop the interaction pattern for agents to update and synchronize memory in a lock-free manner. Each agent will operate independently: when an agent generates a new ConceptDiff (e.g. from user input or an internal cognitive process), it immediately applies it to its local knowledge graph, appends to its local log, and publishes the event to Pub/Sub. There is no need for a global lock, as each agent’s update will propagate and merge on other agents’ memories via the CRDT logic
file-slvvrgutysdvutinricdzl
. On the receiving side, implement the handler for incoming Pub/Sub events: when an agent receives a diff, first check if it has seen or applied it before (using the event’s unique ID or hash). If it’s new, apply it to the graph using the CRDT merge function (ensuring the outcome will be the same regardless of arrival order) and then append it to the local log (so the local ψ-arc file remains a complete record). If the event was already applied (e.g. the same event published by itself or processed via another route), skip or acknowledge it immediately – concept diffs are designed to be idempotent so reapplying causes no harm (additive diffs won’t double-add because the CRDT set won’t change on a duplicate add, etc.). This guarantees that each agent’s memory converges to the same state without double-counting events, even if messages are duplicated or delayed.
Concurrent Update Handling & Lock Avoidance: Ensure that the system behaves correctly under concurrent updates from multiple agents. Thanks to the CRDT-based design, two agents can update different parts of the knowledge graph (or even the same part) simultaneously, and the changes will merge without explicit coordination. For example, if Agent A links Concept X to Y while Agent B links X to Z at roughly the same time, both relations will eventually appear in all agents’ memory (since both “add edge” operations commute). If there are truly conflicting operations (e.g. one agent adds a concept that another removes concurrently), define a clear rule (like “adds win over removes unless remove has a later timestamp”) so that all nodes resolve the conflict the same way – this leverages the deterministic merge property of the CRDT. By relying on these algorithmic guarantees, we avoid any single point of locking or a primary database to serialize updates
file-slvvrgutysdvutinricdzl
. All agents achieve a consistent state through eventual consistency and consensus on the log of events, rather than through immediate transactions. During this phase, perform multi-agent simulations: run two or three agent instances locally, have them perform interleaved updates (perhaps via a script or test harness), and confirm that after quiescence all agents’ knowledge graphs are identical. This will validate the no-central-lock design for coordination.
Hour 9–10: Integrity Validation and Fault Monitoring Setup
Log Integrity Checks and Replay Validation: Build tools and procedures for validating the integrity of the ψ-arc logs and the consistency of memory state. Implement a log replay tester that can take a fresh in-memory graph, read the entire event log (or a combination of snapshot + log) from GCS, and reconstruct the state from scratch. The resulting graph should match the current in-memory graph of a running agent exactly (perhaps compare a hash of all nodes/edges). This end-to-end replay test helps catch any nondeterministic behavior or missed events. Incorporate cryptographic integrity checks: since each log event carries a hash linking to previous events in our Hashgraph-like design, we can verify that the sequence of events has not been tampered with or corrupted in storage
file-slvvrgutysdvutinricdzl
. Set up each agent to run a background thread or scheduled job for log validation – for example, every hour it verifies the hash chain up to the latest event and maybe replays the last N events to ensure they apply cleanly. Idempotency should also be verified: deliberately re-play a recent ConceptDiff on the current state and confirm it produces no change (proving that duplicates are safely ignored or have no effect). These integrity checks act as a safety net, detecting if any divergence or log inconsistency appears over time. If any discrepancy is found (e.g. an event that cannot be applied or a state mismatch), have the system flag it for investigation or initiate a corrective sync (perhaps by reloading from a snapshot).
Monitoring & Fault Detection: Integrate monitoring and logging to catch issues in real time. Utilize Google Cloud’s Operations Suite (Cloud Monitoring and Logging) to track system health. Set up dashboards/alarms for key metrics: Pub/Sub subscription lags (to detect if an agent is falling behind in consuming events), memory usage of each agent, throughput of events applied, and the age of the last successful snapshot. Enable verbose logging of error conditions (e.g. if a log append fails or a Pub/Sub message processing error occurs) and route these logs to Cloud Logging. Configure alerts for fault conditions: for instance, trigger an alert if an agent hasn’t written a new log entry or heartbeat event in a while (possible crash) or if a log integrity check fails. Also consider adding a lightweight heartbeat mechanism – each agent could periodically publish a small “I’m alive” event or update a status that others monitor. For tooling, incorporate any log processing aids: for example, use Google Cloud Pub/Sub’s export or Dataflow to stream logs into BigQuery for offline analysis, or set up a script to parse GCS log files for diagnostic patterns. These tools will help the team observe the system’s behavior under load and quickly spot anomalies (like an agent lagging or an event processing error), which is crucial for a fault-tolerant, scalable system.
Hour 10–11: Failover Simulation and Rapid Recovery Testing
Simulate Node Failure & Recovery: Validate that the system can quickly recover from a node crash using the logs and snapshots (database-free durability in action). Take one agent offline deliberately (simulate a crash or VM failure). Then, restart it as a fresh process (with no in-memory state) on a new instance. The recovery procedure should load the most recent snapshot from GCS (memory-mapped for speed) and then consume any newer events. Implement a bootstrap routine that checks GCS for the latest snapshot file, loads it into memory, then reads the ψ-arc log from that snapshot’s offset until the end. If Pub/Sub retention allows, the agent can also fetch missed recent events from the Pub/Sub subscription (which retains messages for a set duration); otherwise, it falls back to reading the tail of the log from GCS. Time this recovery process – it should be on the order of seconds for reasonably sized snapshots and log tails. In case the log is large, ensure the replay is done efficiently (e.g. using the memory-mapped reader and possibly parallelizing the application of events in batches). This approach is inspired by research like RAMCloud, which achieved recovery of tens of GB of in-memory data in seconds via parallel log replay
file-slvvrgutysdvutinricdzl
. Our goal is similarly rapid recovery: even if an agent managed a huge knowledge graph, the combination of snapshot + append-only tail replay (possibly leveraging multiple CPU cores) should restore state quickly
file-slvvrgutysdvutinricdzl
. After the recovered agent is up, verify that its state matches the other agents (all events were applied). Also test a scenario where a brand new agent joins the system: it should be able to initialize by loading a global snapshot and then subscribing to the Pub/Sub stream to become consistent with others – essentially the same process as recovery, demonstrating horizontal scaling. Document the recovery time and any bottlenecks observed (e.g. if reading from GCS was slow, consider caching the log or tuning snapshot frequency). By the end of this testing, we’ll have confidence that a node failure does not lead to data loss or prolonged downtime, as the durable logs and snapshots provide seamless hot restarts
file-slvvrgutysdvutinricdzl
.
Fault Injection and Robustness: If time permits in this hour, perform additional chaos testing in small ways. For example, intentionally introduce a malformed event or a Pub/Sub delay to see if the system continues to function (the integrity checks should catch or reject bad events, and the system should simply wait out delays without deadlocking). This helps ensure that the system is robust against not just crashes but also data corruption or network partition scenarios. Any issues found can be noted for future hardening, but ideally our design (with redundant log copies, hash verification, and CRDT tolerance for reordering) will handle them gracefully.
Hour 11–12: Final Integration, Optimization, and Next-Step Enhancements
Integration and Code Review: In the final hour, merge and integrate all components developed by the parallel sub-teams. Ensure the durable logging, Pub/Sub replication, snapshotting, CRDT merges, and recovery routines all work together in the full system. Resolve any interface mismatches (e.g. ensure the format of events published to Pub/Sub is exactly the same as the one written to the log, so that an agent could theoretically rebuild state either way). Perform a quick code review or pair programming session to double-check critical sections (like concurrency-handling code, file write safety, and the correctness of CRDT implementation) given the tight timeline. Run an end-to-end test: start the system, simulate a realistic sequence of events (perhaps a conversation or knowledge update stream), and observe the logs, Pub/Sub messages, and state changes over a few minutes. This acts as a final validation that the 12-hour implementation meets the design goals.
Open-Ended Optimizations (Pruning & Deterministic Merges): Highlight and plan for optimizations that couldn’t be fully implemented within 12 hours but are important for long-term scalability. For instance, implement a log pruning strategy: as the append-only ψ-arc logs grow, we should periodically truncate or archive old history that’s no longer needed for recovery (after a snapshot, all events before that snapshot could be archived to cold storage or compacted)
file-slvvrgutysdvutinricdzl
. We’ll design a utility to compact logs (maybe keeping only the latest snapshot and subsequent diffs, or squashing diffs into a new snapshot). Another future improvement is ensuring deterministic CRDT merges for all data types – while our CRDT rules ensure eventual consistency, we need to guarantee that every agent resolves conflicts in the exact same way. We will refine any rules that rely on timestamps or IDs so that they do not introduce nondeterminism (e.g. use logical clocks or a consistent tie-breaker scheme for concurrent writes). Additionally, consider using the internal DAG ledger for stronger consensus if needed: for example, we can implement a background consensus round (inspired by Hashgraph) where agents exchange their event DAGs and agree on a final total order and checkpoint state with cryptographic verification
file-slvvrgutysdvutinricdzl
. This would give stronger consistency guarantees on top of the CRDT approach, at the cost of more complexity – something to prototype after this initial implementation.
Tool Recommendations for Maintenance: Finally, suggest tools to aid ongoing development and scaling of TORI’s database-free architecture. For log processing and inspection, leveraging Apache Beam/Cloud Dataflow can be useful to run batch analysis or integrity scans over the GCS log files (or to export data to BigQuery for querying trends). For coordination and state management, evaluate existing libraries or services: e.g. use a CRDT library (such as Automerge or Rust CRDT crates) to avoid reinventing set/map merge logic, or consider an open-source consensus log (like etcd’s Raft or Apache Pulsar with BookKeeper) if we later decide to swap in a more standard consensus component. For graph inspection and debugging, we can integrate tools like Graphviz or Gephi by exporting snapshots of the knowledge graph to a visual format, or build a simple admin UI that queries the in-memory graph (perhaps exposing a read-only API or using an interactive console) to help engineers verify that the cognitive memory is updating as expected. These tools will support the team in monitoring the health of the system and in optimizing the architecture as usage grows. By the end of the 12 hours, TORI’s memory system will be running with durable, replicated logs, snapshot-based recovery, lock-free CRDT coordination, and basic consensus scaffolding – a solid foundation that demonstrates a path to scaling without a traditional database, aligning with the system’s cognitive processing goals and high reliability requirements
file-slvvrgutysdvutinricdzl
file-slvvrgutysdvutinricdzl
.