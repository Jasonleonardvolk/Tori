{
  "metadata": {
    "processing_date": "2025-06-06T07:33:43.820255",
    "total_documents": 11,
    "total_pages": 44,
    "ingestion_stats": {
      "processed_pdfs": 10,
      "failed_pdfs": 0,
      "processing_rate": 0.15090968178034642
    }
  },
  "pdf_documents": [
    {
      "file_path": "C:\\Users\\jason\\Desktop\\tori\\kha\\data\\2.pdf",
      "title": "2",
      "pages": 10,
      "size": 957756,
      "status": "success"
    },
    {
      "file_path": "C:\\Users\\jason\\Desktop\\tori\\kha\\data\\USB Drive\\docs\\banksy_equations_v10.pdf",
      "file_type": "pdf_document",
      "title": "untitled",
      "author": "anonymous",
      "subject": "unspecified",
      "pages": 1,
      "text_content": "Banksy Equations v1.0\nθ_i(t+∆t)=θ_i(t)+∆t[ω_i+Σ_j K_ij sin(θ_j−θ_i)]\n\n\n",
      "concepts": [
        "banksy equations"
      ],
      "document_type": "general_document",
      "content_hash": "a929e0736d727e24",
      "size": 1785,
      "ingestion_time": "2025-06-06T07:33:42.243436",
      "research_field": "mathematics",
      "technical_level": "undergraduate",
      "citation_count": 0
    },
    {
      "file_path": "C:\\Users\\jason\\Desktop\\tori\\kha\\data\\USB Drive\\docs\\tutorial_intro.pdf",
      "file_type": "pdf_document",
      "title": "untitled",
      "author": "anonymous",
      "subject": "unspecified",
      "pages": 1,
      "text_content": "LCN Tutorial: Introduction\nWelcome to the Large Concept Network tutorial PDF.\n\n\n",
      "concepts": [
        "research",
        "introduction",
        "lcn tutorial",
        "academic_paper",
        "introduction\nwelcome to the large concept network tutorial pdf"
      ],
      "document_type": "research_document",
      "content_hash": "47cb5ec26c925a1c",
      "size": 1635,
      "ingestion_time": "2025-06-06T07:33:42.243436",
      "research_field": "general",
      "technical_level": "general_public",
      "citation_count": 0
    },
    {
      "file_path": "C:\\Users\\jason\\Desktop\\tori\\kha\\data\\USB Drive\\may1move\\Teach\\Learn2\\document.pdf",
      "file_type": "pdf_document",
      "title": "Context.Revised.pdf.source",
      "author": "Christian Balkenius",
      "subject": "",
      "pages": 10,
      "text_content": "To appear in Jean-Arcady Meyer, Alain Berthoz, Dario Floreano, Herbert L. Roitblat, Stewart W. Wilson, (Eds) From Animals to Animats 6:\nProceedings of the 6th International Conference on the Simulation of Adaptive Behaviour, Cambridge, Mass., 2000. The MIT Press.\nA Computational Model of Context Processing\nChristian Balkenius\nJan Morén\nLund University Cognitive Science\nKungshuset, Lundagård\nSE-222 22 Lund, Sweden\nchristian.balkenius@lucs.lu.se\njan.moren@lucs.lu.se\nAbstract\nA computational model of the context processing is\npresented. It is shown in computer simulations how\na stable context representation can be learned from a\ndynamic sequence of attentional shifts between\nvarious stimuli in the environment. The\nmechanism can automatically create the required\ncontext representations, store memories of stimuli\nand bind them to locations. The model also shows\nhow an explicit matching between expected and\nactual stimuli can be used for novelty detection.\nThe novelty detection system is used to decide when\nnew binding nodes should be created and when the\ncontext representation should be shifted from one\ncontext to another. The role of context in\nconditioning and habituation is illustrated in two\nsimple simulations where context learning is\ncombined with conditioning or habituation.\n1. Introduction\nIn learning theory the concept of a context is often used in\nan apparently unproblematic way. Traditionally, contextual\nstimuli are assumed to have the same properties as other\nstimuli with the exception that they somehow code the\nentire experimental situation (Mackintosh, 1983). Another\nway of defining contextual stimuli are as stimuli that are\nnot manipulated by the experimenter (Donahoe and Palmer,\n1994). More generally, many cognitive phenomena are\nregarded as being context sensitive, but very seldom is the\nconcept of a context explained. Given the important role of\ncontext in many theories, it is surprising how little\nattention this concept has received.\nIn descriptions of experiments, the contexts often enters\nin the same way as other stimuli, but what justification do\nwe have for including such a hypothetical stimulus in a\nmodel of learning? While a context certainly exists around\nthe animal, what evidence do we have that supports that it\ncould be regarded as a single stimulus? Consider, for\nexample, a situation where the experimental apparatus is\nseen as the context. It is possible that a single stimulus\nsuch as the smell of the box or the feeling of the floor is\nused to code the situation internally. A more likely\nexplanation, however, is that a contextual representation\nreflects a combination of several stimuli. The context must\nbe seen as a configurational stimulus. A computational\naccount for contextual processing must address how several\nexternal stimuli can be combined into a contextual\nrepresentation.\nWe would like to suggest that a contextual\nrepresentation is constructed from the sequential scanning of\nexternal stimuli and can be equated with a sequence of\nattentional shifts. As attention moves across a scene, a\nnumber of stimuli are perceived in sequence. When each\nstimulus is attended, the animal has access both to the\nlocation of the stimulus and to the stimulus itself. It is\nthen in a position to bind the stimulus to the location and\nstore the result in memory. By combining several stimulus-\nlocation bindings, a contextual representation emerges over\ntime.\nIt is clear that the construction of a contextual\nrepresentation must proceed sequentially since an animal is\nnot able to perceive all stimuli around it simultaneously. If\nfor nothing else, because it has to turn around to see objects\nbehind it. A mechanism is needed that can somehow\nintegrate several sensory impressions. Since the context is\nthe same regardless of the order in which the environment is\nscanned, this mechanism must have the ability to disregard\nthis order. However, if the order in which the scene is\nscanned is controlled by external events, then it is possible\nthat the order has some significance. In this case, it would\nbe useful if the contextual representation contained this\ninformation even if it could be ignored when not needed.\nVery often, the context is the location where the\nexperiment takes place. Given that a location is defined by\nlocal or distal stimuli and that these stimuli must be\nattended in sequence, no difference exists between a\nrepresentation of a context and that of a place. A reasonable\nassumption is that the same learning mechanism could be\nused in both cases.\nAssuming that stimuli in a context are bound to\nlocations, we may consider in what form these locations are\nspecified. There exists ample evidence that multiple\ncoordinate systems are possible for the representation of\nlocations. These coordinates can be egocentric, i. e.\nanchored in the body, for example, in the retina, the head,\nthe mouth or arm. It is also possible that allocentric\n\n\ncoordinates can be used that are anchored in the\nenvironment. A complicating factor is that the context\nitself may contribute to these coordinate representations.\nWhatever coordinate system is used to represent\nlocations, the binding of two stimuli to locations in the\nsame coordinate system will also implicitly represent the\nrelation between these two stimuli.\nIn summary, the concept of a context covers many areas\n(figure 1). For example, a context can be a place indicated\nby a number of landmarks. It can also be a sequence of\nevents or actions. In most cases, of course, a context is\nboth spatial  and temporal since stimuli are usually located\nand must be attended in sequence. In the limiting case, a\ncontext can consist of a single event such as the\npresentation of stimulus some time ago. In this case, the\ncontext essentially acts as a stimulus trace. A more\ninteresting context occurs when the learning experiment\nitself is the context. The stimuli for the context could even\nbe internally generated such as thought and the like.\nA\nB\nC\nt0\nt1\nt2\nA\nC\nB\np\nl0\nl1\nl2\nFigure 1. Two types of contexts. left: A spatial context\ngiven by three stimuli at three locations. right: A\ntemporal context consisting of three events A, B and C.\n2. The Role of Context in Learning\nMany types of learning are context dependent. Here we will\nmainly consider the role of context in habituation and\nconditioning.\nHabituation can be defined as a learning process where\nan animal learns to ignore a stimulus that does not predict\nanything of value to it. Usually this decreasing interest in a\nstimulus is studied through its effect on the orienting\nresponse toward the stimulus. This reaction can be\noperationally defined as any response that (1) is elicited by\nnovel stimuli of any modality, and (2) habituates upon\nrepetition of the stimulus (Gray, 1975). Figure 2a shows\nthe basic habituation experiment. A stimulus S is shown in\ncontext CX1, and generates an orienting reaction OR. After\na number of presentation, the orienting reaction disappears.\nThe orienting reaction reappears when a new novel\nstimulus is presented (Thompson and Spencer, 1966). This\nnew stimulus will make the organism more likely to attend\nto the original stimulus again. This situation is called\ndishabituation (Gray, 1975). This is shown in figure 2b. A\nnovel stimulus N is presented together with the original\nstimulus S and the previously habituated orienting reaction\nreappears. A possible explanation for this phenomenon is\nthat there is a direct link between the detection of novelty\nand the temporary shut down of the habituation system. It\nis important to realize that for dishabituation to occur, the\nhabituation system must be able to react to novelty. This\nimplies that it must learn expectations to which it can\nmatch every stimulus to detect whether it is novel or not.\nA possibly related fact is that a new context will also\ncause the orienting reaction to reappear (Gray, 1975,\nO’Keefe and Nadel, 1978). Since this is similar to the\nsituation above it may be explained by the same\nmechanisms. However, it does not seem entirely\nunreasonable that a novel object in an old contexts should\nbe distinguished from a known object in a novel context.\nAs we will see below, the proposed model of context\nprocessing is able to function in conditioning in precisely\nthis way.\nThe context also influences conditioning. However, in a\nnormal conditioning experiment, the main influence of\nconditioning appears in extinction. While the initial\nlearning of a response is associated mainly with the\nconditioned stimulus, the extinction of the response appears\nto be controlled by the context (Bouton and Nelson, 1998).\nFigure 2d-e shows the structure of a typical acquisition-\nextinction experiment. Like habituation, the effect of\nextinction can also be temporarily removed by the\npresentation of a novel stimulus (Pavlov, 1927). This is\ncalled disinhibition (figure 2f) and allows the conditioned\nstimulus to elicit the conditioned response again. The\nconditioned response is also restored by a context change\n(figure 2g).\na. Habituation\nCX1 + S -> OR\n=>\nCX1 + S -> no OR\nb. Dishabituation\nCX1 + S + N -> OR\nc. Context Change\nCX2 + S -> OR\nd. Conditioning\nCX1 + CS + US\n=>\nCX1 + CS -> CR\ne. Extinction\nCX1 + CS\n=>\nCX1 + CS -> no CR\nf. Disinhibition\nCX1 + CS + N -> CR\ng. Context Change\nCX2 + CS -> CR\nFigure 2. Habituation and Conditioning Paradigms. CX1\nand CX2: contexts, S: stimulus, N: novel stimulus, CS:\nconditioned stimulus, US: unconditioned stimulus, OR:\norienting reaction, CR: conditioned response.\n\n\nIt is interesting to note that these properties are very\nsimilar to the ones described for habituation above. It\nsuggests that both processes can be explained in terms of a\ncontextually controlled learning system that acquires an\ninhibitory influence on either the innate orienting reactions\nor on classically conditioned responses. This requires that\nthe context system can produce an output that can be\nassociated with inhibition of a response.\nThe relation between context and memory is also very\ninteresting. On one hand, memory is often context\ndependent. On the other hand, a temporally extended context\nwill act in much the same way as a memory. Donahoe and\nPalmer (1994) have suggested that working memory could\nbe equated with context in such tasks as matching-to-\nsample where a subject has to remember an object and later\nmatch it to one of several presented stimuli. The\nremembered stimulus has been incorporated in the context\nand can later control responding or attention.\nAnother interesting relation between context and\nattention is that attention can sometimes be seen as\ncontextual discrimination (Donahoe and Palmer, 1994), that\nis, one stimulus is attended and not the others and this is\ncontrolled by the context.\n3. Neural Correlates\nSeveral brain regions have been associated with context\nprocessing and learning. This section briefly reviews the\nrelation between the different learning processes described\nabove and various areas of the mammalian brain.\nThe neural structure most closely connected with\ncontextual processing appears to be the hippocampus. The\nhippocampus must be intact for normal habituation and\nextinction (O’Keefe and Nadel, 1978). Many different roles\nhave been assigned to the hippocampus in different theories\nand models. The perhaps most influential theory of the\nhippocampus is the cognitive map theory of O’Keefe and\nNadel (1978). They suggest that the hippocampus is\nresponsible for the mapping of the environment mainly\nbased on environmental cues. Other suggestions include the\nhippocampus as a memory for sequences or events\n(Solomon 1979, Rawlins 1985, Olton, 1986), working\nmemory (Olton and Samuelson, 1976) or configurational\ncodes (Solomon, 1980). It has been suggested that the\nrepresentation of a location of a stimulus and the stimulus\nitself that are segregated in neocortex are bound together in\nmemory by the hippocampus (Mishkin, Ungerleider and\nMacko, 1983). Another function associated with the\nhippocampal system is the comparison between stored\nregularities and actual stimuli (Gray, 1995). The role of the\nhippocampus in contextual control of memory and learning\nis also well known (Hall and Pearce, 1979).\nThe prefrontal cortex is often described as a structure\nwhose role is to inhibit responses that are inappropriate in a\ncertain context or situation (Shimamura, 1995). It has been\nargued that extinction is controlled by the inhibition from\nthis area (Rolls 1995, Balkenius and Morén, 2000).\nSimilarily, habituation can be seen as the active process of\ninhibiting the orienting reaction to stimuli that are of no\nvalue to the animal (Gray 1975, Balkenius, 2000). The\nprefrontal cortex has also been implicated in this process\n(Fuster 1997). It is likely that the frontal cortex receives\ninformation about the current context from the\nhippocampus. Working together, the hippocampus and\nprefrontal cortex could be responsible for the inhibition that\noccurs in habituation and extinction (Rolls, 1995, Fuster,\n1997).\nSeveral structures in the brain have been implicated in\nconditioning. Especially interesting is the role of the\namygdala in emotional conditioning. This structure is\nknown to be under inhibitory control of the prefrontal\ncortex. It appears that the amygdala is involved in the\ninitial learning of an emotional response while the\nprefrontal cortex is necessary for extinction (Rolls, 1995).\nAnother structure that is inhibited by the prefrontal\ncortex is the basal ganglia (Fuster, 1997). This system in\nthe brain may be involved with the learning of response-\nreward associations (Houk, Davis and Beiser, 1995) and the\ninhibition from frontal cortex could be used to select among\ndifferent motor patterns.\nCONT\nMATCH\nBIND\nMEM\nrecall\nreset\nreset\ncontext\nrepresentation\nStimulus\nLocation\nFigure 3. A Model of Contextual Processing. BIND:\nstimulus location binding. MEM: context dependent\nmemory. CONT: contextual representation. MATCH:\nmatching between actual and recalled stimuli.\n4. A Model of Contextual\nProcessing\nWe have seen above that a model of context processing\nmust include a number of components. The most important\nbeing an explicit context dependent memory that stores\nstimuli or events that can be matched with the current\nperceptual state and a subsystem that constructs a\nrepresentation for the current context. This section presents\na model of context processing in the restricted case of a\nspatial context.\nFigure 3 shows the proposed architecture for the context\nprocessing system. It consists of four main components.\nThese modules perform computations in a neural network\n\n\nlike fashion, but we do not attempt to be biologically\nrealistic. The input to each module as well as its output\nuses vector representations. This allows for distributed\nrepresentations of stimuli and contexts and makes it easy to\ninterface the model with other neural network based models\nof learning. However, in its present state, the model is best\nviewed as a mathematical model rather than as a neural\nnetwork since some of the equations below can not directly\nbe interpreted as local computations in a network.\nThe external input is assumed to consist of two parts, a\nstimulus representation and a place representation. These\ntwo representations correspond to the content of the current\nfocus of attention (Balkenius, 2000).\nThe BIND system contains representations for the\nbinding of a stimulus to a location. Every combination of a\nstimulus and a location can recruit a compact representation\nin this system. Once a binding node is activated it stays on\nuntil it is explicitly reset. A sequence of inputs of\nstimulus-location bindings will activate a set of nodes in\nBIND. Together, these nodes can represent several stimuli\nand their locations although only one enters the system at\neach time. The representation in BIND is context\nindependent in the sense that the representation of a\nstimulus-location binding is independent on the context\nwhere it occurs.\nThe MEM system has complementary properties. Each\nstimulus is stored directly in this subsystem and can be\nexplicitly recalled when cued by a location. This memory is\ncontext dependent in the sense that the location to stimulus\nmapping depends on the current context as received from the\nCONT module. This implies that there can in principle\nexists one specific memory for each combination of a\ncontext and a location. MEM is thus essentially a\nheteroassociator where context and location is input and the\nexpected stimulus at that location is output.\nThe output from the MEM module is thus used by the\nMATCH module as the expected stimulus at the currently\nattended location. This expectation is compared with the\nactual stimulus at this location as represented in the input,\nand if a mismatch occurs the modules BIND and CONT are\nreset.\nThe output from the BIND module is used by the\nCONT to create a representation for the current context. A\ncontext representation is a compact code for the whole set\nof binding nodes active in BIND. The context representation\nevolves in two ways. As long as the context is not reset by\nthe MATCH module, every new BIND node that is\nactivated will be included in the current context. This\nimplies that the context code can be gradually learnt as the\nanimal scans its environment. In the second case, if the\nMATCH module detects a mismatch and resets CONT\nsimultaneously with the recruitment of a new BIND node, a\nnew context will be created.\nTogether, the mechanisms described above will develop\ncontext representations based on sequences of stimulus-\nlocation inputs. It will also detect novel stimuli and reset\nthe context when appropriate.\n5. Formal Description\nThis section presents a formal description of the context\nmodel and its computer implementation. The input to the\ncontext system is assumed to vary over time and is not\ncontrolled in any way by the context system itself. In this\nrespect, the system passively processes whatever signals\nreaches its input structures. The input is divided into two\ntypes: locations and stimuli. Both inputs are coded as\nvectors to allow distributed representations. The current\nstimulus is represented by a vector S and the location is\ncoded by the vector L. For computational purposes, the\nlocation is also represented by the index l of the maximum\nelement of L. When the two vectors L and S are appended\ntogether they are called X.\nThe state of the binding module BIND uses two vectors\nB and W, where B is the activity of all the binding nodes\nand W are the weights on the connections from the input to\nthese nodes. The context module CONT uses the vectors C\nand U to represent the activity of the context nodes and the\nweights on the connections from BIND respectively.\nFinally, the MEM module uses the structure M to store the\nmemory of each context-dependent stimulus place binding.\nThe state of the different modules are calculated in the\nfollowing way: First, the binding module activates a\nbinding node for the current input. Second, the memory of\nthe currently attended location is recalled and compared with\nthe current stimulus input in the matching module. Third,\nif novelty is detected the context and binding nodes are reset\notherwise the new context is calculated. Fourth, the activity\nof the context nodes are updates, and finally, working\nmemory is updated.\n5.1 Binding\nFirst the distance to each bound pattern is calculated,\nDi = d(Wi, X),\n(1)\nwhere d() is the Euclidean distance, W are the weights from\nthe input X to the binding nodes B. The index of the best\nmatching binding is stored in b,\nb = arg max\ni\nDi.\n(2)\nIf Db > 0 5.  then a new binding node is recruited, i. e.,\nWnew = X ,\n(3)\nBnew = 1, and\nIf Db ≤0 5.  then the best binding node is activated,\nBb = 1.\n(4)\nIn the version of the model described here, the order of the\nbindings is disregarded.\n\n\n5.2 Matching\nIn the second step, the match between the input stimulus\nand the expected  stimulus at the current location l in the\ncurrent context c is calculated,\nN =\nMc,l,i −Si\n[\n]\ni\n∑\n+,\n(5)\nwhere [x]+ = max(0, x). The value N expresses the novelty\nof the stimulus and Mc,l is the memory of a stimulus at\nlocation l in the current context.\n5.3. Reset or Update\nIf N > 0 then the stimulus is novel and the context nodes\nCi as well as the binding nodes Bj must be reset. For all i,\nCi  = 0, and,\n(6)\nBi = 1\nif i = new\n0\notherwise\n\n\n\nAs the current input is not reset, Bnew is assumed to resist\nbeing reset and is kept active.\nIf a new binding node was created in equation (3) above,\na new context representation is recruited,\nUnew = B.\n(7)\nHere, Uc is the pattern of the active binding nodes for\ncontext c. Otherwise if N = 0, no novelty was detected and\nthe current context is updated with the new stimulus-\nlocation binding, for all i,\nUc,i = max(Bi,Uc,i )\n(8)\nEquation (8) implies that a context can gradually be\nexpanded to include more and more bindings between\nstimuli and locations.\n5.4 Context Calculation\nIn the next step, unless a reset has occurred, the activity of\nthe context nodes are calculated,\nCi = BUi,\n(9)\nand the current maximum context is found,\nc = arg max\ni\nCi\n(10)\n5.5 Memory\nFinally, the memory of the currently attended stimulus is\nstored,\nMc,l = S.\n(11)\nThis completes one cycle of the context system.\n6. Simulation Studies\nThis section described a number of simulations of the\ncontext model. First the basic properties of the context\nmechanism is demonstrated. The subsequent simulations\nillustrates how a context representation can be used in\nhabituation, classical conditioning, and extinction. The aim\nis here only to show how a context system can interact\nwith other learning processes. The other systems are kept at\na minimal complexity to illustrate the role of context in\nvarious learning paradigms. We do not pretend to present\nfull-fledged models of habituation or conditioning.\n6.1 Context Learning\nFigure 4 illustrates the basic operation of the model. A\ncontextual representation is built from a number of\nstimulus-location bindings. In the simulation, attention\nalternates between a location to the left, called LL, and a\nlocation to the right, called LR. In the first context,\nstimulus A is to the left and stimulus B is to the right. In\nthe next context, both locations contains instances of\nstimulus B. Finally, in the last context, stimulus B is to\nthe left and stimulus A is to the right.\nIn the beginning of the simulation, the simulated\nanimal first attends to stimulus A to the left. This will lead\nto the recruitment of a binding node coding for this\nconjunction. The node B0 is activated to represent this\nbinding. An explicit memory of stimulus A at location LL\nis also stored in the working memory MEM. After a few\ntime-steps the animal attends the location to the right with\nstimulus B which will recruit a new binding node, B1. The\ncombination of binding node B0 and B1 will subsequently\nbe learned by a context node CX0 which will become active\nand represent the current context. In the following time-\nsteps attention shifts back and fourth between stimulus A\nand B which will not lead to any new internal state.\nIn the second phase of the simulation, the context is\nchanged by placing a second stimulus B to the left. When\nthe simulated animal attends to this location, the stored\nmemory of this location will no longer match the actual\nstimulus and novelty will be detected. This novelty will\ncause the context and binding representations to be reset. A\nnew binding node will simultaneously be recruited to\nrepresent the novel stimulus and a new context node will\nbecome active.\nWhen the animal again attends to the stimulus to the\nright, the binding node for this stimulus will be reactivated\nand included in the new context. Since one of the stimuli\nare common to both contexts, the original context will also\nbe partially active. This distributed context-representation\nallows for better generalization between similar contexts\n(Balkenius, 1996).\n\n\nLL\nLR\nSA\nSB\nB0\nB1\nB2\nB3\nCX0\nCX1\nCX2\nNov\nBind\nA\nB\nB\nB\nA\nB\nB\nB\nB\nA\nContext 0\nContext 1\nContext 0\nContext 1\nContext 2\nFigure 4. Simulation of context learning, novelty detection and context change. Ll and LR: location representations. SA and\nSB: two stimuli. B0-B3: binding nodes. CX0-CX2: context nodes. Nov: novelty detection. Bind: recruitment of binding node.\nIn the next phase, the context is changed back again.\nThis will again trigger a mismatch event and the binding\nand context nodes will be reset. This time, however, no\nnew binding nodes are recruited since there already exist\nrepresentations for both A to the left and B to the right.\nInstead, the original context node will become active\nagain. As above, the previous context will also receive\nsome activation since it has one binding in common with\nthe current context.\nThe following phase shows the effect of a second\ncontext change without any learning. Finally, a new\ncontext is encountered with B to the left and A to the right.\nIn this context, one new binding node will be recruited for\nrepresenting A to the right since it has not been encountered\nbefore. The distributed context representation includes some\nactivation of context 1 but none of context 0 since it has no\nbindings in common with the current context.\nIt should be noted that context learning is an entirely\npassive process, it does not depend on what the animal\ndoes. Here it was assumed that the attention alternated\nbetween two locations, but does not matter whether these\nattentional shifts are controlled by external stimulus events\nor internally by an active scanning. Nor is it necessary that\nattention shifts in the orderly way used in this simulation.\nIn fact, the order in which attention shifts is not used by the\nsystem. The system also ignores the time between each\nattentional fixation. This is a simplification since ideally\nthe context system should represent this information.\n6.2 Context in Habituation\nWe now turn to how the context system can support\ncontext dependent habituation. It is not the aim of this\nsection to develop a complete model of habituation. The\nmodel below is intended as an elementary example of how\nthe context system can interact with other learning\nprocesses.\nThe magnitude of the orienting reaction to a novel\nstimulus is assumed to be equal the intensity of the\nstimulus reduced with the total inhibition from the context\nsystem,\nOR = N −\nwiCXi\ni\n∑\n,\n(12)\n\n\nLL\nLR\nSA\nSB\nOR\nN\nInhib\nCX0\nCX1\nNov\nBind\nHabituation\nDishabituation\nFigure 5. Habituation and dishabituation by context change.\nwhere, OR is the orienting reaction, N is the intensity\nof the novel stimulus. CXi are the activity of the context\nnodes as above and wi are the inhibition from each context\nnode to the orienting reaction. Habituation is modelled in\nthe following way,\n∆wi = α CXi OR,\n(13)\nwhere, α is the habituation rate.\nFigure 5 shows a simulation of habituation in one\ncontext and a subsequent context change. The simulation\ncan be seen as two distinct tasks where one is the context\nlearning task as described above and the second is a\nhabituation task. The two tasks interact in several ways\nhowever.\nIn the habituation phase, the simulated animal alternates\nbetween looking at the context stimuli and the novel\nstimulus N. Since the model does not know whether a\nstimiulus is part of the context or not, all stimuli will\ninfluence the processing in the context system. As can be\nseen in figure 5, the context system will recruit binding\nnodes for the novel stimulus N as well as for the context\nstimuli SA and SB.\nAn orienting reaction occurs when the novel stimulus is\nattended. This reaction will gradually vanish as the\ninhibition from the context representation grows in\nstrength. In the second phase, the context changes and as a\nresult, the orienting reaction to stimulus N is reinstated. In\nthis phase, however, habituation is quicker as a result of\ngeneralization from the previous context.\n6.3 Context in Classical Conditioning and\nExtinction\nIn the final simulation, we show how the context system\ncan be used to control extinction in classical conditioning.\nA basic equation describing conditioning is the following,\nCR = CSu −\nwiCXi\ni\n∑\n,\n(14)\nwhere CR is the conditioned response, CS is the intensity\nof the conditioned stimulus and u is the excitatory\nconnection from CS to CR. The sum is identical to the\ncase of habituation except that the inhibition now acts on\nthe conditioned response and not on the orienting reaction.\nAdmittedly, this is not much of a conditioning model. As\nabove, the conditioning equation described here is not\nintended as a full-fledged model. Instead, the aim is to show\nthe operation of the context system in a classical\nconditioning paradigm.\nAcquisition of the conditioned response is modelled by\nthe following equation,\n∆u = β US −CR\n[\n],\n(15)\nwhere β is the learning rate. Extinction is modelled in a\nsimilar way as habituation above,\n∆wi = α CXi CR −US\n[\n]\n(16)\n\n\nLL\nLR\nSA\nSB\nUS\nCR\nCS\nExcit\nInhib\nCX0\nCX1\nNov\nBind\nAcquistion\nExtinction\nDisinhibition\nFigure 6. Acquisition, extinction and disinhibition in classical conditioning embedded into a context learning\nexperiment.\nFigure 6 shows a simulation of context in extinction. In\nthe acquisition phase, the CS is paired with the US and this\ncauses the excitatory connection between the CS and the\nCR to grow. In the second phase, the US is omitted which\nwill cause the inhibition from the context to increase in\nstrength according to equation (16). After a few\npresentations, the CR will almost disappear. Finally, the\ncontext changes which resets the context. As a consequence,\nthe inhibition will be removed making the animal react to\nthe CS again. As for habituation, the CR is now\nextinguished again, but with a faster rate than before.\n7. Discussion\nThe model presented is limited in several respects. The\nmost severe shortcoming is that there exist cases when it\nwill not be able to resolve which context it is currently in.\nFor example, assume that the model first encounters a\ncontext with stimulus A both to the right and to the left,\nthen a second with B at both locations, and finally a context\nwith A to the left and B to the right. When the final context\nis presented it will first look at A then B and so on. Neither\nA nor B will trigger the binding system since both bindings\nhave been seen before. When the system looks at A it will\nstart to activate the first context, but when attention is\nshifted to B, it will be reset again. The same happens when\nthe system starts to attend B and then moves on to A. In\neffect, the context representation will oscillate between the\nfirst and second context, and a new one will not be created.\nIn a sense, this is correct since the system does not\nknow that the context does not change with each attentional\nshift. On the other hand, it is clearly incorrect since we\nknow that the context is the same. To resolve this situation\nadditional assumption have to be made. For instance, a\nmechanism similar to that used in the ART model could be\nused, where a node that has been reset cannot be activated\nfor some time (Carpenter and Grossberg, 1986). This\nhowever requires an ad hoc time constant that we did not\nwant to include in the model.\nAnother possibility is that a series of novelty detections\nare somehow accumulated in such a way that too much\nnovelty will trigger the creation of a new context. Again,\nthis will require an ad hoc constant defining what too much\nnovelty is.\nAn intriguing possibility would be to code the new\ncontext in a recursive way using the previous contexts. In\nthis case, the novel context would be represented as a\ncontext that resembles both the first and the second context.\nFor more complicated situations, it appears that smaller\ncontexts should be embedded into larger contexts. It would\nbe useful if a context could be represented at a number of\nlevels simultaneously (Balkenius, 1996). For example, a\ncontext could represent the spatial location of an animal,\nthe experimental situation, and the fact that a stimulus was\n\n\npresented two seconds ago. These are all different\ncomponents of a hierarchical context.\nIn the future, we want to develop an expectancy\nmatching mechanism that can handle such hierarchical\ncontext. When an expectation is not met, only the part of\nthe context that produced the expectation need to be reset.\nHowever, it is far from clear how such a mechanism should\nwork and the experimental data on animals are sparse.\nAnother limitation of the model is that sequential\ninformation is not represented. In the future, we want to\nextend the model with a temporal coding of bindings. This\nwould allow the output of the context system to be used for\nsequence recognition and sequential discrimination.\nA final direction for future research is to incorporate\nmore physiological data into the model. It was hinted above\nthat the task solved by the model is similar to that ascribed\nto the hippocampal system in the brain, but we did not try\nto model this structure directly. It is interesting to note that\nthe context system presented above, especially with the\nextensions discussed here, could account for many of the\nseemingly disparate properties of the hippocampus. If the\ninputs are visual landmarks, then the context nodes will\nhave properties similar to place cells. Since they store\npreviously attended stimuli, they can also act as a working\nmemory. The grouping of several stimuli into a context\nwill make the context act as a configurational code. Finally,\nif a sequential code is used rather than the one described\nabove, the model could conceivably describe some the\nsequence effects and stimulus-trace properties sometimes\nassociated with the hippocampus.\n8. Conclusion\nA computational model of the context processing has been\npresented. It was shown in compute simulations how a\nstable context representation could be learned from a\ndynamic sequence of attentional shifts between various\nstimuli in the environment. The mechanism can\nautomatically create the required context representations,\nstore memories of stimuli and bind them to locations. The\nmodel also shows how an explicit matching between\nexpected and actual stimuli can be used for novelty\ndetection. The novelty detection system is used to decide\nwhen new binding nodes should be created and when the\ncontext representation should be shifted from one context to\nanother.\nThe role of context is conditioning and habituation was\nshown in two simple simulations where context learning\nwas combined with conditioning or habituation. It has also\nbeen suggested that the function of the context processing\nsystem is similar to that carried out by the hippocampus.\nReferences\nBalkenius, C. (1996). Generalization in instrumental\nlearning. In Maes, P., Mataric, M., Meyer, J.-A.,\nPollack, J., Wilson, S. W. (Eds.) From Animals to\nAnimats 4: Proceedings of the Fourth International\nConference on Simulation of Adaptive Behavior .\nCambridge, MA: The MIT Press/Bradford Books.\nBalkenius, C. (2000). Attention, Habituation and\nConditioning: Toward A Computational Model,\nsubmitted to Cognitive Science Quarterly.\nBalkenius, C., Morén, J. (1999) Dynamics of a Classical\nConditioning Model. Autonomous Robots, 7 (1).\nBalkenius, C., Morén, J. (2000) Emotional Learning: A\nComputational Model of the Amygdala, Cybernetics and\nSystems, in press.\nBouton, M. E. and Nelson, J. B. (1998). Mechanisms of\nfeature-positive and feature negative discrimination\nlearning in an appetitive conditioning paradigm. In N.\nA. Schmajuk and P. C. Holland (Eds.), Occasion\nSetting: Associative Learning and Cognition in\nAnimals (pp. 69-112). Washington, DC: American\nPsychological Association.\nCarpenter, G. A., Grossberg, S. (1986). Neural dynamics\nof category learning and recognition: attention, memory\nconsolidation and amnesia. In Grossberg, S. (Ed.) The\nadaptive brain (pp. 238-308). Amsterdam: North-\nHolland.\nDonahoe, J. W., Palmer, D. C. (1994). Learning and\ncomplex behavior. Boston, MA: Allyn and Bacon.\nFuster, J. M. (1997). The prefrontal cortex: anatomy,\nphysiology, and neuropsychology of the frontal lobe.\nPhiladelphia: Lippincott-Raven.\nGray, J. A. (1975). Elements of a two-process theory of\nlearning. London: Academic Press.\nGray, J. A. (1995). A Model of the Limbic System and\nBasal Ganglia: Application to anxiety and\nschizophrenia. In Gazzaniga, M. S. (Ed.) The cognitive\nneurosciences (pp. 1165-1176). Cambridge, MA: MIT\nPress.\nHall, G., Pearce, J. M. (1979). Latent inhibition of a CS\nduring CS-US pairings. J. exp. Psychol. anim. Behav.\nProcess., 5,  32–42.\nHouk, J. C., Davis, J. L., Beiser, D. G. (1995). Models of\ninformation processing in the basal ganglia. Cambridge,\nMA: MIT Press.\nMackintosh, N. J. (1983). Conditioning and associative\nlearning. Oxford: Oxford University Press.\nMishkin, M., Ungerleider, L. G., Macko, K. A. (1983).\nObject vision and spatial vision: two cortical pathways.\n6,  414-417.\n\n\nO’Keefe, J., Nadel, L. (1978). The hippocampus as a\ncognitive map. Oxford: Clarenon Press.\nOlton, D. S. (1986). Hippocampal function and memory\nfor temporal context. In R. L. Isaacson and K. H.\nPribram (Eds.), The hippocampus, Vol 4 (pp. 281-298).\nNew York: Plenum Press.\nOlton, D. S., Samuelson, R. J. (1976). Remembrance of\nplaces past: spatial memory in rats. 2, 97-116.\nPavlov, I. P. (1927). Conditioned reflexes. Oxford: Oxford\nUniversity Press.\nRawlins, J. N. P. (1985). Associations across time: The\nhippocampus as a temporary memory store. The\nBehavioral and Brain Sciences, 8,  479-96.\nRolls, E. T. (1995). A theory of emotion and\nconsciousness, and its application to understanding the\nneural basis of emotion. In Gazzaniga, M. S. (Ed.) The\ncognitive neurosciences (pp. 1091-1106). Cambridge,\nMA: MIT Press.\nShimamura, A. P. (1995). Memory and frontal lobe\nfunction. In Gazzaniga, M. S. (Ed.) The cognitive\nneurosciences (pp. 803-813). Cambridge, MA: MIT\nPress.\nSolomon, P. R. (1979). Temporal versus spatial\ninformation processing theories of hippocampal\nfunction. Psychol Bull, 86, 6, 1272-9.\nSolomon, R. L. (1980). The opponent process theory of\nacquired motivation. American Psychologist, 35,  691–\n712.\nThompson, R. F. and Spencer, W. A. (1966). Habituation:\nA model phenomenon for the study of neuronal\nsubstrates of behavior. Psychological Review, 73, 16–\n43.\n\n\n",
      "concepts": [
        "it consists of four main components",
        "in most cases",
        "the external input is assumed to consist of two parts",
        "neural basis",
        "model of learning",
        "this section briefly reviews the\nrelation between the different learning processes described\nabove and various areas of the mammalian brain",
        "memory for sequences or events",
        "dynamic sequence of attentional shifts between various\nstimuli in the environment",
        "the subsequent simulations\nillustrates how",
        "are appended\ntogether they are called",
        "in\neffect",
        "after",
        "number of stimuli are perceived in sequence",
        "contextual\nrepresentation is constructed from the sequential scanning of\nexternal stimuli and can be equated with",
        "as we will see below",
        "extinction",
        "it has been suggested that the\nrepresentation of",
        "ll and lr",
        "which will recruit",
        "the novelty detection system is used to decide when\nnew binding nodes should be created and when the\ncontext representation should be shifted from one\ncontext to another",
        "computational\naccount for contextual processing must address how several\nexternal stimuli can be combined into",
        "nov\nbind\nacquistion\nextinction\ndisinhibition\nfigure",
        "for example",
        "new bind node",
        "habituation and dishabituation by context change",
        "in the version of the model described here",
        "more\ninteresting context occurs when the learning experiment\nitself is the context",
        "american psychologist",
        "situation",
        "new one will not be created",
        "each\nstimulus is stored directly in this subsystem and can be\nexplicitly recalled when cued by",
        "subsystem that constructs",
        "oxford university press",
        "behav",
        "situation where the experimental apparatus is\nseen as the context",
        "set of nodes in\nbind",
        "model",
        "schmajuk and",
        "context\nwill make the context act as",
        "bradford books",
        "the\nmechanism can automatically create the required\ncontext representations",
        "in the\ncurrent context",
        "the magnitude of the orienting reaction to",
        "every new bind node that is\nactivated will be included in the current context",
        "mismatch event and the binding\nand context nodes will be reset",
        "cybernetics and\nsystems",
        "this expectation is compared with the\nactual stimulus at this location as represented in the input",
        "known object in",
        "conditioning and associative\nlearning",
        "stimulus representation and",
        "extinction and disinhibition in classical conditioning embedded into",
        "number of components",
        "raven",
        "many cognitive phenomena are\nregarded as being context sensitive",
        "association",
        "it is not the aim of this\nsection to develop",
        "is activated to represent this\nbinding",
        "equation",
        "information",
        "working\nmemory",
        "if the order in which the scene is\nscanned is controlled by external events",
        "fixation",
        "however",
        "the\nmodel also shows how an explicit matching between\nexpected and actual stimuli can be used for novelty\ndetection",
        "context is\nboth spatial  and temporal since stimuli are usually located\nand must be attended in sequence",
        "stimulus to",
        "mackintosh",
        "the\nadaptive brain",
        "when each\nstimulus is attended",
        "it is interesting to note that\nthe context system presented above",
        "there exists ample evidence that multiple\ncoordinate systems are possible for the representation of\nlocations",
        "the context system will recruit binding\nnodes for the novel stimulus",
        "two types of contexts",
        "version",
        "when the animal again attends to the stimulus to the\nright",
        "neural dynamics\nof category learning and recognition",
        "assuming that stimuli in",
        "direct link between the detection of novelty\nand the temporary shut down of the habituation system",
        "pribram",
        "context can consist of",
        "similarily",
        "the cs is paired with the us and this\ncauses the excitatory connection between the cs and the\ncr to grow",
        "one stimulus is attended and not the others and this is\ncontrolled by the context",
        "operation",
        "no or",
        "first the basic properties of the context\nmechanism is demonstrated",
        "mechanism similar to that used in the art model could be\nused",
        "fledged models of habituation or conditioning",
        "olton",
        "especially with the\nextensions discussed here",
        "lund university cognitive science\nkungshuset",
        "expresses the novelty\nof the stimulus and mc",
        "relation",
        "direction",
        "and the location is\ncoded by the vector",
        "we show how the context system\ncan be used to control extinction in classical conditioning",
        "whatever coordinate system is used to represent\nlocations",
        "model of contextual\nprocessing\nwe have seen above that",
        "they suggest that the hippocampus is\nresponsible for the mapping of the environment mainly\nbased on environmental cues",
        "location inputs"
      ],
      "document_type": "academic_paper",
      "content_hash": "d579e5e553acec28",
      "size": 56887,
      "ingestion_time": "2025-06-06T07:33:42.402932",
      "research_field": "mathematics",
      "technical_level": "graduate",
      "citation_count": 13
    },
    {
      "file_path": "C:\\Users\\jason\\Desktop\\tori\\kha\\data\\memory\\temporal spectral\\2502.11878v1.pdf",
      "file_type": "pdf_document",
      "title": "arXiv:2502.11878v1  [stat.AP]  17 Feb 2025",
      "author": "Unknown",
      "subject": "",
      "pages": 4,
      "text_content": "arXiv:2502.11878v1  [stat.AP]  17 Feb 2025\nForecasting Italian daily electricity generation\ndisaggregated by geographical zones and energy\nsources using coherent forecast combination\nDaniele Girolimetto and Tommaso Di Fonzo\nAbstract A novel approach is applied for improving forecast accuracy and achiev-\ning coherence in forecasting the Italian daily energy generation time series. In hier-\narchical frameworks such as national energy generation disaggregated by geograph-\nical zones and energy sources, independently generated base forecasts often result\nin inconsistencies across the constraints. We deal with this issue through a coher-\nent balanced multi-task forecast combination approach, which combines unbiased\nforecasts from multiple experts while ensuring coherence. Applied to the daily Ital-\nian electricity generation data, our method shows superior accuracy compared to\nsingle-task base and combined forecasts, and a state-of-the-art single-expert recon-\nciliation technique, demonstrating to be an effective approach to forecasting linearly\nconstrained multiple time series.\nKey words: Italian electricity generation, Linearly constrained multiple time se-\nries, Forecast combination, Forecast reconciliation\n1 Introduction\nSelecting an effective forecasting method among many options presents a complex\nand resource-intensive challenge, as inherent uncertainties often prevent any method\nfrom achieving the best accuracy across all scenarios. This complexity becomes\neven more signiﬁcant when dealing with multiple interrelated variables that must\nsatisfy linear constraints, as in National Accounts [10] and in hierarchical time se-\nries like electricity demand segmented by regions and sources [1]. Such settings\nrequire appropriate forecasting approaches ensuring coherence across interrelated\nvariables. Traditional univariate forecast combination methods [2, 4] focus on indi-\nvidual time series. In contrast, global forecast combination methods leverage shared\ninformation across multiple series to improve accuracy [13]. Similarly, forecast rec-\nDepartment of Statistical Sciences (University of Padova) Via Cesare Battisti, 241, 35121 Padova,\ne-mail: daniele.girolimetto@unipd.it\n1\n\n\n2\nDaniele Girolimetto and Tommaso Di Fonzo\nonciliation approaches search for coherence by adjusting forecasts to satisfy some a\npriori constraints [14], with signiﬁcant empirical advantages in various ﬁelds [1].\nIn this paper, we exploit a recently developed linear framework [9], that joins\nsingle-variable forecast combination [2] and the least squares reconciliation proce-\ndure for a linearly constrained multiple time series [12] to produce optimal (in the\nleast squares sense) coherent forecasts. The remainder of the paper is structured as\nfollows. Sect. 2 succinctly presents the optimal coherent forecast combination pro-\ncedure for incoherent base forecasts of a linearly constrained multiple time series,\nof which hierarchical time series are a special case. Sect. 3 presents the main ﬁnd-\nings of a forecasting experiment on 76 daily time series of the Italian electricity\ngeneration disaggregated by geographical zones and energy sources.\n2 Optimal coherent multi-task forecast combination\nLet y = [y1 ... yi ... yn]⊤∈Rn represent the target forecast vector for a linearly\nconstrained time series Yt, such that CYt = 0(nu×1), where C ∈Rnu×n is a prede-\nﬁned matrix describing the (homogeneous) linear constraints valid for the single\ncomponent series of Yt, i.e., Cy = 0(nu×1). We assume that p ≥2 experts indepen-\ndently produced a base forecast for every one of the n individual components of y,\nyielding m = np base forecasts overall1.\nDenote byj\ni the unbiased base forecast of the i-th individual variable of y pro-\nduced by the j-th expert, and let ε j\ni = byj\ni −yi the zero-mean base forecast error.\nLet byj ∈Rnj, j = 1,..., p, be the vector of base forecasts produced by the j-\nth expert, with error given by ε j = byj −y, by = vec\n\u0002\nby1 ... by j ... byp\u0003\n∈Rm, and\nε = vec\n\u0002\nε1 ... ε j ... ε p\u0003\n∈Rm. The linear relationship linking all the available base\nforecasts by and the unknown target forecast vector y can be expressed through the\nmultiple regression model by = Ky + ε, where K = (1p ⊗In), E(ε) = 0(np×1) and\nW = E(εε⊤) ∈Rm×m is a p.d. matrix.\nWe are looking for a coherent forecast vector eyc, i.e., Ceyc = 0(nu×1), which ex-\nploits all the available base forecasts and improves their accuracy. Forecast combi-\nnation and reconciliation can be simultaneously dealt with through an optimization-\nbased technique that combines multiple individual experts’ base forecasts byj:\neyc = argmin\ny\n\u0000by−Ky\n\u0001⊤W−1\u0000by−Ky\n\u0001\ns.t.\nCy = 0(nu×1).\n(1)\nBy extending the well known least squares adjustment procedure for a single\nvector of preliminary incoherent estimates (i.e., base forecasts) [12] to the case\nof p ≥2 vectors, the coherent combined forecast vector eyc can be expressed\nas eyc = MΩ⊤by, where M =\nh\nIn −WcC⊤\u0000CWcC⊤\u0001−1C\ni\n, Ω= W−1KWc, and\nWc =\n\u0000K⊤W−1K\n\u0001−1 (see [9] for details). It is worth noting that this result is ob-\ntained under the commonly assumed hypotheses that the base forecasts are unbiased,\nand that the joint dispersion matrix of the p forecast experts is known.\n1 A complete discussion of the optimal coherent combination approach, including the extension to\nthe general case where the experts may produce base forecasts only for a subset of the totality of\nvariables, can be found in [9].\n\n\nForecasting Italian daily electricity generation using coherent forecast combination\n3\n3 Forecasting Italian daily electricity generation\nIn this section, we consider a forecasting experiment for 76 Italian daily electric-\nity generation time series from 01/01/2023 to 31/12/2023, obtained from the\nENTSO-E Transparency Platform. The Italian electricity market is a grouped time\nseries with two hierarchies, both sharing the top and bottom levels. In the ﬁrst hi-\nerarchy, generation is aggregated into 7 different sources, classiﬁed into two cate-\ngories: renewable (Biomass, Hydro, Solar, Wind, Geothermal) and non-renewable\n(Fossil, Waste). The second hierarchy organizes the data geographically in 7 zones\naccording to the Terna electricity market map in compliance with the EU CACM\nregulation.\nIn order to assess the forecast accuracy of the various approaches, we perform a\nrolling forecast experiment with an expanding window (01/01/2023 −21/5/2023\nﬁrst training set). One- to seven-step-ahead forecasts were generated leading to Q1 =\n225 one-, ..., and Q7 = 119 seven-step-ahead daily forecasts for evaluation. Each\nseries was independently modeled using three different approaches: stlf [5], arima\n[3], tbats [7]. Moreover, we consider the following eleven forecasting procedures:\n1. Single best model and its reconciled counterpart: two approaches. For the best\nbase forecast expert (tbats), the reconciled forecasts are obtained through the\nshrunk MinT approach by [14] (tbatsshr).\n2. Single-task combination: three approaches. Equal weights (ew) and the optimal\nweighting schemes proposed by [2] and [11], called respectively owvar and owcov.\n3. Coherent combination: six approaches. A sequential reconciliation-then-equal-\nweight-combination(src), three sequential combination-then-reconciliation(screw,\nscrvar, scrcov), and ﬁnally two optimal coherent combination approach, occwlsv\nand occbe, with diagonal and block-diagonal shrunk in-sample covariance ma-\ntrix, respectively (see [9] for details).\nThe forecast accuracy is evaluated in Table 1 using the Average Relative Mean Ab-\nsolute (AR-MAE) and Squared (AR-MSE) Error [8, 6] computed as, respectively,\nAR-MAEapp\nh\n=\n\u0012\n76\n∏\ni=1\nMAEapp\nh,i\nMAEew\nh,i\n\u0013 1\n76\nand AR-MSEapp\nh\n=\n\u0012\n76\n∏\ni=1\nMSEapp\nh,i\nMSEew\nh,i\n\u0013 1\n76\n, with MSEapp\nh,i =\n1\nQh\nQh\n∑\nq=1\n\u0000yi,h,q −yapp\ni,h,q\n\u00012 and MAEapp\nh,i\n=\n1\nQh\nQh\n∑\nq=1\n\f\fyi,h,q −yapp\ni,h,q\n\f\f, where h = 1,...,7 is the\nforecast horizon, i = 1,...,76 denotes the variable, Qh is the test set dimension, app\nis the approach used, yi,h,q is the observed value and yapp\ni,h,q is the forecast value us-\ning the app approach (coherent or incoherent). The equal weights (ew) single-task\ncombination is used as benchmark.\nLooking at Table 1, single-task combination and single-expert reconciliation\nstrategies consistently improve forecasting accuracy compared to the three base\nmodels. However, tbatsshr tends to perform less effectively when compared to the\nmore advanced single-task combination approaches. In contrast, coherent combi-\nnation approaches consistently outperform all the other forecasting strategies. Fur-\nthermore, these coherently combined forecasts satisfy all the necessary constraints,\nwhich is a critical advantage in this, as in most applied contexts.\nAcknowledgements The authors acknowledge ﬁnancial support from project PRIN 2022: PRICE\n– “A New Paradigm for High-Frequency Finance”– 2022C799SX.\n\n\n4\nDaniele Girolimetto and Tommaso Di Fonzo\nTable 1 AR-MAE and AR-MSE with equal weights combination approach (ew) as benchmark.\nBold entries identify the best performing approaches, italic entries identify the second best.\nAR-MAE\nAR-MSE\nApproach\n1\n2\n3\n5\n7\n1:7\n1\n2\n3\n5\n7\n1:7\nBase (incoherent forecasts) and single model reconciliation\ntbats\n1.038 1.042 1.041 1.034 1.031 1.036 1.072 1.078 1.073 1.056 1.041 1.059\ntbatsshr\n1.008 1.009 1.018 1.016 1.017 1.014 1.004 1.005 1.029 1.022 1.018 1.018\nCombination (incoherent forecasts)\new\n1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000\nowvar\n0.988 0.987 0.991 0.994 0.996 0.992 0.978 0.978 0.986 0.991 0.997 0.989\nowcov\n1.021 1.015 1.021 1.017 1.006 1.015 1.022 1.020 1.036 1.039 1.039 1.035\nCoherent combination\nsrc\n0.980 0.982 0.987 0.992 0.995 0.989 0.959 0.962 0.977 0.985 0.994 0.981\nscrew\n0.983 0.985 0.992 0.996 0.998 0.993 0.965 0.969 0.986 0.991 0.997 0.987\nscrvar\n0.974 0.978 0.986 0.993 0.995 0.988 0.950 0.958 0.977 0.989 0.998 0.982\nscrcov\n1.008 1.009 1.014 1.015 1.005 1.011 0.994 1.003 1.024 1.035 1.037 1.026\noccwls\n0.984 0.982 0.986 0.991 0.992 0.988 0.970 0.970 0.978 0.985 0.992 0.983\noccbe\n0.970 0.973 0.979 0.988 0.992 0.983 0.940 0.946 0.966 0.981 0.993 0.974\nReferences\n1. G. Athanasopoulos, R. J. Hyndman, N. Kourentzes, and A. Panagiotelis. Forecast reconcilia-\ntion: A review. International Journal of Forecasting, 40(2):430–456, 2024.\n2. J. Bates and C. W. Granger. The combination of forecasts. Operations Research Quarterly,\n20(4):451–468, 1969.\n3. G. E. P. Box and G. M. Jenkins. Time series analysis: Forecasting and control. San Francisco:\nHolden-Day, 1976.\n4. R. T. Clemen. Combining forecasts: A review and annotated bibliography. International\nJournal of Forecasting, 5(4):559–583, 1989.\n5. R. Cleveland. STL : A seasonal-trend decomposition procedure based on loess. Journal of\nOfﬁcial Statistics, 6(1):3–73, 1990.\n6. A. Davydenko and R. Fildes. Measuring forecasting accuracy: The case of judgmental adjust-\nments to sku-level demand forecasts. International Journal of Forecasting, 29(3):510–522,\n2013.\n7. A. M. De Livera, R. J. Hyndman, and R. D. Snyder. Forecasting time series with complex sea-\nsonal patterns using exponential smoothing. Journal of the American Statistical Association,\n106(496):1513–1527, 2011.\n8. P. J. Fleming and J. J. Wallace. How not to lie with statistics: The correct way to summarize\nbenchmark results. Communications of the ACM, 29(3):218–221, 1986.\n9. D. Girolimetto and T. Di Fonzo. Coherent forecast combination for linearly constrained mul-\ntiple time series. arXiv, 2024.\n10. D. Girolimetto and T. Di Fonzo. Point and probabilistic forecast reconciliation for general\nlinearly constrained multiple time series. Statistical Methods & Applications, 33(2):581–607,\n2024.\n11. P. Newbold and C. W. Granger. Experience with forecasting univariate time series and the\ncombination of forecasts. Journal of the Royal Statistical Society, A, 137(2):131–146, 1974.\n12. R. Stone, D. G. Champernowne, and J. E. Meade. The precision of national income estimates.\nThe Review of Economic Studies, 9(2):111–125, 1942.\n13. R. Thompson, Y. Qian, and A. L. Vasnev. Flexible global forecast combinations. Omega,\n126:103073, 2024.\n14. S. L. Wickramasuriya, G. Athanasopoulos, and R. J. Hyndman. Optimal forecast reconcil-\niation for hierarchical and grouped time series through trace minimization. Journal of the\nAmerican Statistical Association, 114(526):804–819, 2019.\n\n\n",
      "concepts": [
        "and in hierarchical time se",
        "tiple time series",
        "stlf",
        "we consider the following eleven forecasting procedures",
        "we exploit",
        "expert reconciliation\nstrategies consistently improve forecasting accuracy compared to the three base\nmodels",
        "snyder",
        "each\nseries was independently modeled using three different approaches",
        "zones\naccording to the terna electricity market map in compliance with the eu cacm\nregulation",
        "ing coherence in forecasting the italian daily energy generation time series",
        "matrix",
        "girolimetto and",
        "we deal with this issue through",
        "statistical methods",
        "is the\nforecast horizon",
        "of which hierarchical time series are",
        "mse with equal weights combination approach",
        "forecasting and control",
        "linearly constrained multiple time se",
        "coherent or incoherent",
        "stone",
        "model",
        "by extending the well known least squares adjustment procedure for",
        "app\nis the approach used",
        "measuring forecasting accuracy",
        "empirical advantages in various",
        "association",
        "information",
        "however",
        "into two cate",
        "kourentzes",
        "forecasting experiment on",
        "mseew",
        "denote byj",
        "mae and ar",
        "is the forecast value us",
        "journal of",
        "international journal of forecasting",
        "onciliation",
        "price",
        "hydro",
        "three sequential combination",
        "expert recon",
        "introduction\nselecting an effective forecasting method among many options presents",
        "box and",
        "combination",
        "evaluation",
        "adjustment",
        "our method shows superior accuracy compared to\nsingle",
        "stat",
        "experience with forecasting univariate time series and the\ncombination of forecasts",
        "base forecasts",
        "level demand forecasts",
        "introduction",
        "the reconciled forecasts are obtained through the\nshrunk mint approach by",
        "rn represent the target forecast vector for",
        "with diagonal and block",
        "as in national accounts",
        "forecasting italian daily electricity generation\nin this section",
        "vasnev",
        "coher",
        "newbold and",
        "totality",
        "operations research quarterly",
        "complexity",
        "focus on indi",
        "this complexity becomes\neven more",
        "coherent forecasts",
        "fildes",
        "matrix describing the",
        "wickramasuriya",
        "flexible global forecast combinations",
        "forecast rec",
        "cedure for incoherent base forecasts of",
        "weight",
        "single best model and its reconciled counterpart",
        "seven",
        "for details",
        "rolling forecast experiment with an expanding window",
        "italian daily electric",
        "optimal coherent multi",
        "yi the zero",
        "variable forecast combination",
        "tained under the commonly assumed hypotheses that the base forecasts are unbiased",
        "wind",
        "in the\nleast squares sense",
        "qh is the test set dimension",
        "ries like electricity demand segmented by regions and sources",
        "dently produced",
        "meade",
        "diagonal shrunk in",
        "global forecast combination methods leverage shared\ninformation across multiple series to improve accuracy",
        "occbe",
        "complex\nand resource",
        "critical advantage in this",
        "we perform",
        "communications of the acm",
        "review and annotated bibliography",
        "reconciliation",
        "base forecast for every one of the"
      ],
      "document_type": "academic_paper",
      "content_hash": "f99465c8c21f6563",
      "size": 66730,
      "ingestion_time": "2025-06-06T07:33:42.450602",
      "research_field": "artificial_intelligence",
      "technical_level": "graduate",
      "citation_count": 18
    },
    {
      "file_path": "C:\\Users\\jason\\Desktop\\tori\\kha\\data\\USB Drive\\docs\\BANKSY\\alan-core\\banksy_files\\Hierarchical and Multi-Dimensional Synchronization\\Fourier oscillator\\1106.2474v1.pdf",
      "file_type": "pdf_document",
      "title": "1106.2474v1",
      "author": "Unknown",
      "subject": "",
      "pages": 3,
      "text_content": "arXiv:1106.2474v1  [stat.ML]  13 Jun 2011\n1\nSource Separation and Clustering\nof Phase-Locked Subspaces: Derivations and Proofs\nMiguel Almeida, Jan-Hendrik Schleimer, Jos´e Bioucas-Dias, Ricardo Vig´ario\nAbstract—Due to space limitations, our submission “Source\nSeparation and Clustering of Phase-Locked Subspaces”, accepted\nfor publication on the IEEE Transactions on Neural Networks\nin 2011, presented some results without proof. Those proofs are\nprovided in this paper.\nIndex Terms—phase-locking, synchrony, source separation,\nclustering, subspaces\nAPPENDIX A\nGRADIENT OF |̺|2 IN RPA\nIn this section we derive that the gradient of |̺|2 is given by\nEq. 6 of [1], where |̺| is deﬁned as in Eq. 5 of [1]. Recall that\n∆φ(t) = φ(t)−ψ(t), where φ(t) is the phase of the estimated\nsource y(t) = wT x(t) and ψ(t) is the phase of the reference\nu(t). Further, deﬁne ̺ ≡|̺|eiΦ.\nWe begin by noting that |̺|2 = (|̺| cos(Φ))2+(|̺| sin(Φ))2,\nso that\n∇|̺|2 = ∇(|̺| cos(Φ))2 + ∇(|̺| sin(Φ))2\n= 2|̺| [cos(Φ)∇(|̺| cos(Φ)) + sin(Φ)∇(|̺| sin(Φ))] .\nNote that we have\n1\nT\nPT\nt=1 cos(∆φ(t))) = |̺| cos(Φ) and\n1\nT\nPT\nt=1 sin(∆φ(t))) = |̺| sin(Φ), so we get\n∇|̺|2 = 2|̺|\n(\ncos(Φ)∇\n\"\n1\nT\nT\nX\nt=1\ncos(∆φ(t)))\n#\n+\n+ sin(Φ)∇\n\"\n1\nT\nT\nX\nt=1\nsin(∆φ(t)))\n#)\n= 2|̺|\nT\nT\nX\nt=1\nh\nsin(Φ) cos(∆φ(t)) −cos(Φ) sin(∆φ(t))\ni\n×\n× ∇∆φ(t)\n= 2|̺|\nT\nT\nX\nt=1\nsin[Φ −∆φ(t)]∇∆φ(t).\n(1)\nMiguel Almeida is with the Institute of Telecommunications, Superior\nTechnical Institute, Portugal. Email: malmeida@lx.it.pt\nJan-Hendrik Schleimer is with the Bernstein Center for Computational Neu-\nroscience, Humboldt University, Berlin. Email: jan-hendrik.schleimer@bccn-\nberlin.de\nJos´e Bioucas-Dias is with the Institute of Telecommunications, Superior\nTechnical Institute, Portugal. Email: bioucas@lx.it.pt\nRicardo Vig´ario is with the Adaptive Informatics Research Centre, Aalto\nUniversity, Finland. Email: ricardo.vigario@hut.ﬁ\nLet’s now take a closer look on ∇∆φ(t). Note that\nφ(t) = arctan\n\u0012wTxh(t)\nwTx(t)\n\u0013\nor\nφ(t) = arctan\n\u0012wTxh(t)\nwTx(t)\n\u0013\n+ π.\nBecause of this we can say, if wTx(t) ̸= 0, that ∇φ(t) =\n∇arctan\n\u0010\nwTxh(t)\nwTx(t)\n\u0011\n. On the other hand, since ∆φ(t) = φ(t)−\nψ(t) and ψ(t) does not depend on w, we have (we will omit\nthe time dependence for the sake of clarity):\n∇∆φ = ∇φ −∇ψ = ∇φ = ∇arctan\n\u0012wTxh\nwTx\n\u0013\n=\nxh · wTx −x · wTxh\n\u0014\n1 +\n\u0010\nwTxh\nwTx\n\u00112\u0015\n·\n\u0010\nwTx\n\u00112 = Γ x(t) · w\nY 2(t)\n,\nwhere Y 2(t) =\n\u0000wTx(t)\n\u00012 +\n\u0000wTxh(t)\n\u00012 is the squared\nmagnitude of the estimated source, and Γ x(t) = xh(t)xT(t)−\nx(t)xhT(t), thus Γ xij(t) = Xi(t)Xj(t) sin(φi(t) −φj(t)).\nWe can now replace ∇∆φ(t) in (1) to obtain\n∇|̺|2 = 2|̺|\nT\n\" T\nX\nt=1\nsin[Φ −∆φ(t)]\nY 2(t)\nΓ x(t)\n#\nw\n= 2|̺|\n\u001csin[Φ −∆φ(t)]\nY 2(t)\nΓ x(t)\n\u001d\nw.\n(2)\nAPPENDIX B\nGRADIENT OF Jl IN IPA\nIn this section we show that the gradient of Jl in Eq. 7 of\n[1] is given by Eq. 8 of [1]. Throughout this whole section,\nwe will omit the dependence on the subspace l, for the sake\nof clarity. In other words, we are assuming (with no loss of\ngenerality) that only one subspace was found. Whenever we\nwrite W, y, ym or z, we will be referring to Wl, yl, (yl)m\nor zl.\nThe derivative of log |det W| is W−T. We will therefore\nfocus on the gradient of the ﬁrst term of Eq. 7 of [1], which\nwe will denote by P:\nP ≡1 −λ\nN 2\nX\nm,n\n|̺mn|2.\nLet’s rewrite P as P = 1−λ\nN 2\nP\nm,n pmn with pmn = |̺mn|2.\nDeﬁne ∆φmn = φm −φn. Omitting the time dependency, we\n\n\n2\nhave\n∇wjpmn = 2|̺mn|∇wj\n\f\f\nei∆φmn\u000b\f\f\n= |̺mn| ×\n\u0010\ncos(∆φmn)\n\u000b2 + i\n\nsin(∆φmn)\n\u000b2\u0011−1/2\n×\n×\nh\n2\n\ncos(∆φmn)\n\u000b\n∇wj\n\ncos(∆φmn)\n\u000b\n+\n+ 2\n\nsin(∆φmn)\n\u000b\n∇wj\n\nsin(∆φmn)\n\u000bi\n= 2|̺mn|\n\f\f\nei∆φmn\u000b\f\f−1 ×\n×\nh\n−\n\ncos(∆φmn)\n\u000bD\nsin(∆φmn)∇wj∆φmn\nE\n+\n+\n\nsin(∆φmn)\n\u000bD\ncos(∆φmn)∇wj∆φmn\nEi\n,\n(3)\nwhere we have interchanged the partial derivative and the time\naverage operators, and used\n\u0010\ncos(∆φmn)\n\u000b2 + i\n\nsin(∆φmn)\n\u000b2\u00111/2\n=\n\f\f\nei∆φmn\u000b\f\f .\nSince φm is the phase of the m-th measurement, its deriva-\ntive with respect to any wj is zero unless m = j or n = j.\nIn the former case, a reasoning similar to Appendix A shows\nthat\n∇wj∆φjk ≡∇wjφj −∇wjφk = ∇wjφj =\n= [zh · z −z · zh] · wj\nY 2\nj\n= Γ z · wj\nY 2\nj\n,\n(4)\nwhere Γ z(t) = zh(t)zT(t) −z(t)zhT(t). It is easy to see\nthat ∇wj∆φjk = −∇wj∆φkj. Furthermore, pmm = 1 by\ndeﬁnition, hence ∇wjpmm = 0 for all m and j. From these\nconsiderations, the only nonzero terms in the derivative of P\nare of the form\n∇wjpjk = ∇wjpkj = 2|̺jk|\n\f\f\f\nD\nei(φj−φk)E\f\f\f\n−1\n×\n×\n\"\n−\n\ncos(∆φjk)\n\u000b\n*\nsin(∆φjk)Γ z · wj\nY 2\nj\n+\n+\n+\n\nsin(∆φjk)\n\u000b\n*\ncos(∆φjk)Γ z · wj\nY 2\nj\n+#\n.\n(5)\nWe now deﬁne Ψjk ≡⟨φj −φk⟩= ⟨∆φjk⟩. Plugging in\nthis deﬁnition into Eq. (5) we obtain\n∇wjpjk = 2|̺jk|×\n×\n\"\n−cos(Ψjk)\n*\nsin(∆φjk)Γ z · wj\nY 2\nj\n+\n+\n+ sin(Ψjk)\n*\ncos(∆φjk)Γ z · wj\nY 2\nj\n+#\n= 2|̺jk|\n\"*\n−cosΨjk sin ∆φjk\nΓ z · wj\nY 2\nj\n+\n+\n+\n*\nsin Ψjk cos ∆φjk\nΓ z · wj\nY 2\nj\n+#\n= 2|̺jk|\n*\nsin (Ψjk −∆φjk) Γ z\nY 2\nj\n+\n· wj,\nwhere we again used sin(a −b) = sin a cos b −cos a sin b in\nthe last step. Finally,\n∇wjP = 1 −λ\nN 2\nX\nm,n\n∇wjpmn = 21 −λ\nN 2\nX\nm<n\n∇wjpmn =\n= 41 −λ\nN 2\nN\nX\nk=1\n|̺jk|\n\u001c\nsin [Ψjk −∆φjk(t)] Γ z(t)\nYj(t)2\n\u001d\n· wj.\nwhich is Eq. 8 of [1].\nAPPENDIX C\nGRADIENT OF J IN PSCA\nIn this section we derive Eq. 10 of [1] for the gradient of\nJ. Recall that J is given by\nJ ≡\nP\nX\nj=1\n\f\f\f\f\f\nN\nX\ni=1\nuij\n\f\f\f\f\f =\nP\nX\nj=1\n\f\f\f\f\f\nN\nX\ni=1\nP\nX\nk=1\nvikwkj\n\f\f\f\f\f\nwhere the wkj are real coefﬁcients that we want to optimize\nand the vik are ﬁxed complex numbers. Also recall that Re(.)\nand Im(.) denote the real and imaginary parts.\nWe begin by expanding the complex absolute value:\nX\nj\n\f\f\f\f\f\f\nX\ni,k\nvikwkj\n\f\f\f\f\f\f\n=\nX\nj\n\n\n\n\"\nRe\n X\ni,k\nvikwkj\n!#2\n+\n\"\nIm\n X\ni,k\nvikwkj\n!#2\n\n\n1/2\n.\nWhen computing the derivative in order to wkj, only one term\nin the leftmost sum matters. Thus,\n∂J\n∂wkj\n=\n= 2\n\n\n\"\nRe\n X\ni,k\nvikwkj\n!#2\n+\n\"\nIm\n X\ni,k\nvikwkj\n!#2\n\n−1/2\n×\n×\n\n\n∂\nh\nRe\n\u0010P\ni,k vikwkj\n\u0011i2\n∂wkj\n+\n∂\nh\nIm\n\u0010P\ni,k vikwkj\n\u0011i2\n∂wkj\n\n\n=\n1\n|P\ni uij|\n\nRe\n\nX\ni,k\nvikwkj\n\n\n∂Re\n\u0010P\ni,k vikwkj\n\u0011\n∂wkj\n+\n+ Im\n\nX\ni,k\nvikwkj\n\n\n∂Im\n\u0010P\ni,k vikwkj\n\u0011\n∂wkj\n\n.\n(6)\nIn the sums inside the derivatives, the sum on k can be\ndropped as only one of those terms will be nonzero. Therefore,\n∂Re\n\u0010 P\ni,k vikwkj\n\u0011\n∂wkj\n=\n∂Re\n\u0010 P\ni vikwkj\n\u0011\n∂wkj\n= ∂Re (P\ni vik) wkj\n∂wkj\n= Re\n X\ni\nvik\n!\n= Re (¯vk) ,\n\n\n3\nwhere we used ¯vi ≡P\nk vki to denote the sum of the i-th\ncolumn of V. Similarly,\n∂Im\n\u0010 P\ni,k vikwkj\n\u0011\n∂wkj\n= Im (¯vk) .\nThese results, with the notation ¯uj ≡P\nk vjk as the sum of\nthe j-th column of U, can be plugged into Eq. (6) to yield\nGkj =\n∂J\n∂wkj\n=\n1\n|¯uj|\nh\nRe( ¯vk) × Re(¯uj) + Im( ¯vk) × Im(¯uj)\ni\n.\nAPPENDIX D\nMEAN FIELD\nIn this section we derive Eq. 9 of [1] for the interaction\nof an oscillator with the cluster it is part of. We will assume\nthat there are Nj oscillators in this cluster, coupled all-to-\nall with the same coupling coefﬁcient κ, and that all inter-\ncluster interactions are weak enough to be disregarded. We\nbegin with Kuramoto’s model (Eq. 1 of [1]) omitting the time\ndependency:\n˙φi = ωi +\nX\nk∈cj\nκik sin(φk −φi) +\nX\nk /∈cj\nκik sin(φk −φi)\n˙φi = ωi +\nX\nk∈cj\nκik sin(φk −φi)\n= ωi +\nX\nk∈cj\nκik\nei(φj−φi) −e−i(φj−φi)\n2i\n= ωi + e−iφi\n2i\nX\nk∈cj\nκikeiφk −eiφi\n2i\nX\nk∈cj\nκike−iφk.\nWe now plug in the deﬁnition of mean ﬁeld ̺cjeiΦcj\n=\n1\nNj\nP\nk∈cj eiφk to obtain\n˙φi = ωi + Nj\ne−iφi\n2i κ̺cjeiΦcj −Nj\neiφi\n2i κ̺cje−iΦcj\n= ωi + Njκ̺cj\n\u0002\nsin(Φcj −φi) −sin(φi −Φcj)\n\u0003\n= ωi + 2Njκ̺cj sin(Φcj −φi).\nREFERENCES\n[1] M. Almeida, J.-H. Schleimer, J. Bioucas-Dias, and R. Vig´ario, “Source\nseparation and clustering of phase-locked subspaces,” IEEE Transactions\non Neural Networks (accepted), 2011.\n\n\n",
      "concepts": [
        "also recall that re",
        "clustering",
        "closer look on",
        "to yield\ngkj",
        "from these\nconsiderations",
        "ai",
        "in other words",
        "due to space limitations",
        "can be plugged into eq",
        "separation",
        "we begin by noting that",
        "appendix",
        "we are assuming",
        "deﬁnition",
        "vki to denote the sum of the",
        "which\nwe will denote by",
        "is the phase of the",
        "vigario",
        "and that all inter",
        "complex numbers",
        "clarity",
        "wj\n\nsin",
        "denote the real and imaginary parts",
        "mean field\nin this section we derive eq",
        "we\nbegin with kuramoto",
        "arctan",
        "is given by\neq",
        "aalto\nuniversity",
        "these results",
        "is given by eq",
        "schleimer",
        "in\nthe last step",
        "vikwkj\n\f\f\f\f\f\nwhere the wkj are real",
        "we will omit the dependence on the subspace",
        "wjpmm",
        "cj sin",
        "model",
        "berlin",
        "in rpa\nin this section we derive that the gradient of",
        "and used",
        "when computing the derivative in order to wkj",
        "dias is with the institute of telecommunications",
        "thus",
        "in psca\nin this section we derive eq",
        "we have",
        "only one term\nin the leftmost sum matters",
        "th measurement",
        "are of the form",
        "finally",
        "references",
        "research",
        "subspaces\nappendix",
        "superior\ntechnical institute",
        "similarly",
        "for the sake\nof clarity",
        "throughout this whole section",
        "which is eq",
        "with no loss of\ngenerality",
        "wjpmn",
        "university",
        "as in eq",
        "where we again used sin",
        "in the sums inside the derivatives",
        "plugging in\nthis",
        "cluster interactions are weak enough to be disregarded",
        "wtxh",
        "that only one subspace was found",
        "on the other hand",
        "we obtain",
        "ario",
        "the sum on",
        "humboldt university",
        "presented some results without proof",
        "wjpkj",
        "source\nseparation and clustering of phase",
        "gradient of",
        "is the phase of the estimated\nsource",
        "does not depend on",
        "de\njos",
        "locking",
        "th column of",
        "we begin by expanding the complex absolute value",
        "wtxh\nwtx",
        "we can now replace",
        "coupled all",
        "of mean",
        "measurement",
        "where we have interchanged the partial derivative and the time\naverage operators",
        "reasoning similar to appendix",
        "therefore",
        "neural_network",
        "miguel almeida is with the institute of telecommunications",
        "hendrik",
        "note that",
        "where we used",
        "the only nonzero terms in the derivative of",
        "whenever we\nwrite",
        "for the interaction\nof an oscillator with the cluster it is part of",
        "wj\n\ncos",
        "almeida"
      ],
      "document_type": "academic_paper",
      "content_hash": "ba3ce0c21df79df4",
      "size": 67590,
      "ingestion_time": "2025-06-06T07:33:42.498166",
      "research_field": "mathematics",
      "technical_level": "graduate",
      "citation_count": 10
    },
    {
      "file_path": "C:\\Users\\jason\\Desktop\\tori\\kha\\data\\USB Drive\\docs\\BANKSY\\alan-core\\banksy_files\\Hierarchical and Multi-Dimensional Synchronization\\neural oscillation\\Fourier oscillator\\1106.2474v1.pdf",
      "file_type": "pdf_document",
      "title": "1106.2474v1",
      "author": "Unknown",
      "subject": "",
      "pages": 3,
      "text_content": "arXiv:1106.2474v1  [stat.ML]  13 Jun 2011\n1\nSource Separation and Clustering\nof Phase-Locked Subspaces: Derivations and Proofs\nMiguel Almeida, Jan-Hendrik Schleimer, Jos´e Bioucas-Dias, Ricardo Vig´ario\nAbstract—Due to space limitations, our submission “Source\nSeparation and Clustering of Phase-Locked Subspaces”, accepted\nfor publication on the IEEE Transactions on Neural Networks\nin 2011, presented some results without proof. Those proofs are\nprovided in this paper.\nIndex Terms—phase-locking, synchrony, source separation,\nclustering, subspaces\nAPPENDIX A\nGRADIENT OF |̺|2 IN RPA\nIn this section we derive that the gradient of |̺|2 is given by\nEq. 6 of [1], where |̺| is deﬁned as in Eq. 5 of [1]. Recall that\n∆φ(t) = φ(t)−ψ(t), where φ(t) is the phase of the estimated\nsource y(t) = wT x(t) and ψ(t) is the phase of the reference\nu(t). Further, deﬁne ̺ ≡|̺|eiΦ.\nWe begin by noting that |̺|2 = (|̺| cos(Φ))2+(|̺| sin(Φ))2,\nso that\n∇|̺|2 = ∇(|̺| cos(Φ))2 + ∇(|̺| sin(Φ))2\n= 2|̺| [cos(Φ)∇(|̺| cos(Φ)) + sin(Φ)∇(|̺| sin(Φ))] .\nNote that we have\n1\nT\nPT\nt=1 cos(∆φ(t))) = |̺| cos(Φ) and\n1\nT\nPT\nt=1 sin(∆φ(t))) = |̺| sin(Φ), so we get\n∇|̺|2 = 2|̺|\n(\ncos(Φ)∇\n\"\n1\nT\nT\nX\nt=1\ncos(∆φ(t)))\n#\n+\n+ sin(Φ)∇\n\"\n1\nT\nT\nX\nt=1\nsin(∆φ(t)))\n#)\n= 2|̺|\nT\nT\nX\nt=1\nh\nsin(Φ) cos(∆φ(t)) −cos(Φ) sin(∆φ(t))\ni\n×\n× ∇∆φ(t)\n= 2|̺|\nT\nT\nX\nt=1\nsin[Φ −∆φ(t)]∇∆φ(t).\n(1)\nMiguel Almeida is with the Institute of Telecommunications, Superior\nTechnical Institute, Portugal. Email: malmeida@lx.it.pt\nJan-Hendrik Schleimer is with the Bernstein Center for Computational Neu-\nroscience, Humboldt University, Berlin. Email: jan-hendrik.schleimer@bccn-\nberlin.de\nJos´e Bioucas-Dias is with the Institute of Telecommunications, Superior\nTechnical Institute, Portugal. Email: bioucas@lx.it.pt\nRicardo Vig´ario is with the Adaptive Informatics Research Centre, Aalto\nUniversity, Finland. Email: ricardo.vigario@hut.ﬁ\nLet’s now take a closer look on ∇∆φ(t). Note that\nφ(t) = arctan\n\u0012wTxh(t)\nwTx(t)\n\u0013\nor\nφ(t) = arctan\n\u0012wTxh(t)\nwTx(t)\n\u0013\n+ π.\nBecause of this we can say, if wTx(t) ̸= 0, that ∇φ(t) =\n∇arctan\n\u0010\nwTxh(t)\nwTx(t)\n\u0011\n. On the other hand, since ∆φ(t) = φ(t)−\nψ(t) and ψ(t) does not depend on w, we have (we will omit\nthe time dependence for the sake of clarity):\n∇∆φ = ∇φ −∇ψ = ∇φ = ∇arctan\n\u0012wTxh\nwTx\n\u0013\n=\nxh · wTx −x · wTxh\n\u0014\n1 +\n\u0010\nwTxh\nwTx\n\u00112\u0015\n·\n\u0010\nwTx\n\u00112 = Γ x(t) · w\nY 2(t)\n,\nwhere Y 2(t) =\n\u0000wTx(t)\n\u00012 +\n\u0000wTxh(t)\n\u00012 is the squared\nmagnitude of the estimated source, and Γ x(t) = xh(t)xT(t)−\nx(t)xhT(t), thus Γ xij(t) = Xi(t)Xj(t) sin(φi(t) −φj(t)).\nWe can now replace ∇∆φ(t) in (1) to obtain\n∇|̺|2 = 2|̺|\nT\n\" T\nX\nt=1\nsin[Φ −∆φ(t)]\nY 2(t)\nΓ x(t)\n#\nw\n= 2|̺|\n\u001csin[Φ −∆φ(t)]\nY 2(t)\nΓ x(t)\n\u001d\nw.\n(2)\nAPPENDIX B\nGRADIENT OF Jl IN IPA\nIn this section we show that the gradient of Jl in Eq. 7 of\n[1] is given by Eq. 8 of [1]. Throughout this whole section,\nwe will omit the dependence on the subspace l, for the sake\nof clarity. In other words, we are assuming (with no loss of\ngenerality) that only one subspace was found. Whenever we\nwrite W, y, ym or z, we will be referring to Wl, yl, (yl)m\nor zl.\nThe derivative of log |det W| is W−T. We will therefore\nfocus on the gradient of the ﬁrst term of Eq. 7 of [1], which\nwe will denote by P:\nP ≡1 −λ\nN 2\nX\nm,n\n|̺mn|2.\nLet’s rewrite P as P = 1−λ\nN 2\nP\nm,n pmn with pmn = |̺mn|2.\nDeﬁne ∆φmn = φm −φn. Omitting the time dependency, we\n\n\n2\nhave\n∇wjpmn = 2|̺mn|∇wj\n\f\f\nei∆φmn\u000b\f\f\n= |̺mn| ×\n\u0010\ncos(∆φmn)\n\u000b2 + i\n\nsin(∆φmn)\n\u000b2\u0011−1/2\n×\n×\nh\n2\n\ncos(∆φmn)\n\u000b\n∇wj\n\ncos(∆φmn)\n\u000b\n+\n+ 2\n\nsin(∆φmn)\n\u000b\n∇wj\n\nsin(∆φmn)\n\u000bi\n= 2|̺mn|\n\f\f\nei∆φmn\u000b\f\f−1 ×\n×\nh\n−\n\ncos(∆φmn)\n\u000bD\nsin(∆φmn)∇wj∆φmn\nE\n+\n+\n\nsin(∆φmn)\n\u000bD\ncos(∆φmn)∇wj∆φmn\nEi\n,\n(3)\nwhere we have interchanged the partial derivative and the time\naverage operators, and used\n\u0010\ncos(∆φmn)\n\u000b2 + i\n\nsin(∆φmn)\n\u000b2\u00111/2\n=\n\f\f\nei∆φmn\u000b\f\f .\nSince φm is the phase of the m-th measurement, its deriva-\ntive with respect to any wj is zero unless m = j or n = j.\nIn the former case, a reasoning similar to Appendix A shows\nthat\n∇wj∆φjk ≡∇wjφj −∇wjφk = ∇wjφj =\n= [zh · z −z · zh] · wj\nY 2\nj\n= Γ z · wj\nY 2\nj\n,\n(4)\nwhere Γ z(t) = zh(t)zT(t) −z(t)zhT(t). It is easy to see\nthat ∇wj∆φjk = −∇wj∆φkj. Furthermore, pmm = 1 by\ndeﬁnition, hence ∇wjpmm = 0 for all m and j. From these\nconsiderations, the only nonzero terms in the derivative of P\nare of the form\n∇wjpjk = ∇wjpkj = 2|̺jk|\n\f\f\f\nD\nei(φj−φk)E\f\f\f\n−1\n×\n×\n\"\n−\n\ncos(∆φjk)\n\u000b\n*\nsin(∆φjk)Γ z · wj\nY 2\nj\n+\n+\n+\n\nsin(∆φjk)\n\u000b\n*\ncos(∆φjk)Γ z · wj\nY 2\nj\n+#\n.\n(5)\nWe now deﬁne Ψjk ≡⟨φj −φk⟩= ⟨∆φjk⟩. Plugging in\nthis deﬁnition into Eq. (5) we obtain\n∇wjpjk = 2|̺jk|×\n×\n\"\n−cos(Ψjk)\n*\nsin(∆φjk)Γ z · wj\nY 2\nj\n+\n+\n+ sin(Ψjk)\n*\ncos(∆φjk)Γ z · wj\nY 2\nj\n+#\n= 2|̺jk|\n\"*\n−cosΨjk sin ∆φjk\nΓ z · wj\nY 2\nj\n+\n+\n+\n*\nsin Ψjk cos ∆φjk\nΓ z · wj\nY 2\nj\n+#\n= 2|̺jk|\n*\nsin (Ψjk −∆φjk) Γ z\nY 2\nj\n+\n· wj,\nwhere we again used sin(a −b) = sin a cos b −cos a sin b in\nthe last step. Finally,\n∇wjP = 1 −λ\nN 2\nX\nm,n\n∇wjpmn = 21 −λ\nN 2\nX\nm<n\n∇wjpmn =\n= 41 −λ\nN 2\nN\nX\nk=1\n|̺jk|\n\u001c\nsin [Ψjk −∆φjk(t)] Γ z(t)\nYj(t)2\n\u001d\n· wj.\nwhich is Eq. 8 of [1].\nAPPENDIX C\nGRADIENT OF J IN PSCA\nIn this section we derive Eq. 10 of [1] for the gradient of\nJ. Recall that J is given by\nJ ≡\nP\nX\nj=1\n\f\f\f\f\f\nN\nX\ni=1\nuij\n\f\f\f\f\f =\nP\nX\nj=1\n\f\f\f\f\f\nN\nX\ni=1\nP\nX\nk=1\nvikwkj\n\f\f\f\f\f\nwhere the wkj are real coefﬁcients that we want to optimize\nand the vik are ﬁxed complex numbers. Also recall that Re(.)\nand Im(.) denote the real and imaginary parts.\nWe begin by expanding the complex absolute value:\nX\nj\n\f\f\f\f\f\f\nX\ni,k\nvikwkj\n\f\f\f\f\f\f\n=\nX\nj\n\n\n\n\"\nRe\n X\ni,k\nvikwkj\n!#2\n+\n\"\nIm\n X\ni,k\nvikwkj\n!#2\n\n\n1/2\n.\nWhen computing the derivative in order to wkj, only one term\nin the leftmost sum matters. Thus,\n∂J\n∂wkj\n=\n= 2\n\n\n\"\nRe\n X\ni,k\nvikwkj\n!#2\n+\n\"\nIm\n X\ni,k\nvikwkj\n!#2\n\n−1/2\n×\n×\n\n\n∂\nh\nRe\n\u0010P\ni,k vikwkj\n\u0011i2\n∂wkj\n+\n∂\nh\nIm\n\u0010P\ni,k vikwkj\n\u0011i2\n∂wkj\n\n\n=\n1\n|P\ni uij|\n\nRe\n\nX\ni,k\nvikwkj\n\n\n∂Re\n\u0010P\ni,k vikwkj\n\u0011\n∂wkj\n+\n+ Im\n\nX\ni,k\nvikwkj\n\n\n∂Im\n\u0010P\ni,k vikwkj\n\u0011\n∂wkj\n\n.\n(6)\nIn the sums inside the derivatives, the sum on k can be\ndropped as only one of those terms will be nonzero. Therefore,\n∂Re\n\u0010 P\ni,k vikwkj\n\u0011\n∂wkj\n=\n∂Re\n\u0010 P\ni vikwkj\n\u0011\n∂wkj\n= ∂Re (P\ni vik) wkj\n∂wkj\n= Re\n X\ni\nvik\n!\n= Re (¯vk) ,\n\n\n3\nwhere we used ¯vi ≡P\nk vki to denote the sum of the i-th\ncolumn of V. Similarly,\n∂Im\n\u0010 P\ni,k vikwkj\n\u0011\n∂wkj\n= Im (¯vk) .\nThese results, with the notation ¯uj ≡P\nk vjk as the sum of\nthe j-th column of U, can be plugged into Eq. (6) to yield\nGkj =\n∂J\n∂wkj\n=\n1\n|¯uj|\nh\nRe( ¯vk) × Re(¯uj) + Im( ¯vk) × Im(¯uj)\ni\n.\nAPPENDIX D\nMEAN FIELD\nIn this section we derive Eq. 9 of [1] for the interaction\nof an oscillator with the cluster it is part of. We will assume\nthat there are Nj oscillators in this cluster, coupled all-to-\nall with the same coupling coefﬁcient κ, and that all inter-\ncluster interactions are weak enough to be disregarded. We\nbegin with Kuramoto’s model (Eq. 1 of [1]) omitting the time\ndependency:\n˙φi = ωi +\nX\nk∈cj\nκik sin(φk −φi) +\nX\nk /∈cj\nκik sin(φk −φi)\n˙φi = ωi +\nX\nk∈cj\nκik sin(φk −φi)\n= ωi +\nX\nk∈cj\nκik\nei(φj−φi) −e−i(φj−φi)\n2i\n= ωi + e−iφi\n2i\nX\nk∈cj\nκikeiφk −eiφi\n2i\nX\nk∈cj\nκike−iφk.\nWe now plug in the deﬁnition of mean ﬁeld ̺cjeiΦcj\n=\n1\nNj\nP\nk∈cj eiφk to obtain\n˙φi = ωi + Nj\ne−iφi\n2i κ̺cjeiΦcj −Nj\neiφi\n2i κ̺cje−iΦcj\n= ωi + Njκ̺cj\n\u0002\nsin(Φcj −φi) −sin(φi −Φcj)\n\u0003\n= ωi + 2Njκ̺cj sin(Φcj −φi).\nREFERENCES\n[1] M. Almeida, J.-H. Schleimer, J. Bioucas-Dias, and R. Vig´ario, “Source\nseparation and clustering of phase-locked subspaces,” IEEE Transactions\non Neural Networks (accepted), 2011.\n\n\n",
      "concepts": [
        "also recall that re",
        "clustering",
        "closer look on",
        "to yield\ngkj",
        "from these\nconsiderations",
        "ai",
        "in other words",
        "due to space limitations",
        "can be plugged into eq",
        "separation",
        "we begin by noting that",
        "appendix",
        "we are assuming",
        "deﬁnition",
        "vki to denote the sum of the",
        "which\nwe will denote by",
        "is the phase of the",
        "vigario",
        "and that all inter",
        "complex numbers",
        "clarity",
        "wj\n\nsin",
        "denote the real and imaginary parts",
        "mean field\nin this section we derive eq",
        "we\nbegin with kuramoto",
        "arctan",
        "is given by\neq",
        "aalto\nuniversity",
        "these results",
        "is given by eq",
        "schleimer",
        "in\nthe last step",
        "vikwkj\n\f\f\f\f\f\nwhere the wkj are real",
        "we will omit the dependence on the subspace",
        "wjpmm",
        "cj sin",
        "model",
        "berlin",
        "in rpa\nin this section we derive that the gradient of",
        "and used",
        "when computing the derivative in order to wkj",
        "dias is with the institute of telecommunications",
        "thus",
        "in psca\nin this section we derive eq",
        "we have",
        "only one term\nin the leftmost sum matters",
        "th measurement",
        "are of the form",
        "finally",
        "references",
        "research",
        "subspaces\nappendix",
        "superior\ntechnical institute",
        "similarly",
        "for the sake\nof clarity",
        "throughout this whole section",
        "which is eq",
        "with no loss of\ngenerality",
        "wjpmn",
        "university",
        "as in eq",
        "where we again used sin",
        "in the sums inside the derivatives",
        "plugging in\nthis",
        "cluster interactions are weak enough to be disregarded",
        "wtxh",
        "that only one subspace was found",
        "on the other hand",
        "we obtain",
        "ario",
        "the sum on",
        "humboldt university",
        "presented some results without proof",
        "wjpkj",
        "source\nseparation and clustering of phase",
        "gradient of",
        "is the phase of the estimated\nsource",
        "does not depend on",
        "de\njos",
        "locking",
        "th column of",
        "we begin by expanding the complex absolute value",
        "wtxh\nwtx",
        "we can now replace",
        "coupled all",
        "of mean",
        "measurement",
        "where we have interchanged the partial derivative and the time\naverage operators",
        "reasoning similar to appendix",
        "therefore",
        "neural_network",
        "miguel almeida is with the institute of telecommunications",
        "hendrik",
        "note that",
        "where we used",
        "the only nonzero terms in the derivative of",
        "whenever we\nwrite",
        "for the interaction\nof an oscillator with the cluster it is part of",
        "wj\n\ncos",
        "almeida"
      ],
      "document_type": "academic_paper",
      "content_hash": "ba3ce0c21df79df4",
      "size": 67590,
      "ingestion_time": "2025-06-06T07:33:43.120304",
      "research_field": "mathematics",
      "technical_level": "graduate",
      "citation_count": 10
    },
    {
      "file_path": "C:\\Users\\jason\\Desktop\\tori\\kha\\data\\USB Drive\\docs\\BANKSY\\HierarchicalSynchronization\\Fourier oscillator\\1106.2474v1.pdf",
      "file_type": "pdf_document",
      "title": "1106.2474v1",
      "author": "Unknown",
      "subject": "",
      "pages": 3,
      "text_content": "arXiv:1106.2474v1  [stat.ML]  13 Jun 2011\n1\nSource Separation and Clustering\nof Phase-Locked Subspaces: Derivations and Proofs\nMiguel Almeida, Jan-Hendrik Schleimer, Jos´e Bioucas-Dias, Ricardo Vig´ario\nAbstract—Due to space limitations, our submission “Source\nSeparation and Clustering of Phase-Locked Subspaces”, accepted\nfor publication on the IEEE Transactions on Neural Networks\nin 2011, presented some results without proof. Those proofs are\nprovided in this paper.\nIndex Terms—phase-locking, synchrony, source separation,\nclustering, subspaces\nAPPENDIX A\nGRADIENT OF |̺|2 IN RPA\nIn this section we derive that the gradient of |̺|2 is given by\nEq. 6 of [1], where |̺| is deﬁned as in Eq. 5 of [1]. Recall that\n∆φ(t) = φ(t)−ψ(t), where φ(t) is the phase of the estimated\nsource y(t) = wT x(t) and ψ(t) is the phase of the reference\nu(t). Further, deﬁne ̺ ≡|̺|eiΦ.\nWe begin by noting that |̺|2 = (|̺| cos(Φ))2+(|̺| sin(Φ))2,\nso that\n∇|̺|2 = ∇(|̺| cos(Φ))2 + ∇(|̺| sin(Φ))2\n= 2|̺| [cos(Φ)∇(|̺| cos(Φ)) + sin(Φ)∇(|̺| sin(Φ))] .\nNote that we have\n1\nT\nPT\nt=1 cos(∆φ(t))) = |̺| cos(Φ) and\n1\nT\nPT\nt=1 sin(∆φ(t))) = |̺| sin(Φ), so we get\n∇|̺|2 = 2|̺|\n(\ncos(Φ)∇\n\"\n1\nT\nT\nX\nt=1\ncos(∆φ(t)))\n#\n+\n+ sin(Φ)∇\n\"\n1\nT\nT\nX\nt=1\nsin(∆φ(t)))\n#)\n= 2|̺|\nT\nT\nX\nt=1\nh\nsin(Φ) cos(∆φ(t)) −cos(Φ) sin(∆φ(t))\ni\n×\n× ∇∆φ(t)\n= 2|̺|\nT\nT\nX\nt=1\nsin[Φ −∆φ(t)]∇∆φ(t).\n(1)\nMiguel Almeida is with the Institute of Telecommunications, Superior\nTechnical Institute, Portugal. Email: malmeida@lx.it.pt\nJan-Hendrik Schleimer is with the Bernstein Center for Computational Neu-\nroscience, Humboldt University, Berlin. Email: jan-hendrik.schleimer@bccn-\nberlin.de\nJos´e Bioucas-Dias is with the Institute of Telecommunications, Superior\nTechnical Institute, Portugal. Email: bioucas@lx.it.pt\nRicardo Vig´ario is with the Adaptive Informatics Research Centre, Aalto\nUniversity, Finland. Email: ricardo.vigario@hut.ﬁ\nLet’s now take a closer look on ∇∆φ(t). Note that\nφ(t) = arctan\n\u0012wTxh(t)\nwTx(t)\n\u0013\nor\nφ(t) = arctan\n\u0012wTxh(t)\nwTx(t)\n\u0013\n+ π.\nBecause of this we can say, if wTx(t) ̸= 0, that ∇φ(t) =\n∇arctan\n\u0010\nwTxh(t)\nwTx(t)\n\u0011\n. On the other hand, since ∆φ(t) = φ(t)−\nψ(t) and ψ(t) does not depend on w, we have (we will omit\nthe time dependence for the sake of clarity):\n∇∆φ = ∇φ −∇ψ = ∇φ = ∇arctan\n\u0012wTxh\nwTx\n\u0013\n=\nxh · wTx −x · wTxh\n\u0014\n1 +\n\u0010\nwTxh\nwTx\n\u00112\u0015\n·\n\u0010\nwTx\n\u00112 = Γ x(t) · w\nY 2(t)\n,\nwhere Y 2(t) =\n\u0000wTx(t)\n\u00012 +\n\u0000wTxh(t)\n\u00012 is the squared\nmagnitude of the estimated source, and Γ x(t) = xh(t)xT(t)−\nx(t)xhT(t), thus Γ xij(t) = Xi(t)Xj(t) sin(φi(t) −φj(t)).\nWe can now replace ∇∆φ(t) in (1) to obtain\n∇|̺|2 = 2|̺|\nT\n\" T\nX\nt=1\nsin[Φ −∆φ(t)]\nY 2(t)\nΓ x(t)\n#\nw\n= 2|̺|\n\u001csin[Φ −∆φ(t)]\nY 2(t)\nΓ x(t)\n\u001d\nw.\n(2)\nAPPENDIX B\nGRADIENT OF Jl IN IPA\nIn this section we show that the gradient of Jl in Eq. 7 of\n[1] is given by Eq. 8 of [1]. Throughout this whole section,\nwe will omit the dependence on the subspace l, for the sake\nof clarity. In other words, we are assuming (with no loss of\ngenerality) that only one subspace was found. Whenever we\nwrite W, y, ym or z, we will be referring to Wl, yl, (yl)m\nor zl.\nThe derivative of log |det W| is W−T. We will therefore\nfocus on the gradient of the ﬁrst term of Eq. 7 of [1], which\nwe will denote by P:\nP ≡1 −λ\nN 2\nX\nm,n\n|̺mn|2.\nLet’s rewrite P as P = 1−λ\nN 2\nP\nm,n pmn with pmn = |̺mn|2.\nDeﬁne ∆φmn = φm −φn. Omitting the time dependency, we\n\n\n2\nhave\n∇wjpmn = 2|̺mn|∇wj\n\f\f\nei∆φmn\u000b\f\f\n= |̺mn| ×\n\u0010\ncos(∆φmn)\n\u000b2 + i\n\nsin(∆φmn)\n\u000b2\u0011−1/2\n×\n×\nh\n2\n\ncos(∆φmn)\n\u000b\n∇wj\n\ncos(∆φmn)\n\u000b\n+\n+ 2\n\nsin(∆φmn)\n\u000b\n∇wj\n\nsin(∆φmn)\n\u000bi\n= 2|̺mn|\n\f\f\nei∆φmn\u000b\f\f−1 ×\n×\nh\n−\n\ncos(∆φmn)\n\u000bD\nsin(∆φmn)∇wj∆φmn\nE\n+\n+\n\nsin(∆φmn)\n\u000bD\ncos(∆φmn)∇wj∆φmn\nEi\n,\n(3)\nwhere we have interchanged the partial derivative and the time\naverage operators, and used\n\u0010\ncos(∆φmn)\n\u000b2 + i\n\nsin(∆φmn)\n\u000b2\u00111/2\n=\n\f\f\nei∆φmn\u000b\f\f .\nSince φm is the phase of the m-th measurement, its deriva-\ntive with respect to any wj is zero unless m = j or n = j.\nIn the former case, a reasoning similar to Appendix A shows\nthat\n∇wj∆φjk ≡∇wjφj −∇wjφk = ∇wjφj =\n= [zh · z −z · zh] · wj\nY 2\nj\n= Γ z · wj\nY 2\nj\n,\n(4)\nwhere Γ z(t) = zh(t)zT(t) −z(t)zhT(t). It is easy to see\nthat ∇wj∆φjk = −∇wj∆φkj. Furthermore, pmm = 1 by\ndeﬁnition, hence ∇wjpmm = 0 for all m and j. From these\nconsiderations, the only nonzero terms in the derivative of P\nare of the form\n∇wjpjk = ∇wjpkj = 2|̺jk|\n\f\f\f\nD\nei(φj−φk)E\f\f\f\n−1\n×\n×\n\"\n−\n\ncos(∆φjk)\n\u000b\n*\nsin(∆φjk)Γ z · wj\nY 2\nj\n+\n+\n+\n\nsin(∆φjk)\n\u000b\n*\ncos(∆φjk)Γ z · wj\nY 2\nj\n+#\n.\n(5)\nWe now deﬁne Ψjk ≡⟨φj −φk⟩= ⟨∆φjk⟩. Plugging in\nthis deﬁnition into Eq. (5) we obtain\n∇wjpjk = 2|̺jk|×\n×\n\"\n−cos(Ψjk)\n*\nsin(∆φjk)Γ z · wj\nY 2\nj\n+\n+\n+ sin(Ψjk)\n*\ncos(∆φjk)Γ z · wj\nY 2\nj\n+#\n= 2|̺jk|\n\"*\n−cosΨjk sin ∆φjk\nΓ z · wj\nY 2\nj\n+\n+\n+\n*\nsin Ψjk cos ∆φjk\nΓ z · wj\nY 2\nj\n+#\n= 2|̺jk|\n*\nsin (Ψjk −∆φjk) Γ z\nY 2\nj\n+\n· wj,\nwhere we again used sin(a −b) = sin a cos b −cos a sin b in\nthe last step. Finally,\n∇wjP = 1 −λ\nN 2\nX\nm,n\n∇wjpmn = 21 −λ\nN 2\nX\nm<n\n∇wjpmn =\n= 41 −λ\nN 2\nN\nX\nk=1\n|̺jk|\n\u001c\nsin [Ψjk −∆φjk(t)] Γ z(t)\nYj(t)2\n\u001d\n· wj.\nwhich is Eq. 8 of [1].\nAPPENDIX C\nGRADIENT OF J IN PSCA\nIn this section we derive Eq. 10 of [1] for the gradient of\nJ. Recall that J is given by\nJ ≡\nP\nX\nj=1\n\f\f\f\f\f\nN\nX\ni=1\nuij\n\f\f\f\f\f =\nP\nX\nj=1\n\f\f\f\f\f\nN\nX\ni=1\nP\nX\nk=1\nvikwkj\n\f\f\f\f\f\nwhere the wkj are real coefﬁcients that we want to optimize\nand the vik are ﬁxed complex numbers. Also recall that Re(.)\nand Im(.) denote the real and imaginary parts.\nWe begin by expanding the complex absolute value:\nX\nj\n\f\f\f\f\f\f\nX\ni,k\nvikwkj\n\f\f\f\f\f\f\n=\nX\nj\n\n\n\n\"\nRe\n X\ni,k\nvikwkj\n!#2\n+\n\"\nIm\n X\ni,k\nvikwkj\n!#2\n\n\n1/2\n.\nWhen computing the derivative in order to wkj, only one term\nin the leftmost sum matters. Thus,\n∂J\n∂wkj\n=\n= 2\n\n\n\"\nRe\n X\ni,k\nvikwkj\n!#2\n+\n\"\nIm\n X\ni,k\nvikwkj\n!#2\n\n−1/2\n×\n×\n\n\n∂\nh\nRe\n\u0010P\ni,k vikwkj\n\u0011i2\n∂wkj\n+\n∂\nh\nIm\n\u0010P\ni,k vikwkj\n\u0011i2\n∂wkj\n\n\n=\n1\n|P\ni uij|\n\nRe\n\nX\ni,k\nvikwkj\n\n\n∂Re\n\u0010P\ni,k vikwkj\n\u0011\n∂wkj\n+\n+ Im\n\nX\ni,k\nvikwkj\n\n\n∂Im\n\u0010P\ni,k vikwkj\n\u0011\n∂wkj\n\n.\n(6)\nIn the sums inside the derivatives, the sum on k can be\ndropped as only one of those terms will be nonzero. Therefore,\n∂Re\n\u0010 P\ni,k vikwkj\n\u0011\n∂wkj\n=\n∂Re\n\u0010 P\ni vikwkj\n\u0011\n∂wkj\n= ∂Re (P\ni vik) wkj\n∂wkj\n= Re\n X\ni\nvik\n!\n= Re (¯vk) ,\n\n\n3\nwhere we used ¯vi ≡P\nk vki to denote the sum of the i-th\ncolumn of V. Similarly,\n∂Im\n\u0010 P\ni,k vikwkj\n\u0011\n∂wkj\n= Im (¯vk) .\nThese results, with the notation ¯uj ≡P\nk vjk as the sum of\nthe j-th column of U, can be plugged into Eq. (6) to yield\nGkj =\n∂J\n∂wkj\n=\n1\n|¯uj|\nh\nRe( ¯vk) × Re(¯uj) + Im( ¯vk) × Im(¯uj)\ni\n.\nAPPENDIX D\nMEAN FIELD\nIn this section we derive Eq. 9 of [1] for the interaction\nof an oscillator with the cluster it is part of. We will assume\nthat there are Nj oscillators in this cluster, coupled all-to-\nall with the same coupling coefﬁcient κ, and that all inter-\ncluster interactions are weak enough to be disregarded. We\nbegin with Kuramoto’s model (Eq. 1 of [1]) omitting the time\ndependency:\n˙φi = ωi +\nX\nk∈cj\nκik sin(φk −φi) +\nX\nk /∈cj\nκik sin(φk −φi)\n˙φi = ωi +\nX\nk∈cj\nκik sin(φk −φi)\n= ωi +\nX\nk∈cj\nκik\nei(φj−φi) −e−i(φj−φi)\n2i\n= ωi + e−iφi\n2i\nX\nk∈cj\nκikeiφk −eiφi\n2i\nX\nk∈cj\nκike−iφk.\nWe now plug in the deﬁnition of mean ﬁeld ̺cjeiΦcj\n=\n1\nNj\nP\nk∈cj eiφk to obtain\n˙φi = ωi + Nj\ne−iφi\n2i κ̺cjeiΦcj −Nj\neiφi\n2i κ̺cje−iΦcj\n= ωi + Njκ̺cj\n\u0002\nsin(Φcj −φi) −sin(φi −Φcj)\n\u0003\n= ωi + 2Njκ̺cj sin(Φcj −φi).\nREFERENCES\n[1] M. Almeida, J.-H. Schleimer, J. Bioucas-Dias, and R. Vig´ario, “Source\nseparation and clustering of phase-locked subspaces,” IEEE Transactions\non Neural Networks (accepted), 2011.\n\n\n",
      "concepts": [
        "also recall that re",
        "clustering",
        "closer look on",
        "to yield\ngkj",
        "from these\nconsiderations",
        "ai",
        "in other words",
        "due to space limitations",
        "can be plugged into eq",
        "separation",
        "we begin by noting that",
        "appendix",
        "we are assuming",
        "deﬁnition",
        "vki to denote the sum of the",
        "which\nwe will denote by",
        "is the phase of the",
        "vigario",
        "and that all inter",
        "complex numbers",
        "clarity",
        "wj\n\nsin",
        "denote the real and imaginary parts",
        "mean field\nin this section we derive eq",
        "we\nbegin with kuramoto",
        "arctan",
        "is given by\neq",
        "aalto\nuniversity",
        "these results",
        "is given by eq",
        "schleimer",
        "in\nthe last step",
        "vikwkj\n\f\f\f\f\f\nwhere the wkj are real",
        "we will omit the dependence on the subspace",
        "wjpmm",
        "cj sin",
        "model",
        "berlin",
        "in rpa\nin this section we derive that the gradient of",
        "and used",
        "when computing the derivative in order to wkj",
        "dias is with the institute of telecommunications",
        "thus",
        "in psca\nin this section we derive eq",
        "we have",
        "only one term\nin the leftmost sum matters",
        "th measurement",
        "are of the form",
        "finally",
        "references",
        "research",
        "subspaces\nappendix",
        "superior\ntechnical institute",
        "similarly",
        "for the sake\nof clarity",
        "throughout this whole section",
        "which is eq",
        "with no loss of\ngenerality",
        "wjpmn",
        "university",
        "as in eq",
        "where we again used sin",
        "in the sums inside the derivatives",
        "plugging in\nthis",
        "cluster interactions are weak enough to be disregarded",
        "wtxh",
        "that only one subspace was found",
        "on the other hand",
        "we obtain",
        "ario",
        "the sum on",
        "humboldt university",
        "presented some results without proof",
        "wjpkj",
        "source\nseparation and clustering of phase",
        "gradient of",
        "is the phase of the estimated\nsource",
        "does not depend on",
        "de\njos",
        "locking",
        "th column of",
        "we begin by expanding the complex absolute value",
        "wtxh\nwtx",
        "we can now replace",
        "coupled all",
        "of mean",
        "measurement",
        "where we have interchanged the partial derivative and the time\naverage operators",
        "reasoning similar to appendix",
        "therefore",
        "neural_network",
        "miguel almeida is with the institute of telecommunications",
        "hendrik",
        "note that",
        "where we used",
        "the only nonzero terms in the derivative of",
        "whenever we\nwrite",
        "for the interaction\nof an oscillator with the cluster it is part of",
        "wj\n\ncos",
        "almeida"
      ],
      "document_type": "academic_paper",
      "content_hash": "ba3ce0c21df79df4",
      "size": 67590,
      "ingestion_time": "2025-06-06T07:33:43.120304",
      "research_field": "mathematics",
      "technical_level": "graduate",
      "citation_count": 10
    },
    {
      "file_path": "C:\\Users\\jason\\Desktop\\tori\\kha\\data\\USB Drive\\docs\\BANKSY\\HierarchicalSynchronization\\neural oscillation\\Fourier oscillator\\1106.2474v1.pdf",
      "file_type": "pdf_document",
      "title": "1106.2474v1",
      "author": "Unknown",
      "subject": "",
      "pages": 3,
      "text_content": "arXiv:1106.2474v1  [stat.ML]  13 Jun 2011\n1\nSource Separation and Clustering\nof Phase-Locked Subspaces: Derivations and Proofs\nMiguel Almeida, Jan-Hendrik Schleimer, Jos´e Bioucas-Dias, Ricardo Vig´ario\nAbstract—Due to space limitations, our submission “Source\nSeparation and Clustering of Phase-Locked Subspaces”, accepted\nfor publication on the IEEE Transactions on Neural Networks\nin 2011, presented some results without proof. Those proofs are\nprovided in this paper.\nIndex Terms—phase-locking, synchrony, source separation,\nclustering, subspaces\nAPPENDIX A\nGRADIENT OF |̺|2 IN RPA\nIn this section we derive that the gradient of |̺|2 is given by\nEq. 6 of [1], where |̺| is deﬁned as in Eq. 5 of [1]. Recall that\n∆φ(t) = φ(t)−ψ(t), where φ(t) is the phase of the estimated\nsource y(t) = wT x(t) and ψ(t) is the phase of the reference\nu(t). Further, deﬁne ̺ ≡|̺|eiΦ.\nWe begin by noting that |̺|2 = (|̺| cos(Φ))2+(|̺| sin(Φ))2,\nso that\n∇|̺|2 = ∇(|̺| cos(Φ))2 + ∇(|̺| sin(Φ))2\n= 2|̺| [cos(Φ)∇(|̺| cos(Φ)) + sin(Φ)∇(|̺| sin(Φ))] .\nNote that we have\n1\nT\nPT\nt=1 cos(∆φ(t))) = |̺| cos(Φ) and\n1\nT\nPT\nt=1 sin(∆φ(t))) = |̺| sin(Φ), so we get\n∇|̺|2 = 2|̺|\n(\ncos(Φ)∇\n\"\n1\nT\nT\nX\nt=1\ncos(∆φ(t)))\n#\n+\n+ sin(Φ)∇\n\"\n1\nT\nT\nX\nt=1\nsin(∆φ(t)))\n#)\n= 2|̺|\nT\nT\nX\nt=1\nh\nsin(Φ) cos(∆φ(t)) −cos(Φ) sin(∆φ(t))\ni\n×\n× ∇∆φ(t)\n= 2|̺|\nT\nT\nX\nt=1\nsin[Φ −∆φ(t)]∇∆φ(t).\n(1)\nMiguel Almeida is with the Institute of Telecommunications, Superior\nTechnical Institute, Portugal. Email: malmeida@lx.it.pt\nJan-Hendrik Schleimer is with the Bernstein Center for Computational Neu-\nroscience, Humboldt University, Berlin. Email: jan-hendrik.schleimer@bccn-\nberlin.de\nJos´e Bioucas-Dias is with the Institute of Telecommunications, Superior\nTechnical Institute, Portugal. Email: bioucas@lx.it.pt\nRicardo Vig´ario is with the Adaptive Informatics Research Centre, Aalto\nUniversity, Finland. Email: ricardo.vigario@hut.ﬁ\nLet’s now take a closer look on ∇∆φ(t). Note that\nφ(t) = arctan\n\u0012wTxh(t)\nwTx(t)\n\u0013\nor\nφ(t) = arctan\n\u0012wTxh(t)\nwTx(t)\n\u0013\n+ π.\nBecause of this we can say, if wTx(t) ̸= 0, that ∇φ(t) =\n∇arctan\n\u0010\nwTxh(t)\nwTx(t)\n\u0011\n. On the other hand, since ∆φ(t) = φ(t)−\nψ(t) and ψ(t) does not depend on w, we have (we will omit\nthe time dependence for the sake of clarity):\n∇∆φ = ∇φ −∇ψ = ∇φ = ∇arctan\n\u0012wTxh\nwTx\n\u0013\n=\nxh · wTx −x · wTxh\n\u0014\n1 +\n\u0010\nwTxh\nwTx\n\u00112\u0015\n·\n\u0010\nwTx\n\u00112 = Γ x(t) · w\nY 2(t)\n,\nwhere Y 2(t) =\n\u0000wTx(t)\n\u00012 +\n\u0000wTxh(t)\n\u00012 is the squared\nmagnitude of the estimated source, and Γ x(t) = xh(t)xT(t)−\nx(t)xhT(t), thus Γ xij(t) = Xi(t)Xj(t) sin(φi(t) −φj(t)).\nWe can now replace ∇∆φ(t) in (1) to obtain\n∇|̺|2 = 2|̺|\nT\n\" T\nX\nt=1\nsin[Φ −∆φ(t)]\nY 2(t)\nΓ x(t)\n#\nw\n= 2|̺|\n\u001csin[Φ −∆φ(t)]\nY 2(t)\nΓ x(t)\n\u001d\nw.\n(2)\nAPPENDIX B\nGRADIENT OF Jl IN IPA\nIn this section we show that the gradient of Jl in Eq. 7 of\n[1] is given by Eq. 8 of [1]. Throughout this whole section,\nwe will omit the dependence on the subspace l, for the sake\nof clarity. In other words, we are assuming (with no loss of\ngenerality) that only one subspace was found. Whenever we\nwrite W, y, ym or z, we will be referring to Wl, yl, (yl)m\nor zl.\nThe derivative of log |det W| is W−T. We will therefore\nfocus on the gradient of the ﬁrst term of Eq. 7 of [1], which\nwe will denote by P:\nP ≡1 −λ\nN 2\nX\nm,n\n|̺mn|2.\nLet’s rewrite P as P = 1−λ\nN 2\nP\nm,n pmn with pmn = |̺mn|2.\nDeﬁne ∆φmn = φm −φn. Omitting the time dependency, we\n\n\n2\nhave\n∇wjpmn = 2|̺mn|∇wj\n\f\f\nei∆φmn\u000b\f\f\n= |̺mn| ×\n\u0010\ncos(∆φmn)\n\u000b2 + i\n\nsin(∆φmn)\n\u000b2\u0011−1/2\n×\n×\nh\n2\n\ncos(∆φmn)\n\u000b\n∇wj\n\ncos(∆φmn)\n\u000b\n+\n+ 2\n\nsin(∆φmn)\n\u000b\n∇wj\n\nsin(∆φmn)\n\u000bi\n= 2|̺mn|\n\f\f\nei∆φmn\u000b\f\f−1 ×\n×\nh\n−\n\ncos(∆φmn)\n\u000bD\nsin(∆φmn)∇wj∆φmn\nE\n+\n+\n\nsin(∆φmn)\n\u000bD\ncos(∆φmn)∇wj∆φmn\nEi\n,\n(3)\nwhere we have interchanged the partial derivative and the time\naverage operators, and used\n\u0010\ncos(∆φmn)\n\u000b2 + i\n\nsin(∆φmn)\n\u000b2\u00111/2\n=\n\f\f\nei∆φmn\u000b\f\f .\nSince φm is the phase of the m-th measurement, its deriva-\ntive with respect to any wj is zero unless m = j or n = j.\nIn the former case, a reasoning similar to Appendix A shows\nthat\n∇wj∆φjk ≡∇wjφj −∇wjφk = ∇wjφj =\n= [zh · z −z · zh] · wj\nY 2\nj\n= Γ z · wj\nY 2\nj\n,\n(4)\nwhere Γ z(t) = zh(t)zT(t) −z(t)zhT(t). It is easy to see\nthat ∇wj∆φjk = −∇wj∆φkj. Furthermore, pmm = 1 by\ndeﬁnition, hence ∇wjpmm = 0 for all m and j. From these\nconsiderations, the only nonzero terms in the derivative of P\nare of the form\n∇wjpjk = ∇wjpkj = 2|̺jk|\n\f\f\f\nD\nei(φj−φk)E\f\f\f\n−1\n×\n×\n\"\n−\n\ncos(∆φjk)\n\u000b\n*\nsin(∆φjk)Γ z · wj\nY 2\nj\n+\n+\n+\n\nsin(∆φjk)\n\u000b\n*\ncos(∆φjk)Γ z · wj\nY 2\nj\n+#\n.\n(5)\nWe now deﬁne Ψjk ≡⟨φj −φk⟩= ⟨∆φjk⟩. Plugging in\nthis deﬁnition into Eq. (5) we obtain\n∇wjpjk = 2|̺jk|×\n×\n\"\n−cos(Ψjk)\n*\nsin(∆φjk)Γ z · wj\nY 2\nj\n+\n+\n+ sin(Ψjk)\n*\ncos(∆φjk)Γ z · wj\nY 2\nj\n+#\n= 2|̺jk|\n\"*\n−cosΨjk sin ∆φjk\nΓ z · wj\nY 2\nj\n+\n+\n+\n*\nsin Ψjk cos ∆φjk\nΓ z · wj\nY 2\nj\n+#\n= 2|̺jk|\n*\nsin (Ψjk −∆φjk) Γ z\nY 2\nj\n+\n· wj,\nwhere we again used sin(a −b) = sin a cos b −cos a sin b in\nthe last step. Finally,\n∇wjP = 1 −λ\nN 2\nX\nm,n\n∇wjpmn = 21 −λ\nN 2\nX\nm<n\n∇wjpmn =\n= 41 −λ\nN 2\nN\nX\nk=1\n|̺jk|\n\u001c\nsin [Ψjk −∆φjk(t)] Γ z(t)\nYj(t)2\n\u001d\n· wj.\nwhich is Eq. 8 of [1].\nAPPENDIX C\nGRADIENT OF J IN PSCA\nIn this section we derive Eq. 10 of [1] for the gradient of\nJ. Recall that J is given by\nJ ≡\nP\nX\nj=1\n\f\f\f\f\f\nN\nX\ni=1\nuij\n\f\f\f\f\f =\nP\nX\nj=1\n\f\f\f\f\f\nN\nX\ni=1\nP\nX\nk=1\nvikwkj\n\f\f\f\f\f\nwhere the wkj are real coefﬁcients that we want to optimize\nand the vik are ﬁxed complex numbers. Also recall that Re(.)\nand Im(.) denote the real and imaginary parts.\nWe begin by expanding the complex absolute value:\nX\nj\n\f\f\f\f\f\f\nX\ni,k\nvikwkj\n\f\f\f\f\f\f\n=\nX\nj\n\n\n\n\"\nRe\n X\ni,k\nvikwkj\n!#2\n+\n\"\nIm\n X\ni,k\nvikwkj\n!#2\n\n\n1/2\n.\nWhen computing the derivative in order to wkj, only one term\nin the leftmost sum matters. Thus,\n∂J\n∂wkj\n=\n= 2\n\n\n\"\nRe\n X\ni,k\nvikwkj\n!#2\n+\n\"\nIm\n X\ni,k\nvikwkj\n!#2\n\n−1/2\n×\n×\n\n\n∂\nh\nRe\n\u0010P\ni,k vikwkj\n\u0011i2\n∂wkj\n+\n∂\nh\nIm\n\u0010P\ni,k vikwkj\n\u0011i2\n∂wkj\n\n\n=\n1\n|P\ni uij|\n\nRe\n\nX\ni,k\nvikwkj\n\n\n∂Re\n\u0010P\ni,k vikwkj\n\u0011\n∂wkj\n+\n+ Im\n\nX\ni,k\nvikwkj\n\n\n∂Im\n\u0010P\ni,k vikwkj\n\u0011\n∂wkj\n\n.\n(6)\nIn the sums inside the derivatives, the sum on k can be\ndropped as only one of those terms will be nonzero. Therefore,\n∂Re\n\u0010 P\ni,k vikwkj\n\u0011\n∂wkj\n=\n∂Re\n\u0010 P\ni vikwkj\n\u0011\n∂wkj\n= ∂Re (P\ni vik) wkj\n∂wkj\n= Re\n X\ni\nvik\n!\n= Re (¯vk) ,\n\n\n3\nwhere we used ¯vi ≡P\nk vki to denote the sum of the i-th\ncolumn of V. Similarly,\n∂Im\n\u0010 P\ni,k vikwkj\n\u0011\n∂wkj\n= Im (¯vk) .\nThese results, with the notation ¯uj ≡P\nk vjk as the sum of\nthe j-th column of U, can be plugged into Eq. (6) to yield\nGkj =\n∂J\n∂wkj\n=\n1\n|¯uj|\nh\nRe( ¯vk) × Re(¯uj) + Im( ¯vk) × Im(¯uj)\ni\n.\nAPPENDIX D\nMEAN FIELD\nIn this section we derive Eq. 9 of [1] for the interaction\nof an oscillator with the cluster it is part of. We will assume\nthat there are Nj oscillators in this cluster, coupled all-to-\nall with the same coupling coefﬁcient κ, and that all inter-\ncluster interactions are weak enough to be disregarded. We\nbegin with Kuramoto’s model (Eq. 1 of [1]) omitting the time\ndependency:\n˙φi = ωi +\nX\nk∈cj\nκik sin(φk −φi) +\nX\nk /∈cj\nκik sin(φk −φi)\n˙φi = ωi +\nX\nk∈cj\nκik sin(φk −φi)\n= ωi +\nX\nk∈cj\nκik\nei(φj−φi) −e−i(φj−φi)\n2i\n= ωi + e−iφi\n2i\nX\nk∈cj\nκikeiφk −eiφi\n2i\nX\nk∈cj\nκike−iφk.\nWe now plug in the deﬁnition of mean ﬁeld ̺cjeiΦcj\n=\n1\nNj\nP\nk∈cj eiφk to obtain\n˙φi = ωi + Nj\ne−iφi\n2i κ̺cjeiΦcj −Nj\neiφi\n2i κ̺cje−iΦcj\n= ωi + Njκ̺cj\n\u0002\nsin(Φcj −φi) −sin(φi −Φcj)\n\u0003\n= ωi + 2Njκ̺cj sin(Φcj −φi).\nREFERENCES\n[1] M. Almeida, J.-H. Schleimer, J. Bioucas-Dias, and R. Vig´ario, “Source\nseparation and clustering of phase-locked subspaces,” IEEE Transactions\non Neural Networks (accepted), 2011.\n\n\n",
      "concepts": [
        "also recall that re",
        "clustering",
        "closer look on",
        "to yield\ngkj",
        "from these\nconsiderations",
        "ai",
        "in other words",
        "due to space limitations",
        "can be plugged into eq",
        "separation",
        "we begin by noting that",
        "appendix",
        "we are assuming",
        "deﬁnition",
        "vki to denote the sum of the",
        "which\nwe will denote by",
        "is the phase of the",
        "vigario",
        "and that all inter",
        "complex numbers",
        "clarity",
        "wj\n\nsin",
        "denote the real and imaginary parts",
        "mean field\nin this section we derive eq",
        "we\nbegin with kuramoto",
        "arctan",
        "is given by\neq",
        "aalto\nuniversity",
        "these results",
        "is given by eq",
        "schleimer",
        "in\nthe last step",
        "vikwkj\n\f\f\f\f\f\nwhere the wkj are real",
        "we will omit the dependence on the subspace",
        "wjpmm",
        "cj sin",
        "model",
        "berlin",
        "in rpa\nin this section we derive that the gradient of",
        "and used",
        "when computing the derivative in order to wkj",
        "dias is with the institute of telecommunications",
        "thus",
        "in psca\nin this section we derive eq",
        "we have",
        "only one term\nin the leftmost sum matters",
        "th measurement",
        "are of the form",
        "finally",
        "references",
        "research",
        "subspaces\nappendix",
        "superior\ntechnical institute",
        "similarly",
        "for the sake\nof clarity",
        "throughout this whole section",
        "which is eq",
        "with no loss of\ngenerality",
        "wjpmn",
        "university",
        "as in eq",
        "where we again used sin",
        "in the sums inside the derivatives",
        "plugging in\nthis",
        "cluster interactions are weak enough to be disregarded",
        "wtxh",
        "that only one subspace was found",
        "on the other hand",
        "we obtain",
        "ario",
        "the sum on",
        "humboldt university",
        "presented some results without proof",
        "wjpkj",
        "source\nseparation and clustering of phase",
        "gradient of",
        "is the phase of the estimated\nsource",
        "does not depend on",
        "de\njos",
        "locking",
        "th column of",
        "we begin by expanding the complex absolute value",
        "wtxh\nwtx",
        "we can now replace",
        "coupled all",
        "of mean",
        "measurement",
        "where we have interchanged the partial derivative and the time\naverage operators",
        "reasoning similar to appendix",
        "therefore",
        "neural_network",
        "miguel almeida is with the institute of telecommunications",
        "hendrik",
        "note that",
        "where we used",
        "the only nonzero terms in the derivative of",
        "whenever we\nwrite",
        "for the interaction\nof an oscillator with the cluster it is part of",
        "wj\n\ncos",
        "almeida"
      ],
      "document_type": "academic_paper",
      "content_hash": "ba3ce0c21df79df4",
      "size": 67590,
      "ingestion_time": "2025-06-06T07:33:43.216268",
      "research_field": "mathematics",
      "technical_level": "graduate",
      "citation_count": 10
    },
    {
      "file_path": "C:\\Users\\jason\\Desktop\\tori\\kha\\data\\better\\0701227v1.pdf",
      "file_type": "pdf_document",
      "title": "Novel Technique for Volatile Optical Memory Using Solitons ",
      "author": "Mohd Abubakr",
      "subject": "",
      "pages": 4,
      "text_content": "Novel Technique for Volatile Optical Memory Using Solitons  \n \nMohd Abubakr \nStudent member, \nElectronics and Communication \nEngineering, \nGokaraju Rangaraju Inst. Of Engg & Tech. \nHyderabad, India. \nEmail: mohdabubakr@gmail.com \n \nR. M. Vinay \nStudent member, \nElectronics and Communication \nEngineering, \nGokaraju Rangaraju Inst. Of Engg & Tech. \nHyderabad, India. \nEmail: manikyavinay@yahoo.com \n \n \nABSTRACT \nRecent advances in optics have shown \nthat solitons have a great potential for \nupgrading the future optical systems \nwhich demand fast and reliable data \ntransfer. \nAlong \nside \nDifferent \narchitectures have evolved to realize an \noptical computer. Due to difference in \nprocessing \nspeeds \nof \noptical \nand \nelectronic devices and usage of opto-\nelectronic converters the efficiency of the \noptical computers is affected. In this \npaper we bring a novel technique for \nvolatile optical memory using solitons \nwhich would be part of realizing an \noptical computer.  Also the principle of \nread and write operations in the soliton \nbased \nvolatile \nmemory \nsystems \nis \nshown.   \n \nKeywords: Solitons, optical processors, \nRAM  \n \n1.0 INTRODUCTION \nThe communication revolution of this era \ndemands fast and reliable data transfer from \ntransmitter to receiver. Optical networks \nhave become an essential requirement in \nkeeping the pace of development in terms of \ntransmission of data, voice and multimedia. \nAlready the industry has responded to this \nchange and various technologies have \nemerged in optics. The undersea fiber optic \nconnections to ultra-long haul connections \nindicate the impact of this revolution.  \nPhotonics promises to revolutionize the area \nof information technology and the 21st \ncentury, just as electronics revolutionized \nthe past hundred years or so [1-2].  \nAlmost all of voice and data (internet) traffic \nis routed through terrestrial and submarine \noptical fiber links, connecting the world \ntogether. Invention of the optical amplifiers \n(OAs) and wavelength-division multiplexing  \n \n \n \n(WDM) \ntechnology \nenabled \nvery \nhigh \ncapacity optical fiber communication links \nthat run for thousands of kilometers without \nany electronic repeaters, but at the same \ntime brought many design challenges. In \nWDM \noptical \nfiber \ncommunications, \ninformation bits are used to modulate the \n(light) carriers at many wavelengths, which \nare then transmitted in a single strand of \nfiber [3].        \nOne of the emerging technologies in optical \nnetworks is solitons which promises to \nbenefit the commercial ultra-long haul all-\noptical \nmulti-terabit \nnetworks \nspanning \ndistances up to many millions of kilometers. \nSolitons are localized nonlinear waves that \nhave highly stable properties that allow them  \nto propagate very long distances with very \nlittle change [4-14].   \nIn this paper we present a novel technique \nof memory storage using soliton waves. This \ntechnique reduces the expenditure spend in \noptical networks for memory storage. Also \nthe memory techniques we present is \nvolatile in nature, hence could be used in \nrealization of all optical networks without the \nusage \nof \nelectronic \nmemory \nstorage \nsystems.  \n  \n2.0 SOLITON WAVES \nA soliton was first observed by Russell as a \nwater surface wave in 1844 Korteweg and \nde Vries in 1895 derived known as the KdV \nequation, a model equation describing a far-\nfield property of the surface wave in the \nlowest order of dispersion and nonlinearity. \nWhen the equation was numerically solved \nin a periodic boundary condition by Zabusky \nand Kruskal in 1965 a set of solitary waves \nwas found to emerge and stably pass each \nother. They named these solitary waves \nsolitons because of their stability. The term \n‘‘soliton’’ was justified when the KdV \n\n\nequation was solved analytically by means \nof inverse scattering transform (IST) and the \nsolution was described by a set of solitons.  \nSolitons are regarded as a fundamental unit \nof mode in nonlinear dispersive medium and \nplay a role similar to the Fourier mode in a \nlinear medium. In particular, a soliton being \nidentified as an Eigen value in IST supports \nits particle (Fermion) concept. \nMeanwhile, self-focusing in a Kerr medium \nwas demonstrated and a spatially localized \nsolution analogous to a soliton was found to \nemerge by the balance of the cubic \nnonlinearity and refraction. The model \nequation, called the nonlinear Schrödinger \nequation, was later found to be integrable by \nZakharov and Shabat, also by means of the \ninverse \nscattering \ntransform, \nand \nthe \nsolution is given by a set of solitons and \ndispersive waves. The theory warrants the \nstability \nof \nthe \nnonlinear \nSchrödinger \nsolitons. \n \n3.0 SOLITONS FEATURES \nThe monolithic integration of optoelectronic \nintegrated circuits in making rapid progress \nbut due to technical hurdles, are becoming \nincompatible \nin \noptical \nnetworks. \nThe \ntechnical hurdles include the switching \ntiming of transistor, which is of minimum \n5ps, and the processing speed of 5ns. Also \nthe cost of these IC’s plays a negative role \nin building a optical network whereas the \ntheoretical speed of optical fibers is 25THz. \nAt the same time we witnessed the \ndevelopment of new technology called as \nsolitons which had the ability to resolve the \nproblems faced by optical networks.  \nIn 1973 A. Hasegawa and F. D. Tappert, \npredicted the transmission of light through \nan optical fiber is similar to the flow of soliton \nwaves [4]. Any optical pulse, which is \ntransmitted into a loss less fiber, forms itself \nto become solitons like Fourier transmission \nmodes in a linear transmission system. This \nresult was confirmed experimentally in 1980 \nby [5]. In 1988 Mollenauer and Smith were \nsuccessful in demonstrating the first all-\noptical soliton transmission by use of the \nRaman process in fiber over a distance of \n4000 km. They used a 55-ps soliton pulse \nthat was periodically amplified by Raman \npumps injected into the fiber every 41.7 km \nin \nboth \nthe \nparallel \nand \nanti-parallel \ndirections [15-16]. \nDue to the short pulse duration and high \nstability, solitons could form the high-speed \ncommunication backbone of tomorrow's \ninformation super-highway. One of the key \ntechnological developments that make use \nof such soliton pulses for our future cost-\neffective and repeater less communication is \nthe invention of erbium doped fiber amplifier \n(EDFA)[2].  \nThe optical transmission losses such as \ndispersion \nand \nnon-linearity \ncan \nbe \neliminated by transmitting Ultra short pulses. \nThe decrease in the amplitude of the wave \ncan also be eliminated by using Raman \namplification or EDFA along with self \ninduced transparency.   \nIn particular, the use of 40 Gbit/s line is \nbecoming the standard for next generation \nsystems and dispersion managed solitons \nare now believed to be the major candidate. \nIn \naddition \nto \ntheir \napplication \nto \ntransmission between two points, solitons \nhave particular merit for use in high speed \noptical time division networks because of \ntheir intrinsic short pulse structure and the \nstationary pulse shape. Such applications \nare now being pursued in various parts of \nthe world. Thus, most internet traffics in the \n21st century will be carried by optical \nsolitons [17]. \nSince solitons do not suffer distortion from \nnonlinearity and dispersion, which are \ninherent in fibers, the natural next step is to \nconstruct an all-optical transmission system \nin which fiber loss is compensated by \namplification. \nIn \nfact, \nthe \noptical \ntransmission systems prior to 1991 required \nrepeaters \nperiodically \ninstalled \nin \ntransmission lines to regenerate optical \npulses which have been distorted by fiber \ndispersion and loss. A repeater consists of a \nlight detector and light pulse generators. \nConsequently, it is the most expensive unit \nin a transmission system and also the \nbottleneck to increase in the transmission \nspeed.  \nRepeater less transmission is a very \ninnovative concept that few people believed \nto be possible. In the absence of realistic \noptical \namplifiers, \nHasegawa \nin \n1983 \nproposed to use the Raman gain of the fiber \nitself. The idea was used in the first long \ndistance all-optical transmission experiment \nby Mollenauer and Smith in 1988. However, \nMollenauer succeeded in the 4000 km \nrepeater less transmission experiment. This \n\n\nachievement also leads to the concept of \ncontemporary \nrepeater \nless \noptical \ntransmission systems, with and without \nsolitons. \nThis \nexperimental \nresult \nhas \nattracted serious interest in the optical \ncommunication community. In particular, the \ninvention of erbium doped fiber amplifiers \n(EDFA) have enhanced the concept of all-\noptical \ntransmission \nsystems \nto \nmore \nrealistic levels as initiated by Nakazawa et \nal. in the first reshaping experiment of \nsolitons. \n \n4.0 \nREALISATION \nOF \nAN \nOPTICAL \nCOMPUTER \nRealizing an optical computer has been a \nlong time goal in optical industry. Different \narchitectures for optical processors have \nalready been established and implemented \nat various labs. Various optical processors \nlike labyrinth, SWAS, DOC, AOS, etc have \nsuccessfully demonstrated the strength and \nspeed of optical processing. The optical \nprocessor speed was reduced due to the \ntime taken by fetching the instructions and \ndata from the conventional main memory. \nThe conventional main memory uses the \nelectronic \ndesign \nand \nopto-electronic \nconverters are used to send and receive \ndata from main memory and processor. The \nefficiency of the optical system can be \nfurther increased if the electronic devices \nare replaced by optical devices which \nminimize or avoid the use of opto-electronic \nconverters.  \nOne of the electronic devices is RAM which \nis very essential in determining the speed of \nthe processor. We bring forward a novel \ntechnique for RAM implementation using the \nsoliton concept. This technique is very \neffective and doesn’t require any opto-\nelectronic converter.  \n \n5.0 VOLATILE OPTICAL MEMORY \nWe present here a novel technique for \nRandom \naccess \nmemory \n(RAM) \nfor \nrealization for an optical computer. RAM \nplays a major role in determining the speed \nof the computer. To match up to the speed \nof optical processor the data fetch timings \nfor a RAM should be fast. This condition \ndemands for the RAM which uses the \nconcept of photonics rather than the \nelectronics. \nWe \nuse \nthe \nestablished \nequations of propagation of soliton waves to \nbuild a volatile memory system for an optical \nprocessor. \nConsider a flat sheet of fiber optic material \nwith variable refractive index such that \nwhenever a soliton pulse enters into it, it \ndirection of propagation forms a closed loop. \nGenerally \nin \ngraded \nindex \nfibers \nthe \ndirection of the pulse continuously changes \nwith the change in the refractive index of the \nfiber similarly in this flat sheet of optic \nmaterial the refractive index changes such \nthat the optical pulse makes a closed loop.   \nThe pulse gets trapped in the closed loop \ndue to the internal reflection. Considering \nthe stable properties of the solitons, the \noptical pulse used in this process is solitons. \nSuch a trapped soliton pulse can be \ninterpreted as a smallest unit of the optical \nmemory \n(opbit). \nIn \na \nsimple \nsystem \npresence of the soliton can be interpreted as \nlogical ‘1’ and the absence of it as ‘0’.  \n \n \n \n \nFig 1: An optical pulse trapped in an optical \nmaterial.  \n \nWith the further advance in the technology, \neach opbit can represent more than two \nstates. This can be realized by taking into \naccount \ndifferent \nparameters \nsuch \nas \namplitude, phase etc into account.  \nWe confine the analysis only up to two \nstates of the opbit.  Taking the standard \nfrequency of the soliton pulse for storing the \ninformation as 10THz and assuming that the \ncircumference of the loop is equal to the \nwavelength of the pulse, we obtain following \nresults. The optical material is manufactured \nsuch that the loop formed by the trapped \n\n\nsolitons forms a circle or any standard \nclosed curve.  \nFor a 10THz wave, the circumference of the \nloop is approximately 30µm and the area of \nthe loop is 7.15 × 10 -11 m2.  For practical \nreasons, the area of the loop is taken as 10-\n10 m2 which also accounts for the isolation \ngap between two adjacent opbits. Therefore \na 1cm2 of the flat optical material contains \nan optical memory storage capacity of more \nthan 106 opbits.  Similarly the optic material \ncan contain several layers of such memory \nstorage.  A 1 cm3 of material can contain a \ncapacity of approximately 1012 opbits. Such \nlarge dynamic RAM will be compatible with \nthe \nprocessing \nspeed \nof \nthe \noptical \nprocessor.    \nThe read and write operations can be done \nthrough a standard laser diode of 10THz \nfrequency (in this case). For read operations \nthe data can be detected using the Boolean \nAND technique shown in [2]. A standard \nsoliton pulse is sent to the location where \nthe data is present and ‘Boolean AND’ \noperation is performed with the data. If the \noutput is ‘1’ then data is equal to ‘1’ and \nvice-versa.   \n \n6.0 CONCLUSIONS \nThough solitons are in the emerging state, \nthe capabilities of solitons are enormous \ncompared to any other technology. We \npresented in this paper a novel technique for \nthe volatile memory which is immensely \nhelpful in making a fast optical computer. \nWith a proper engineering design of the \noptical materials this type of soliton memory \ncan easily be built.    \n \n7.0 ACKNOWLEDGEMENTS  \nThe authors would like to thank Prof. A.P.N. \nRao, \nHead \nof \nElectronics \nand \nCommunication \nDept., \nGRIET \nfor \nhis \nvaluable suggestions on soliton models. \n \n8.0 REFERENCES: \n[1] Mohd Abubakr, Proc. of ICSCI’06, Vol-2, \n256 – 259 (2006)   \n[2] K Porsezian, Pramana, J. of Phys.,vol \n57,1003, (2001) \n[3] Alper Demir, ICCAD’03, November 11-\n13, 2003, San Jose, California, USA.  \n[4]A. Hasegawa and F.D.Tappert,Appl. Phys \nLett. 23, \n142-4 (1973).  \n[5] L. F. Mollenauer, R.H. Stolen, and J. P. \nGordon, Phys. Rev. Lett. 45, 1095-8 (1980). \n[6] D J Kaup and A C Newell, /. Math. Phys. \n19, 798 (1978) \n[7] M Wadati, K Konno and Y H Ichikawa, J. \nPhys. Soc. Jpn. 46, 1965 (1979) \n[8] R Hirota, J. Math. Phys. 14, 805 (1973) \n[9] S V Manakov, Sov. Phys. JETP38, 248 \n(1974)  \n[10] T Tsuchida and M Wadati, Phys. Lett. \nA257, 53 (1999)  \n[11] K Konno and M Wadati, Prog. Theor. \nPhys. 53, 1652 (1975) \n[12] K Porsezian and M Lakshmanan, J. \nMath. Phys. 32, 2923 (1991) \n[13] J Satsuma and N Yajima, Prog. Theor. \nPhys. Suppl. 55, 284 (1974) \n[14] J Weiss, M Tabor and G J Carnevale, J. \nMath. Phys. 24, 522 (1983) \n[15] L. F. Mollenauer and K. Smith, Opt. \nLett. 13, 675-7 (1988). \n[16] A. Hasegawa, Optics & Photonics \nNews, February 2002 \n[17] Akira Hasegawa, Chaos, vol 10, no. 3, \nsep2000. \n\n\n",
      "concepts": [
        "for \nrealization for an optical computer",
        "have enhanced the concept of all",
        "linear medium",
        "and the processing speed of",
        "the area of the loop is taken as",
        "century",
        "negative role \nin building",
        "novel \ntechnique for ram implementation using the \nsoliton concept",
        "gokaraju rangaraju inst",
        "this condition \ndemands for the ram which uses the \nconcept of photonics rather than the \nelectronics",
        "the idea was used in the first long \ndistance all",
        "model",
        "the undersea fiber optic \nconnections to ultra",
        "in the absence of realistic \noptical \namplifiers",
        "phys",
        "self",
        "thus",
        "mollenauer and",
        "the \noptical pulse used in this process is solitons",
        "griet \nfor \nhis \nvaluable suggestions on soliton models",
        "information",
        "equation",
        "similarly the optic material \ncan contain several layers of such memory \nstorage",
        "however",
        "pramana",
        "in \nwdm \noptical \nfiber \ncommunications",
        "light",
        "operation",
        "of phys",
        "photonics \nnews",
        "kaup and",
        "then data is equal to",
        "direction",
        "repeater less transmission is",
        "and wavelength",
        "one of the key \ntechnological developments that make use \nof such soliton pulses for our future cost",
        "implementation",
        "schrödinger",
        "due to difference in \nprocessing \nspeeds \nof \noptical \nand \nelectronic devices and usage of opto",
        "with the further advance in the technology",
        "yajima",
        "ichikawa",
        "when the equation was numerically solved \nin",
        "the conventional main memory uses the \nelectronic \ndesign \nand \nopto",
        "frequency",
        "optical \ntransmission \nsystems \nto \nmore \nrealistic levels as initiated by nakazawa et \nal",
        "appl",
        "swas",
        "invention of the optical amplifiers",
        "tappert",
        "the optical transmission losses such as \ndispersion \nand \nnon",
        "to match up to the speed \nof optical processor the data fetch timings \nfor",
        "propagation",
        "porsezian and",
        "introduction",
        "nonlinearity",
        "which is \ntransmitted into",
        "smallest unit of the optical \nmemory",
        "which is of minimum",
        "porsezian",
        "we confine the analysis only up to two \nstates of the opbit",
        "optics",
        "distance of",
        "of icsci",
        "soliton waves",
        "this \n\n\nachievement also leads to the concept of \ncontemporary \nrepeater \nless \noptical \ntransmission systems",
        "internet",
        "the decrease in the amplitude of the wave \ncan also be eliminated by using raman \namplification or edfa along with self \ninduced transparency",
        "gmail",
        "head \nof \nelectronics \nand \ncommunication \ndept",
        "one of the emerging technologies in optical \nnetworks is solitons which promises to \nbenefit the commercial ultra",
        "solitons could form the high",
        "optical processors",
        "was justified when the kdv \n\n\nequation was solved analytically by means \nof inverse scattering transform",
        "division multiplexing",
        "line is \nbecoming the standard for next generation \nsystems and dispersion managed solitons \nare now believed to be the major candidate",
        "hasegawa and",
        "capacity",
        "communication",
        "hasegawa",
        "amplification",
        "which are \ninherent in fibers",
        "tabor and",
        "model equation describing",
        "each opbit can represent more than two \nstates",
        "refraction",
        "optical soliton transmission by use of the \nraman process in fiber over",
        "smith",
        "stability",
        "linear transmission system",
        "opbits",
        "most internet traffics in the",
        "nonlinear",
        "community",
        "predicted the transmission of light through \nan optical fiber is similar to the flow of soliton \nwaves",
        "acknowledgements  \nthe authors would like to thank prof",
        "the \ntechnical hurdles include the switching \ntiming of transistor",
        "realisation",
        "this \ntechnique reduces the expenditure spend in \noptical networks for memory storage",
        "of material can contain"
      ],
      "document_type": "research_document",
      "content_hash": "00c4430bcc6d06dd",
      "size": 73557,
      "ingestion_time": "2025-06-06T07:33:43.248085",
      "research_field": "physics",
      "technical_level": "undergraduate",
      "citation_count": 23
    },
    {
      "file_path": "C:\\Users\\jason\\Desktop\\tori\\kha\\data\\USB Drive\\may1move\\Teach\\Learn\\Editorial-Board_2025_Information-Fusion.pdf",
      "file_type": "pdf_document",
      "title": "Editorial-Board_2025_Information-Fusion",
      "author": "Unknown",
      "subject": "",
      "pages": 2,
      "text_content": "INFORMATION FUSION\nAn International Journal on Multi-Sensor, Multi-Source Information Fusion\nEditor-in-Chief\nSalvador S. Garçia-Lopez, University of Jaen, Jaen, Spain\nFounding Editor in Chief\nBelur Dasarathy, Consultant, Information Fusion\nMoloud Abdar \nInstitute for Intelligent Systems Research and Innovation (IISRI), Deakin University, \nGeelong, Australia\n• Deep Learning and Machine Learning • Computer Vision • Uncertainty \nQuantification • Explainable AI\nPedro Henriques Abreu\nCISUC, Department of Informatics Engineering, University of Coimbra, Portugal\nAlberto Cano \nVirginia Commonwealth University, USA\nJavier del Ser\nTECNALIA, Basque Research & Technology Alliance (BRTA), Derio, Spain\nWeiping Ding\nNantong University, School of Artificial Intelligence and Computer Science, \nNantong, China\nYucheng Dong\nSichuan University, Chengdu, China\nGiancarlo Fortino\nDept. of Computer Engineering, Modeling, Electronics and Systems (DIMES),\nUniversità della Calabria, Rende, Italy\nSensor Fusion\nJesus Garcia Herrero\nDpto. de Informática, Universidad Carlos III de Madrid, Madrid, Colmenarejo, Spain\nXiaojie Guo\nTianjin University, Tianjin, China\n• Research field • Computer Vision • Pattern Recognition • Machine Learning\nJunjun Jiang\nHarbin Institute of Technology\nSensor Fusion\nJun Liu\nXi’an Jiaotong University, Xi’an, China\n• Knowledge Fusion • Knowledge Graph • Natural Language Processing\nTiancheng Li\nNorthwestern Polytechnical University, Xi’an, China\nYe Li\nShenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, \nShenzhen, China\n• sensors • wearable devices • deep learning\nZheng Liu\nFaculty of Applied Science, University of British Columbia, Vanvouver, British\nColumbia, Canada\nFusion for Signal/Image Processing and Understanding\nJiayi Ma\nWuhan University; Wuhan, China\n• Deep Learning for Information Fusion\nRui Mao\nNanyang Technological University, Singapore\n• Affective Computing • Cognitive Computing • Computational Linguistics \n• Multimodal Fusion for Quantitative Finance\nEugenio Martínez-Camara\nUniversity of Jaén, Spain\n• Natural Language Processing • sentiment análisis • conversational systems \n• language understanding\nGhulam Muhammad\nDepartment of Computer Engineering, College of Computer and Information \nSciences, King Saud University, Riyadh, Saudi Arabia\n• Smart Healthcare • Internet of Medical Things (IoMT) Fusion • Multi-modal \nMedical Image Fusion\nKhan Muhammad\nSungkyunkwan University (SKKU), Seoul, South Korea\nDaniel Peralta\nIDLab, Department of Information Technology, Ghent University - imec (Belgium)\n• Time series analysis • Biomedical data • Biometrics • Machine Learning\nLior Rokach\nSoftware and Information Systems Engineering, Ben Gurion University of the \nNegev, Beer-Sheva, Israel\nMachine Learning and Pattern Recognition\nPourya Shamsolmoali\nEast China Normal University, China and Queen’s University Belfast, UK\n• Machine learning • computer vision • remote sensing imagery processing\nLauro Snidaro\nDept. of Mathematics and Computer Science, University of Udine, Udine, Italy\nFusion for Signal/Image Processing and Understanding\nShiliang Sun\nDept. of Computer Science and Technology, East China Normal University, \nShanghai, China\nMachine Learning and Pattern Recognition\nMayank Vatsa\nIndraprastha Institute of Information Technology (IIIT) - Delhi, Okhla Industrial \nEstate, Phase III, New Delhi, India\nFusion for Signal/Image Processing and Understanding\nGemine Vivone\nNational Research Council - Institute of Methodologies for Environmental Analysis, \nTito, Italy\n• Pansharpening • Image Fusion • Image Enhancement • Remote Sensing\nLaurence T. Yang\nSt. Francis Xavier University, Antigonish, Nova Scotia, Canada\nIoT and Information Fusion\nWenguan Wang\nZhejiang University, China\n• Visual-Language Modeling • Embodied AI • Visual Understanding • Video \nProcessing\nArea Editors\nNuria Agel\nRamon Llul University, Barcelona, Spain\nÉloi Bossé\nPolytechnic of Montreal, Montreal, Quebec, Canada\nHumberto Bustince\nUniversidad Pública de Navarra, Pamplona, Spain\nDavid Camacho\nTechnical University of Madrid, Spain\nErik Cambria\nSchool of Computer Science and Engineering, Nanyang Technological University, \nSingapore\nRodrigo Capobianco Guido\nSão Paulo State University (UNESP), São José do Rio Preto, São Paulo, Brazil \nVictor Chang\nAston University, UK\nNitesh Chawla\nUniversity of Notre Dame, Notre Dame, Indiana, USA\nMin Chen\nHuazhong University of Science and Technology, Hongshan District, \nWuhan, China\nShyi-Ming Chen\nAsia University, Taichung, Taiwan\nFrancisco Chiclana\nDe Monfort University, Leicester, UK\nDomenico Ciuonzo\nUniversity of Naples, Naples, Italy\nJuan Manuel Corchado\nUniversity of Salamanca, Salamanca, Spain\nSwagatam Das\nIndian Statistical Institute Computer and Communication Sciences Division, \nKolkata, India\nGuy De Tré\nUniversiteit Gent, Gent, Belgium\nRuxi Ding\nSchool of Management and Economics, Beijing Institute of Technology, Beijing, \nChina\nHaris Doukas\nNational Technical University of Athens (NTUA), Athens, Greece\nPetr Ekel\nPontifícia Universidade Católica de Minas Gerais, Belo Horizonte, Brazil\nJulian Fierrez\nAutonomous University of Madrid, Madrid, Spain\nHassan Ghassemian\nTarbiat Modares University, Tehran, Iran\nGiorgio Giacinto\nUniversità di Cagliari, Cagliari, Italy\nManuel Graña\nUniversidad del Pais Vasco (Basque Country), San Sebastian, Spain\nRaffaele Gravina\nUniversity of Calabria, Rende, Italy\nEnrique Herrera-Viedma\nUniversidad de Granada, Granada, Spain\nDanfeng Hong\nAerospace Information Research Institute, Chinese Academy of Sciences, Beijing, \nChina\nAmir Hussain\nEdinburgh Napier University, Scotland, UK\nDeepak Kumar Jain\nDalian University of Technology, Dalian, China\nJason Jung\nChung Ang University Korea, Dongjak, Seoul, The Republic of Korea\nFakhri Karray\nUniversity of Waterloo, Waterloo, Ontario, Canada\nMieczyslaw Kokar\nNortheastern University (NU), Dept. of Electrical and Computer Engineering, \nBoston, Massachusetts, USA\nShutao Li\nHunan University, Hunan, China\nTianrui Li\nSouthwest Jiaotong University, Chengdu, China\nHuchang Liao\nXichuan University, Chengdu, China\nEditorial Board\n\n\nAntonio Liotta\nCollege of Engineering and Technology, University of Derby, Derby, England, UK\nJun Liu\nUlster University, County Londonderry, Northern Ireland, UK\nShuai Liu\nCollege of Information Science and Engineering, Hunan Normal University, \nChangsha, China\nXinwang Liu\nNational University of Defense Technology, Changsha, China\nYu Liu\nHefei University of Technology, Hefei, China\nAngela Locoro\nUniversità degli Studi di Brescia, Italy\nAlessandra Lumini\nUniversità di Bologna, Bologna, Italy\nJuan Manuel Górriz\nUniversity of Granada, Granada, Spain \nLuis Martínez\nUniversity of Jaen, Department of Computer Science, Jaen, Spain\nPnnuthurai Nagaratham Suganthan\nNanyang Technological University, Singapore, Singapore\nGuillermo Navarro-Arribas\nUniversitat Autònoma de Barcelona (UAB), Bellaterra, Spain\nFeiping Nie\nUniversity of Texas at Arlington, Arlington, Texas, USA\nXin Ning\nInstitute of Semiconductors, Chinese Academy of Sciences, Beijing, China\nNikunj Oza\nNASA Ames Research Center, Moffett Field, California, USA\nFrancesco Piccialli\nUniversity of Naples Federico II, Napoli, Italy\nCarmen C. Y. Poon\nChinese University of Hong Kong, Hong Kong, China\nSoujanya Poria\nNanyang Technological University, Singapore\nChuan Qin\nUniversity of Shanghai for Science and Technology, Shanghai, China\nBranko Ristic\nDefence and Science Technology Organisation, Melbourne, Victoria, Australia\nJuan José Rodríguez\nUniversidad de Burgos, Spain\nArun Ross\nMichigan State University, East Lansing, Michigan, USA\nYvan Saeys\nGhent University, Gent, Belgium\nCarlo Sansone\nUniversity of Naples Federico II, Napoli, Italy\nRicha Singh\nIndraprastha Institute of Information Technology (IIIT) - Delhi, New Delhi, India\nPonnuthurai Suganthan\nQatar University, Doha, Qatar\nShuli Sun\nHeilongjiang University, Heilongjiang, China\nXiaohui Tao\nUniversity of Southern Queensland, Australia\nLuigi Troiano\nUniversity of Salerno, Salerno, Italy\nIsis Truck\nUniversité Paris 8, Saint-Denis, France\nJuan Velasquez Silva\nUniversidad de Chile, Santiago, Chile\nSebastián Ventura\nUniversity of Córdoba, Córdoba, Spain\nShuihua Wang\nUniversity of Leicester, Leicester, UK\nZidong Wang\nBrunel University London, London, UK\nZhelong Wang\nDalian University of Technology, Dalian, China\nMichal Wozniak\nWroclaw University of Science and Technology, Wroclaw, Poland\nGuowen Xu\nCity University of Hong Kong, Kowloon, Hong Kong SAR\nZeshui Xu\nSichuan University, Chengdu, China\nLuigi Troiano\nYejun Xu\nTianjin Univeristy, Tianjin, China\nZheng Yan\nXidian University, Xi’an, China\nJianming Zhang\nSchool of Mathematics and Statistics, Hubei Minzu University, Enshi, Hubei, China\nHengjie Zhang\nHohai University, Nanjing, China\nLefei Zhang\nWuhan University, Wuhan, China\nYin Zhang\nUniversity of Electronic Science and Technology of China, Chengdu, China\nZheng Zhang\nHarbin Institute of Technology (HIT), Shenzhen, China\nZhen Zhang\nDalian University of Technology, Dalian, Liaoning, China \nNan Zhang\nWenzhou University, China\nEarly Career Editorial Board\nYifan Zhu, Beijing University of Posts and Telecommunications, Beijing, China\nTong Wu, Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu, China\n\n\n",
      "concepts": [
        "uk\nzhelong wang\ndalian university of technology",
        "napoli",
        "university belfast",
        "jaen",
        "china and queen",
        "china\ntianrui li\nsouthwest jiaotong university",
        "nanjing university of aeronautics and astronautics",
        "india\nponnuthurai suganthan\nqatar university",
        "conversational systems",
        "east china normal university",
        "model",
        "polytechnic of montreal",
        "sentiment",
        "iiit",
        "derby",
        "denis",
        "fusion",
        "information",
        "india\nfusion for signal",
        "visual understanding",
        "hong kong sar\nzeshui xu\nsichuan university",
        "beer",
        "spain\nfounding editor in chief\nbelur dasarathy",
        "computational linguistics",
        "deep learning",
        "china\nxinwang liu\nnational university of defense technology",
        "usa\njavier del ser\ntecnalia",
        "portugal\nalberto cano \nvirginia commonwealth university",
        "scotland",
        "sheva",
        "source information fusion\neditor",
        "iisri",
        "bologna",
        "tianjin",
        "pansharpening",
        "nanjing",
        "spain",
        "san sebastian",
        "uk\nnitesh chawla\nuniversity of notre dame",
        "francis xavier university",
        "hunan normal university",
        "hong kong",
        "spain\nxiaojie guo\ntianjin university",
        "skku",
        "italy\nisis truck",
        "camara\nuniversity of",
        "china \nnan zhang\nwenzhou university",
        "management",
        "pamplona",
        "derio",
        "imec",
        "wuhan",
        "artificial intelligence",
        "modeling",
        "south korea\ndaniel peralta\nidlab",
        "yang\nst",
        "liaoning",
        "notre dame",
        "italy\nfusion for signal",
        "singapore\nchuan qin\nuniversity of shanghai for science and technology",
        "universidad carlos iii de madrid",
        "enhancement",
        "an jiaotong university",
        "hubei minzu university",
        "of electrical and computer engineering",
        "usa\nxin ning\ninstitute of semiconductors",
        "victoria",
        "england",
        "saudi arabia",
        "uncertainty \nquantification",
        "okhla industrial \nestate",
        "university of coimbra",
        "india\nguy de",
        "cognition",
        "arlington",
        "cagliari",
        "deep learning for information fusion\nrui mao\nnanyang technological university",
        "ghent university",
        "china\ntong wu",
        "melbourne",
        "greece\npetr ekel",
        "uk\nshuai liu\ncollege of information science and engineering",
        "china\nzheng zhang\nharbin institute of technology",
        "smart healthcare",
        "boston",
        "beijing institute of technology",
        "communication",
        "beijing",
        "italy\njuan manuel",
        "china\njason jung\nchung ang university korea",
        "colmenarejo",
        "spain\nhassan ghassemian\ntarbiat modares university",
        "china\nyu liu\nhefei university of technology",
        "wearable devices",
        "delhi",
        "nanyang technological university",
        "of computer engineering",
        "multimodal fusion for quantitative finance\neugenio",
        "spain\nswagatam das\nindian statistical institute computer and communication sciences division",
        "madrid"
      ],
      "document_type": "technical_documentation",
      "content_hash": "7765b6e19d04e5e2",
      "size": 74465,
      "ingestion_time": "2025-06-06T07:33:43.279852",
      "research_field": "computer_science",
      "technical_level": "general_public",
      "citation_count": 0
    }
  ]
}