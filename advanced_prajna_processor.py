#!/usr/bin/env python3\n\"\"\"\nAdvanced Prajna Data Processor\n=============================\n\nIntelligent processing of different data types for Prajna's consciousness.\nExtracts concepts, relationships, and knowledge from various file formats.\n\"\"\"\n\nimport os\nimport asyncio\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional\nimport json\nimport hashlib\nfrom datetime import datetime\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"prajna.advanced_ingest\")\n\nclass AdvancedDataProcessor:\n    \"\"\"Advanced data processing for Prajna consciousness\"\"\"\n    \n    def __init__(self, data_directory: str):\n        self.data_dir = Path(data_directory)\n        self.processed_count = 0\n        self.knowledge_base = []\n        \n    async def process_all_data(self):\n        \"\"\"Process all data in the directory\"\"\"\n        logger.info(f\"üß† Advanced processing of {self.data_dir}...\")\n        \n        for root, dirs, files in os.walk(self.data_dir):\n            for file in files:\n                file_path = Path(root) / file\n                await self._process_file_advanced(file_path)\n        \n        await self._save_knowledge_base()\n        \n        logger.info(f\"‚úÖ Processed {self.processed_count} files\")\n        logger.info(f\"üß† Knowledge base contains {len(self.knowledge_base)} entries\")\n    \n    async def _process_file_advanced(self, file_path: Path):\n        \"\"\"Advanced processing of individual files\"\"\"\n        try:\n            file_ext = file_path.suffix.lower()\n            \n            if file_ext == '.pdf':\n                knowledge = await self._process_pdf(file_path)\n            elif file_ext in ['.txt', '.md']:\n                knowledge = await self._process_text(file_path)\n            elif file_ext in ['.py', '.js', '.ts']:\n                knowledge = await self._process_code(file_path)\n            elif file_ext == '.json':\n                knowledge = await self._process_json(file_path)\n            elif file_ext in ['.jpg', '.png']:\n                knowledge = await self._process_image(file_path)\n            else:\n                knowledge = await self._process_generic(file_path)\n            \n            if knowledge:\n                self.knowledge_base.append(knowledge)\n                self.processed_count += 1\n                \n                if self.processed_count % 100 == 0:\n                    logger.info(f\"üìä Processed {self.processed_count} files...\")\n                    \n        except Exception as e:\n            logger.warning(f\"‚ö†Ô∏è Failed to process {file_path}: {e}\")\n    \n    async def _process_pdf(self, file_path: Path) -> Dict[str, Any]:\n        \"\"\"Process PDF documents\"\"\"\n        return {\n            'type': 'document',\n            'path': str(file_path),\n            'title': file_path.stem,\n            'content_type': 'pdf_document',\n            'size': file_path.stat().st_size,\n            'concepts': ['document', 'pdf', 'text_content'],\n            'summary': f\"PDF document: {file_path.name}\",\n            'timestamp': datetime.now().isoformat()\n        }\n    \n    async def _process_text(self, file_path: Path) -> Dict[str, Any]:\n        \"\"\"Process text files\"\"\"\n        try:\n            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                content = f.read()[:10000]  # First 10KB\n            \n            # Extract basic concepts from content\n            words = content.lower().split()\n            important_words = [w for w in words if len(w) > 5][:20]\n            \n            return {\n                'type': 'text',\n                'path': str(file_path),\n                'title': file_path.stem,\n                'content': content,\n                'content_type': 'text_document',\n                'size': file_path.stat().st_size,\n                'concepts': ['text', 'document'] + important_words,\n                'summary': content[:200] + \"...\" if len(content) > 200 else content,\n                'timestamp': datetime.now().isoformat()\n            }\n        except Exception:\n            return None\n    \n    async def _process_code(self, file_path: Path) -> Dict[str, Any]:\n        \"\"\"Process code files\"\"\"\n        try:\n            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                content = f.read()[:5000]  # First 5KB\n            \n            # Extract programming concepts\n            concepts = ['code', 'programming', file_path.suffix[1:]]\n            \n            # Look for common programming patterns\n            if 'def ' in content or 'function' in content:\n                concepts.append('functions')\n            if 'class ' in content:\n                concepts.append('classes')\n            if 'import ' in content or 'from ' in content:\n                concepts.append('modules')\n            \n            return {\n                'type': 'code',\n                'path': str(file_path),\n                'title': file_path.stem,\n                'content': content,\n                'content_type': f'{file_path.suffix[1:]}_code',\n                'size': file_path.stat().st_size,\n                'concepts': concepts,\n                'summary': f\"Code file: {file_path.name} ({file_path.suffix[1:]})\",\n                'timestamp': datetime.now().isoformat()\n            }\n        except Exception:\n            return None\n    \n    async def _process_json(self, file_path: Path) -> Dict[str, Any]:\n        \"\"\"Process JSON files\"\"\"\n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n            \n            # Extract keys as concepts\n            concepts = ['json', 'data', 'structured']\n            if isinstance(data, dict):\n                concepts.extend(list(data.keys())[:10])\n            \n            return {\n                'type': 'structured_data',\n                'path': str(file_path),\n                'title': file_path.stem,\n                'content': json.dumps(data, indent=2)[:2000],\n                'content_type': 'json_data',\n                'size': file_path.stat().st_size,\n                'concepts': concepts,\n                'summary': f\"JSON data file: {file_path.name}\",\n                'timestamp': datetime.now().isoformat()\n            }\n        except Exception:\n            return None\n    \n    async def _process_image(self, file_path: Path) -> Dict[str, Any]:\n        \"\"\"Process image files\"\"\"\n        return {\n            'type': 'image',\n            'path': str(file_path),\n            'title': file_path.stem,\n            'content_type': 'image_file',\n            'size': file_path.stat().st_size,\n            'concepts': ['image', 'visual', file_path.suffix[1:]],\n            'summary': f\"Image file: {file_path.name}\",\n            'timestamp': datetime.now().isoformat()\n        }\n    \n    async def _process_generic(self, file_path: Path) -> Dict[str, Any]:\n        \"\"\"Process generic files\"\"\"\n        return {\n            'type': 'generic',\n            'path': str(file_path),\n            'title': file_path.stem,\n            'content_type': 'generic_file',\n            'size': file_path.stat().st_size,\n            'concepts': ['file', file_path.suffix[1:] if file_path.suffix else 'unknown'],\n            'summary': f\"File: {file_path.name}\",\n            'timestamp': datetime.now().isoformat()\n        }\n    \n    async def _save_knowledge_base(self):\n        \"\"\"Save processed knowledge base\"\"\"\n        output_file = Path(\"prajna_knowledge_base.json\")\n        \n        knowledge_summary = {\n            'metadata': {\n                'total_files': len(self.knowledge_base),\n                'processing_date': datetime.now().isoformat(),\n                'source_directory': str(self.data_dir)\n            },\n            'knowledge_entries': self.knowledge_base\n        }\n        \n        with open(output_file, 'w', encoding='utf-8') as f:\n            json.dump(knowledge_summary, f, indent=2, ensure_ascii=False)\n        \n        logger.info(f\"üíæ Knowledge base saved to: {output_file}\")\n        \n        # Also create a concepts index\n        await self._create_concepts_index()\n    \n    async def _create_concepts_index(self):\n        \"\"\"Create an index of all concepts found\"\"\"\n        concept_counts = {}\n        \n        for entry in self.knowledge_base:\n            for concept in entry.get('concepts', []):\n                concept_counts[concept] = concept_counts.get(concept, 0) + 1\n        \n        # Sort by frequency\n        sorted_concepts = sorted(concept_counts.items(), key=lambda x: x[1], reverse=True)\n        \n        concepts_index = {\n            'total_unique_concepts': len(sorted_concepts),\n            'top_concepts': sorted_concepts[:100],\n            'all_concepts': dict(sorted_concepts)\n        }\n        \n        with open('prajna_concepts_index.json', 'w', encoding='utf-8') as f:\n            json.dump(concepts_index, f, indent=2, ensure_ascii=False)\n        \n        logger.info(f\"üß† Concepts index created: {len(sorted_concepts)} unique concepts\")\n        logger.info(f\"üîù Top concepts: {[c[0] for c in sorted_concepts[:10]]}\")\n\nasync def main():\n    \"\"\"Main function for advanced processing\"\"\"\n    data_directory = \"C:\\\\Users\\\\jason\\\\Desktop\\\\tori\\\\kha\\\\data\"\n    \n    processor = AdvancedDataProcessor(data_directory)\n    await processor.process_all_data()\n    \n    print(\"\\nüéâ ADVANCED DATA PROCESSING COMPLETE!\")\n    print(\"üìÅ Files created:\")\n    print(\"   üìö prajna_knowledge_base.json - Complete knowledge base\")\n    print(\"   üß† prajna_concepts_index.json - Concepts index\")\n    print(\"\\nüåü Prajna's consciousness has been enhanced with structured knowledge!\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n